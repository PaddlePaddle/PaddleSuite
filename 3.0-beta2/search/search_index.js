var __index = {"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"en/index.html","title":"Home","text":""},{"location":"en/index.html#introduction","title":"\ud83d\udd0d Introduction","text":"<p>PaddleX 3.0 is a low-code development tool for AI models built on the PaddlePaddle framework. It integrates numerousready-to-use pre-trained models, enablingfull-process developmentfrom model training to inference, supportinga variety of mainstream hardware both domestic and international, and aiding AI developers in industrial practice.</p> Image Classification Multi-label Image Classification Object Detection Instance Segmentation Semantic Segmentation Image Anomaly Detection OCR Table Recognition PP-ChatOCRv3-doc Time Series Forecasting Time Series Anomaly Detection Time Series Classification"},{"location":"en/index.html#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<p>Warning</p> <p>Please ensure you have a basic Python runtime environment before installing PaddleX (Note: Currently supports Python 3.8 to Python 3.10, with more Python versions being adapted).</p>"},{"location":"en/index.html#installing-paddlepaddle","title":"Installing PaddlePaddle","text":"CPUCUDA 11.8CUDA 12.3 <pre><code>python -m pip install paddlepaddle==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n</code></pre> <pre><code>python -m pip install paddlepaddle-gpu==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n</code></pre> <pre><code>python -m pip install paddlepaddle-gpu==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cu123/\n</code></pre> <p>\u2757 For more PaddlePaddle Wheel versions, please refer to the PaddlePaddle official website.</p>"},{"location":"en/index.html#installing-paddlex","title":"Installing PaddleX","text":"<pre><code>pip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddlex-3.0.0b2-py3-none-any.whl\n</code></pre> <p>\u2757 For more installation methods, please refer to the PaddleX Installation Guide</p>"},{"location":"en/index.html#command-line-usage","title":"\ud83d\udcbb Command Line Usage","text":"<p>A single command can quickly experience the production line effect, with a unified command line format as follows:</p> <pre><code>paddlex --pipeline [production line name] --input [input image] --device [running device]\n</code></pre> <p>You only need to specify three parameters:</p> <ul> <li><code>pipeline</code>: The name of the production line</li> <li><code>input</code>: The local path or URL of the input file to be processed (e.g., an image)</li> <li><code>device</code>: The GPU number used (for example, <code>gpu:0</code> indicates using the 0<sup>th</sup> GPU), or you can choose to use CPU (<code>cpu</code>)</li> </ul> <p>OCR-related CLI</p> OCRTable RecognitionLayout ParsingFormula RecognitionSeal Text Recognition <pre><code>paddlex --pipeline OCR --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_ocr_002.png',\n'dt_polys': [array([[161,  27],\n    [353,  22],\n    [354,  69],\n    [162,  74]], dtype=int16), array([[426,  26],\n    [657,  21],\n    [657,  58],\n    [426,  62]], dtype=int16), array([[702,  18],\n    [822,  13],\n    [824,  57],\n    [704,  62]], dtype=int16), array([[341, 106],\n    [405, 106],\n    [405, 128],\n    [341, 128]], dtype=int16)\n    ...],\n'dt_scores': [0.758478200014338, 0.7021546472698513, 0.8536622648391111, 0.8619181462164781, 0.8321051217096188, 0.8868756173427551, 0.7982964727675609, 0.8289939036796322, 0.8289428877522524, 0.8587063317632897, 0.7786755892491615, 0.8502032769081344, 0.8703346500042997, 0.834490931790065, 0.908291103353393, 0.7614978661708064, 0.8325774055997542, 0.7843421347676149, 0.8680889482955594, 0.8788859304537682, 0.8963341277518075, 0.9364654810069546, 0.8092413027028257, 0.8503743089091863, 0.7920740420391101, 0.7592224394793805, 0.7920547400069311, 0.6641757962457888, 0.8650289477605955, 0.8079483304467047, 0.8532207681055275, 0.8913377034754717],\n'rec_text': ['\u200b\u767b\u673a\u724c\u200b', 'BOARDING', 'PASS', '\u200b\u8231\u4f4d\u200b', 'CLASS', '\u200b\u5e8f\u53f7\u200b SERIALNO.', '\u200b\u5ea7\u4f4d\u53f7\u200b', '\u200b\u65e5\u671f\u200b DATE', 'SEAT NO', '\u200b\u822a\u73ed\u200b FLIGHW', '035', 'MU2379', '\u200b\u59cb\u53d1\u5730\u200b', 'FROM', '\u200b\u767b\u673a\u53e3\u200b', 'GATE', '\u200b\u767b\u673a\u200b\u65f6\u95f4\u200bBDT', '\u200b\u76ee\u7684\u5730\u200bTO', '\u200b\u798f\u5dde\u200b', 'TAIYUAN', 'G11', 'FUZHOU', '\u200b\u8eab\u4efd\u200b\u8bc6\u522b\u200bIDNO', '\u200b\u59d3\u540d\u200bNAME', 'ZHANGQIWEI', \u200b\u7968\u53f7\u200bTKTNO', '\u200b\u5f20\u797a\u4f1f\u200b', '\u200b\u7968\u4ef7\u200bFARE', 'ETKT7813699238489/1', '\u200b\u767b\u673a\u53e3\u200b\u4e8e\u200b\u8d77\u98de\u524d\u200b10\u200b\u5206\u949f\u200b\u5173\u95ed\u200bGATESCLOSE10MINUTESBEFOREDEPARTURETIME'],\n'rec_score': [0.9985831379890442, 0.999696917533874512, 0.9985735416412354, 0.9842517971992493, 0.9383274912834167, 0.9943678975105286, 0.9419361352920532, 0.9221674799919128, 0.9555020928382874, 0.9870321154594421, 0.9664073586463928, 0.9988052248954773, 0.9979352355003357, 0.9985110759735107, 0.9943482875823975, 0.9991195797920227, 0.9936401844024658, 0.9974591135978699, 0.9743705987930298, 0.9980487823486328, 0.9874696135520935, 0.9900962710380554, 0.9952947497367859, 0.9950481653213501, 0.989926815032959, 0.9915552139282227, 0.9938777685165405, 0.997239887714386, 0.9963340759277344, 0.9936134815216064, 0.97223961353302]}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline table_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/table_recognition.jpg --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/table_recognition.jpg', 'layout_result': {'input_path': '/root/.paddlex/predict_input/table_recognition.jpg', 'boxes': [{'cls_id': 3, 'label': 'Table', 'score': 0.6014542579650879, 'coordinate': [0, 21, 551, 118]}]}, 'ocr_result': {'dt_polys': [array([[37., 40.],\n    [75., 40.],\n    [75., 60.],\n    [37., 60.]], dtype=float32), array([[123.,  37.],\n    [200.,  37.],\n    [200.,  59.],\n    [123.,  59.]], dtype=float32), array([[227.,  37.],\n    [391.,  37.],\n    [391.,  58.],\n    [227.,  58.]], dtype=float32), array([[416.,  36.],\n    [535.,  38.],\n    [535.,  61.],\n    [415.,  58.]], dtype=float32), array([[35., 73.],\n    [78., 73.],\n    [78., 92.],\n    [35., 92.]], dtype=float32), array([[287.,  73.],\n    [328.,  73.],\n    [328.,  92.],\n    [287.,  92.]], dtype=float32), array([[453.,  72.],\n    [495.,  72.],\n    [495.,  94.],\n    [453.,  94.]], dtype=float32), array([[ 17., 103.],\n    [ 94., 103.],\n    [ 94., 118.],\n    [ 17., 118.]], dtype=float32), array([[145., 104.],\n    [178., 104.],\n    [178., 118.],\n    [145., 118.]], dtype=float32), array([[277., 104.],\n    [337., 102.],\n    [338., 118.],\n    [278., 118.]], dtype=float32), array([[446., 102.],\n    [504., 104.],\n    [503., 118.],\n    [445., 118.]], dtype=float32)], 'rec_text': ['Dres', '\u200b\u8fde\u7eed\u200b\u5de5\u4f5c\u200b3', '\u200b\u53d6\u51fa\u200b\u6765\u200b\u653e\u5728\u200b\u7f51\u4e0a\u200b\uff0c\u200b\u6ca1\u200b\u60f3\u200b', '\u200b\u6c5f\u200b\u3001\u200b\u6574\u6c5f\u200b\u7b49\u200b\u516b\u5927\u200b', 'Abstr', 'rSrivi', '$709.', 'cludingGiv', '2.72', 'Ingcubic', '$744.78'], 'rec_score': [0.9934158325195312, 0.9990204572677612, 0.9967061877250671, 0.9375461935997009, 0.9947397112846375, 0.9972746968269348, 0.9904290437698364, 0.973427414894104, 0.9983080625534058, 0.993423342704773, 0.9964120984077454], 'input_path': 'table_recognition.jpg'}, 'table_result': [{'input_path': 'table_recognition.jpg', 'layout_bbox': [0, 21, 551, 118], 'bbox': array([[  4.395736 ,  25.238262 , 113.31014  ,  25.316246 , 115.454315 ,\n        71.8867   ,   3.7177477,  71.7937   ],\n    [110.727455 ,  25.94007  , 210.07187  ,  26.028755 , 209.66394  ,\n        65.96484  , 109.59861  ,  66.09809  ],\n    [214.45381  ,  26.027939 , 407.95276  ,  26.112846 , 409.6684   ,\n        66.91336  , 215.27292  ,  67.002014 ],\n    [402.81863  ,  26.123789 , 549.03656  ,  26.231564 , 549.19995  ,\n        66.88339  , 404.48068  ,  66.74034  ],\n    [  2.4458022,  64.68588  , 102.7665   ,  65.10228  , 105.79447  ,\n        96.051254 ,   2.5367072,  95.35514  ],\n    [108.85877  ,  65.80549  , 211.70216  ,  66.02091  , 210.79245  ,\n        94.75581  , 107.59308  ,  94.42664  ],\n    [217.05621  ,  64.98496  , 407.76328  ,  65.133484 , 406.8436   ,\n        96.00133  , 214.67896  ,  95.87226  ],\n    [401.73572  ,  64.60494  , 547.9967   ,  64.73921  , 548.19135  ,\n        96.09901  , 402.26733  ,  95.95529  ],\n    [  2.4882016,  93.589554 , 107.01325  ,  93.67592  , 107.8446   ,\n        120.13259  ,   2.508764 , 119.85027  ],\n    [110.773125 ,  93.98633  , 213.354    ,  94.08046  , 212.46033  ,\n        120.80207  , 109.29008  , 120.613045 ],\n    [216.08781  ,  94.19984  , 405.843    ,  94.28341  , 405.9974   ,\n        121.33152  , 215.10301  , 121.299034 ],\n    [403.92212  ,  94.44883  , 548.30963  ,  94.54982  , 548.4949   ,\n        122.610176 , 404.53433  , 122.49881  ]], dtype=float32), 'img_idx': 0, 'html': '&lt;html&gt;&lt;body&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;Dres&lt;/td&gt;&lt;td&gt;\u200b\u8fde\u7eed\u200b\u5de5\u4f5c\u200b3&lt;/td&gt;&lt;td&gt;\u200b\u53d6\u51fa\u200b\u6765\u200b\u653e\u5728\u200b\u7f51\u4e0a\u200b\uff0c\u200b\u6ca1\u200b\u60f3\u200b&lt;/td&gt;&lt;td&gt;\u200b\u6c5f\u200b\u3001\u200b\u6574\u6c5f\u200b\u7b49\u200b\u516b\u5927\u200b&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Abstr&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;rSrivi&lt;/td&gt;&lt;td&gt;$709.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;cludingGiv&lt;/td&gt;&lt;td&gt;2.72&lt;/td&gt;&lt;td&gt;Ingcubic&lt;/td&gt;&lt;td&gt;$744.78&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;'}]}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline layout_parsing --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/demo_paper.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result <pre><code>{'input_path': PosixPath('/root/.paddlex/temp/tmp5jmloefs.png'), 'parsing_result': [{'input_path': PosixPath('/root/.paddlex/temp/tmpshsq8_w0.png'), 'layout_bbox': [51.46833, 74.22329, 542.4082, 232.77504], 'image': {'img': array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [213, 221, 238],\n        [217, 223, 240],\n        [233, 234, 241]],\n\n    [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8), 'image_text': ''}, 'layout': 'single'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpcd2q9uyu.png'), 'layout_bbox': [47.68295, 243.08054, 546.28253, 295.71045], 'figure_title': 'Overview of RT-DETR, We feed th', 'layout': 'single'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpr_iqa8b3.png'), 'layout_bbox': [58.416977, 304.1531, 275.9134, 400.07513], 'image': {'img': array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8), 'image_text': ''}, 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmphpblxl3p.png'), 'layout_bbox': [100.62961, 405.97458, 234.79774, 414.77414], 'figure_title': 'Figure 5. The fusion block in CCFF.', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmplgnczrsf.png'), 'layout_bbox': [47.81724, 421.9041, 288.01566, 550.538], 'text': 'D, Ds, not only significantly reduces latency (35% faster),\\nRut\\nnproves accuracy (0.4% AP higher), CCFF is opti\\nased on the cross-scale fusion module, which\\nnsisting of convolutional lavers intc\\npath.\\nThe role of the fusion block is t\\n into a new feature, and its\\nFigure 5. The f\\nblock contains tw\\n1 x1\\nchannels, /V RepBlock\\n. anc\\n: two-path outputs are fused by element-wise add. We\\ntormulate the calculation ot the hvbrid encoder as:', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpsq0ey9md.png'), 'layout_bbox': [94.60716, 558.703, 288.04193, 600.19434], 'formula': '\\\\begin{array}{l}{{\\\\Theta=K=\\\\mathrm{p.s.sp{\\\\pm}}\\\\mathrm{i.s.s.}(\\\\mathrm{l.s.}(\\\\mathrm{l.s.}(\\\\mathrm{l.s.}}),{\\\\qquad\\\\mathrm{{a.s.}}\\\\mathrm{s.}}}\\\\\\\\ {{\\\\tau_{\\\\mathrm{{s.s.s.s.s.}}(\\\\mathrm{l.s.},\\\\mathrm{l.s.},\\\\mathrm{s.s.}}\\\\mathrm{s.}\\\\mathrm{s.}}\\\\end{array}),}}\\\\\\\\ {{\\\\bar{\\\\mathrm{e-c.c.s.s.}(\\\\mathrm{s.},\\\\mathrm{s.s.},\\\\ s_{s}}\\\\mathrm{s.s.},\\\\tau),}}\\\\end{array}', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpv30qy0v4.png'), 'layout_bbox': [47.975555, 607.12024, 288.5776, 629.1252], 'text': 'tened feature to the same shape as Ss.\\nwhere Re shape represents restoring the shape of the flat-', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp0jejzwwv.png'), 'layout_bbox': [48.383354, 637.581, 245.96404, 648.20496], 'paragraph_title': '4.3. Uncertainty-minimal Query Selection', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpushex416.png'), 'layout_bbox': [47.80134, 656.002, 288.50192, 713.24994], 'text': 'To reduce the difficulty of optimizing object queries in\\nDETR, several subsequent works [42, 44, 45] propose query\\nselection schemes, which have in common that they use the\\nconfidence score to select the top K\u2019 features from the en-\\ncoder to initialize object queries (or just position queries).', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpki7e_6wc.png'), 'layout_bbox': [306.6371, 302.1026, 546.3772, 419.76724], 'text': 'The confidence score represents the likelihood that the fea\\nture includes foreground objects. Nevertheless, the \\nare required to simultaneously model the category\\nojects, both of which determine the quality of the\\npertor\\ncore of the fes\\nBased on the analysis, the current query\\n considerable level of uncertainty in the\\nresulting in sub-optimal initialization for\\nand hindering the performance of the detector.', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmppbxrfehp.png'), 'layout_bbox': [306.0642, 422.7347, 546.9216, 539.45734], 'text': 'To address this problem, we propose the uncertainty mini\\nmal query selection scheme, which explicitly const\\noptim\\n the epistemic uncertainty to model the\\nfeatures, thereby providing \\nhigh-quality\\nr the decoder. Specifically,\\n the discrepancy between i\\nalization P\\nand classificat\\n.(2\\ntunction for the gradie', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp1mgiyd21.png'), 'layout_bbox': [331.52808, 549.32635, 546.5229, 586.15546], 'formula': '\\\\begin{array}{c c c}{{}}&amp;{{}}&amp;{{\\\\begin{array}{c}{{i\\\\langle X\\\\rangle=({\\\\bar{Y}}({\\\\bar{X}})+{\\\\bar{Z}}({\\\\bar{X}})\\\\mid X\\\\in{\\\\bar{\\\\pi}}^{\\\\prime}}}&amp;{{}}\\\\\\\\ {{}}&amp;{{}}&amp;{{}}\\\\end{array}}}&amp;{{\\\\emptyset}}\\\\\\\\ {{}}&amp;{{}}&amp;{{C(\\\\bar{X},{\\\\bar{X}})=C..\\\\scriptstyle(\\\\bar{0},{\\\\bar{Y}})+{\\\\mathcal{L}}_{{\\\\mathrm{s}}}({\\\\bar{X}}),\\\\ 6)}}&amp;{{}}\\\\end{array}', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp8t73dpym.png'), 'layout_bbox': [306.44016, 592.8762, 546.84314, 630.60126], 'text': 'where  and y denote the prediction and ground truth,\\n= (c, b), c and b represent the category and bounding\\nbox respectively, X represent the encoder feature.', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpftnxeyjm.png'), 'layout_bbox': [306.15652, 632.3142, 546.2463, 713.19073], 'text': 'Effectiveness analysis. To analyze the effectiveness of the\\nuncertainty-minimal query selection, we visualize the clas-\\nsificatior\\nscores and IoU scores of the selected fe\\nCOCO\\na 12017, Figure 6. We draw the scatterplo\\nt with\\ndots\\nrepresent the selected features from the model trained\\nwith uncertainty-minimal query selection and vanilla query', 'layout': 'right'}]}\n</code></pre> <pre><code>paddlex --pipeline formula_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/demo_image/general_formula_recognition.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_formula_recognition.png', 'layout_result': {'input_path': '/root/.paddlex/predict_input/general_formula_recognition.png', 'boxes': [{'cls_id': 3, 'label': 'number', 'score': 0.7580855488777161, 'coordinate': [1028.3635, 205.46213, 1038.953, 222.99033]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8882032632827759, 'coordinate': [272.75305, 204.50894, 433.7473, 226.17996]}, {'cls_id': 2, 'label': 'text', 'score': 0.9685840606689453, 'coordinate': [272.75928, 282.17773, 1041.9316, 374.44687]}, {'cls_id': 2, 'label': 'text', 'score': 0.9559416770935059, 'coordinate': [272.39056, 385.54114, 1044.1521, 443.8598]}, {'cls_id': 2, 'label': 'text', 'score': 0.9610629081726074, 'coordinate': [272.40817, 467.2738, 1045.1033, 563.4855]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8916195034980774, 'coordinate': [503.45743, 594.6236, 1040.6804, 619.73895]}, {'cls_id': 2, 'label': 'text', 'score': 0.973675549030304, 'coordinate': [272.32007, 648.8599, 1040.8702, 775.15686]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9038916230201721, 'coordinate': [554.2307, 803.5825, 1040.4657, 855.3159]}, {'cls_id': 2, 'label': 'text', 'score': 0.9025381803512573, 'coordinate': [272.535, 875.1402, 573.1086, 898.3587]}, {'cls_id': 2, 'label': 'text', 'score': 0.8336610794067383, 'coordinate': [317.48013, 909.60864, 966.8498, 933.7868]}, {'cls_id': 2, 'label': 'text', 'score': 0.8779091238975525, 'coordinate': [19.704018, 653.322, 72.433235, 1215.1992]}, {'cls_id': 2, 'label': 'text', 'score': 0.8832409977912903, 'coordinate': [272.13028, 958.50806, 1039.7928, 1019.476]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9088466167449951, 'coordinate': [517.1226, 1042.3978, 1040.2208, 1095.7457]}, {'cls_id': 2, 'label': 'text', 'score': 0.9587949514389038, 'coordinate': [272.03336, 1112.9269, 1041.0201, 1206.8417]}, {'cls_id': 2, 'label': 'text', 'score': 0.8885666131973267, 'coordinate': [271.7495, 1231.8752, 710.44495, 1255.7981]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8907185196876526, 'coordinate': [581.2295, 1287.4525, 1039.8014, 1312.772]}, {'cls_id': 2, 'label': 'text', 'score': 0.9559596180915833, 'coordinate': [273.1827, 1341.421, 1041.0299, 1401.7255]}, {'cls_id': 2, 'label': 'text', 'score': 0.875311553478241, 'coordinate': [272.8338, 1427.3711, 789.7108, 1451.1359]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9152213931083679, 'coordinate': [524.9582, 1474.8136, 1041.6333, 1530.7142]}, {'cls_id': 2, 'label': 'text', 'score': 0.9584835767745972, 'coordinate': [272.81665, 1549.524, 1042.9962, 1608.7157]}]}, 'ocr_result': {}, 'table_result': [], 'dt_polys': [array([[ 503.45743,  594.6236 ],\n    [1040.6804 ,  594.6236 ],\n    [1040.6804 ,  619.73895],\n    [ 503.45743,  619.73895]], dtype=float32), array([[ 554.2307,  803.5825],\n    [1040.4657,  803.5825],\n    [1040.4657,  855.3159],\n    [ 554.2307,  855.3159]], dtype=float32), array([[ 517.1226, 1042.3978],\n    [1040.2208, 1042.3978],\n    [1040.2208, 1095.7457],\n    [ 517.1226, 1095.7457]], dtype=float32), array([[ 581.2295, 1287.4525],\n    [1039.8014, 1287.4525],\n    [1039.8014, 1312.772 ],\n    [ 581.2295, 1312.772 ]], dtype=float32), array([[ 524.9582, 1474.8136],\n    [1041.6333, 1474.8136],\n    [1041.6333, 1530.7142],\n    [ 524.9582, 1530.7142]], dtype=float32)], 'rec_formula': ['F({\\bf x})=C(F_{1}(x_{1}),\\cdot\\cdot\\cdot,F_{N}(x_{N})).\\qquad\\qquad\\qquad(1)', 'p(\\mathbf{x})=c(\\mathbf{u})\\prod_{i}p(x_{i}).\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\quad~~\\quad~~~~~~~~~~~~~~~(2)', 'H_{c}({\\bf x})=-\\int_{{\\bf{u}}}c({\\bf{u}})\\log c({\\bf{u}})d{\\bf{u}}.~~~~~~~~~~~~~~~~~~~~~(3)', 'I({\\bf x})=-H_{c}({\\bf x}).\\qquad\\qquad\\qquad\\qquad(4)', 'H({\\bf x})=\\sum_{i}H(x_{i})+H_{c}({\\bf x}).\\eqno\\qquad\\qquad\\qquad(5)']}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline seal_recognition --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': PosixPath('/root/.paddlex/temp/tmpa8eqnpus.png'), 'layout_result': {'input_path': PosixPath('/root/.paddlex/temp/tmpa8eqnpus.png'), 'boxes': [{'cls_id': 2, 'label': 'seal', 'score': 0.9813321828842163, 'coordinate': [0, 5.1820183, 639.59314, 637.7533]}]}, 'ocr_result': {'dt_polys': [array([[166, 468],\n        [206, 503],\n    [249, 523],\n    [312, 535],\n    [364, 529],\n    [390, 521],\n    [428, 505],\n    [465, 476],\n    [468, 474],\n    [473, 474],\n    [476, 475],\n    [478, 477],\n    [508, 507],\n    [510, 510],\n    [511, 514],\n    [509, 518],\n    [507, 521],\n    [458, 559],\n    [455, 560],\n    [399, 584],\n    [399, 584],\n    [369, 591],\n    [367, 592],\n    [308, 597],\n    [305, 596],\n    [240, 584],\n    [239, 584],\n    [220, 577],\n    [169, 552],\n    [166, 551],\n    [120, 510],\n    [117, 507],\n    [116, 503],\n    [117, 499],\n    [121, 495],\n    [153, 468],\n    [156, 467],\n    [161, 467]]), array([[439, 444],\n    [443, 444],\n    [446, 446],\n    [448, 448],\n    [450, 451],\n    [450, 454],\n    [448, 498],\n    [448, 502],\n    [445, 505],\n    [442, 507],\n    [439, 507],\n    [399, 505],\n    [196, 506],\n    [192, 505],\n    [189, 503],\n    [187, 500],\n    [187, 497],\n    [186, 458],\n    [186, 456],\n    [187, 451],\n    [188, 448],\n    [192, 444],\n    [194, 444],\n    [198, 443]]), array([[463, 347],\n    [468, 347],\n    [472, 350],\n    [474, 353],\n    [476, 360],\n    [477, 425],\n    [476, 429],\n    [474, 433],\n    [470, 436],\n    [466, 438],\n    [463, 438],\n    [175, 439],\n    [170, 438],\n    [166, 435],\n    [163, 432],\n    [161, 426],\n    [161, 361],\n    [161, 356],\n    [163, 352],\n    [167, 349],\n    [172, 347],\n    [184, 346],\n    [186, 346]]), array([[325,  38],\n    [485,  91],\n    [489,  94],\n    [493,  96],\n    [587, 225],\n    [588, 230],\n    [589, 234],\n    [592, 384],\n    [591, 389],\n    [588, 393],\n    [585, 397],\n    [581, 399],\n    [576, 399],\n    [572, 398],\n    [508, 380],\n    [503, 379],\n    [499, 375],\n    [498, 370],\n    [497, 367],\n    [493, 258],\n    [428, 171],\n    [421, 165],\n    [323, 136],\n    [225, 165],\n    [207, 175],\n    [144, 260],\n    [141, 365],\n    [141, 370],\n    [138, 374],\n    [134, 378],\n    [131, 379],\n    [ 66, 398],\n    [ 61, 398],\n    [ 56, 398],\n    [ 52, 395],\n    [ 48, 391],\n    [ 47, 386],\n    [ 47, 384],\n    [ 47, 235],\n    [ 48, 230],\n    [ 50, 226],\n    [146,  96],\n    [151,  92],\n    [154,  91],\n    [315,  38],\n    [320,  37]])], 'dt_scores': [0.99375725701319, 0.9871711582010613, 0.9937523531067023, 0.9911629231838204], 'rec_text': ['5263647368706', '\u200b\u5417\u200b\u7e41\u7269\u200b', '\u200b\u53d1\u7968\u200b\u4e13\u200b\u5929\u6d25\u200b\u541b\u548c\u7f18\u200b\u5546\u8d38\u200b\u6709\u9650\u516c\u53f8\u200b'], 'rec_score': [0.9933745265007019, 0.998288631439209, 0.9999362230300903, 0.9923253655433655], 'input_path': PosixPath('/Users/chenghong0temp/tmpa8eqnpus.png')}, 'src_file_name': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png', 'page_id': 0}\n</code></pre> <p><p></p></p> <p>Computer VisionCLI</p> Image ClassificationObject DetectionInstance SegmentationSemantic SegmentationImage Multi-label ClassificationSmall Object DetectionImage Anomaly Detection <pre><code>paddlex --pipeline image_classification --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_image_classification_001.jpg', 'class_ids': [296, 170, 356, 258, 248], 'scores': [0.62736, 0.03752, 0.03256, 0.0323, 0.03194], 'label_names': ['ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 'Irish wolfhound', 'weasel', 'Samoyed, Samoyede', 'Eskimo dog, husky']}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline object_detection --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_object_detection_002.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_object_detection_002.png', 'boxes': [{'cls_id': 49, 'label': 'orange', 'score': 0.8188097476959229, 'coordinate': [661, 93, 870, 305]}, {'cls_id': 47, 'label': 'apple', 'score': 0.7743489146232605, 'coordinate': [76, 274, 330, 520]}, {'cls_id': 47, 'label': 'apple', 'score': 0.7270504236221313, 'coordinate': [285, 94, 469, 297]}, {'cls_id': 46, 'label': 'banana', 'score': 0.5570532083511353, 'coordinate': [310, 361, 685, 712]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5484835505485535, 'coordinate': [764, 285, 924, 440]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5160726308822632, 'coordinate': [853, 169, 987, 303]}, {'cls_id': 60, 'label': 'dining table', 'score': 0.5142655968666077, 'coordinate': [0, 0, 1072, 720]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5101479291915894, 'coordinate': [57, 23, 213, 176]}]}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline instance_segmentation --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_instance_segmentation_004.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_instance_segmentation_004.png', 'boxes': [{'cls_id': 0, 'label': 'person', 'score': 0.8698326945304871, 'coordinate': [339, 0, 639, 575]}, {'cls_id': 0, 'label': 'person', 'score': 0.8571141362190247, 'coordinate': [0, 0, 195, 575]}, {'cls_id': 0, 'label': 'person', 'score': 0.8202633857727051, 'coordinate': [88, 113, 401, 574]}, {'cls_id': 0, 'label': 'person', 'score': 0.7108577489852905, 'coordinate': [522, 21, 767, 574]}, {'cls_id': 27, 'label': 'tie', 'score': 0.554280698299408, 'coordinate': [247, 311, 355, 574]}]}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline semantic_segmentation --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/semantic_segmentation/makassaridn-road_demo.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/makassaridn-road_demo.png', 'pred': '...'}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline multi_label_image_classification --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/general_image_classification_001.jpg', 'class_ids': [21, 0, 30, 24], 'scores': [0.99257, 0.70596, 0.63001, 0.57852], 'label_names': ['bear', 'person', 'skis', 'backpack']}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline small_object_detection --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/small_object_detection.jpg --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/small_object_detection.jpg', 'boxes': [{'cls_id': 3, 'label': 'car', 'score': 0.9243856072425842, 'coordinate': [624, 638, 682, 741]}, {'cls_id': 3, 'label': 'car', 'score': 0.9206348061561584, 'coordinate': [242, 561, 356, 613]}, {'cls_id': 3, 'label': 'car', 'score': 0.9194547533988953, 'coordinate': [670, 367, 705, 400]}, {'cls_id': 3, 'label': 'car', 'score': 0.9162291288375854, 'coordinate': [459, 259, 523, 283]}, {'cls_id': 4, 'label': 'van', 'score': 0.9075379371643066, 'coordinate': [467, 213, 498, 242]}, {'cls_id': 4, 'label': 'van', 'score': 0.9066920876502991, 'coordinate': [547, 351, 577, 397]}, {'cls_id': 3, 'label': 'car', 'score': 0.9041045308113098, 'coordinate': [502, 632, 562, 736]}, {'cls_id': 3, 'label': 'car', 'score': 0.8934890627861023, 'coordinate': [613, 383, 647, 427]}, {'cls_id': 3, 'label': 'car', 'score': 0.8803309202194214, 'coordinate': [640, 280, 671, 309]}, {'cls_id': 3, 'label': 'car', 'score': 0.8727016448974609, 'coordinate': [1199, 256, 1259, 281]}, {'cls_id': 3, 'label': 'car', 'score': 0.8705748915672302, 'coordinate': [534, 410, 570, 461]}, {'cls_id': 3, 'label': 'car', 'score': 0.8654043078422546, 'coordinate': [669, 248, 702, 271]}, {'cls_id': 3, 'label': 'car', 'score': 0.8555219769477844, 'coordinate': [525, 243, 550, 270]}, {'cls_id': 3, 'label': 'car', 'score': 0.8522038459777832, 'coordinate': [526, 220, 553, 243]}, {'cls_id': 3, 'label': 'car', 'score': 0.8392605185508728, 'coordinate': [557, 141, 575, 158]}, {'cls_id': 3, 'label': 'car', 'score': 0.8353804349899292, 'coordinate': [537, 120, 553, 133]}, {'cls_id': 3, 'label': 'car', 'score': 0.8322211503982544, 'coordinate': [585, 132, 603, 147]}, {'cls_id': 3, 'label': 'car', 'score': 0.8298957943916321, 'coordinate': [701, 283, 736, 313]}, {'cls_id': 3, 'label': 'car', 'score': 0.8217393159866333, 'coordinate': [885, 347, 943, 377]}, {'cls_id': 3, 'label': 'car', 'score': 0.820313572883606, 'coordinate': [493, 150, 511, 168]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.8183429837226868, 'coordinate': [203, 701, 224, 743]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.815082848072052, 'coordinate': [185, 710, 201, 744]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.7892289757728577, 'coordinate': [311, 371, 344, 407]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.7812919020652771, 'coordinate': [345, 380, 388, 405]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.7748346328735352, 'coordinate': [295, 500, 309, 532]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.7688500285148621, 'coordinate': [851, 436, 863, 466]}, {'cls_id': 3, 'label': 'car', 'score': 0.7466475367546082, 'coordinate': [565, 114, 580, 128]}, {'cls_id': 3, 'label': 'car', 'score': 0.7156463265419006, 'coordinate': [483, 66, 495, 78]}, {'cls_id': 3, 'label': 'car', 'score': 0.704211950302124, 'coordinate': [607, 138, 642, 152]}, {'cls_id': 3, 'label': 'car', 'score': 0.7021926045417786, 'coordinate': [505, 72, 518, 83]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.6897469162940979, 'coordinate': [802, 460, 815, 488]}, {'cls_id': 3, 'label': 'car', 'score': 0.671891450881958, 'coordinate': [574, 123, 593, 136]}, {'cls_id': 9, 'label': 'motorcycle', 'score': 0.6712754368782043, 'coordinate': [445, 317, 472, 334]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.6695684790611267, 'coordinate': [479, 309, 489, 332]}, {'cls_id': 3, 'label': 'car', 'score': 0.6273623704910278, 'coordinate': [654, 210, 677, 234]}, {'cls_id': 3, 'label': 'car', 'score': 0.6070230603218079, 'coordinate': [640, 166, 667, 185]}, {'cls_id': 3, 'label': 'car', 'score': 0.6064521670341492, 'coordinate': [461, 59, 476, 71]}, {'cls_id': 3, 'label': 'car', 'score': 0.5860581398010254, 'coordinate': [464, 87, 484, 100]}, {'cls_id': 9, 'label': 'motorcycle', 'score': 0.5792551636695862, 'coordinate': [390, 390, 419, 408]}, {'cls_id': 3, 'label': 'car', 'score': 0.5559225678443909, 'coordinate': [481, 125, 496, 140]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.5531904697418213, 'coordinate': [869, 306, 880, 331]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.5468509793281555, 'coordinate': [895, 294, 904, 319]}, {'cls_id': 3, 'label': 'car', 'score': 0.5451828241348267, 'coordinate': [505, 94, 518, 108]}, {'cls_id': 3, 'label': 'car', 'score': 0.5398445725440979, 'coordinate': [657, 188, 681, 208]}, {'cls_id': 4, 'label': 'van', 'score': 0.5318890810012817, 'coordinate': [518, 88, 534, 102]}, {'cls_id': 3, 'label': 'car', 'score': 0.5296525359153748, 'coordinate': [527, 71, 540, 81]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.5168400406837463, 'coordinate': [528, 320, 563, 346]}, {'cls_id': 3, 'label': 'car', 'score': 0.5088561177253723, 'coordinate': [511, 84, 530, 95]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.502006471157074, 'coordinate': [841, 266, 850, 283]}]}\n</code></pre> <p><p></p></p> <pre><code>paddlex --pipeline anomaly_detection --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_grid.png --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results and visualized images will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result outputimg <pre><code>{'input_path': '/root/.paddlex/predict_input/uad_grid.png', 'pred': '...'}\n</code></pre> <p><p></p></p> <p>Time Series-relatedCLI</p> Time Series ForecastingTime Series Anomaly DetectionTime Series Classification <pre><code>paddlex --pipeline ts_fc --input https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_fc.csv --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result <pre><code>{'input_path': 'ts_fc.csv', 'forecast':                            OT\ndate\n2018-06-26 20:00:00  9.586131\n2018-06-26 21:00:00  9.379762\n2018-06-26 22:00:00  9.252275\n2018-06-26 23:00:00  9.249993\n2018-06-27 00:00:00  9.164998\n...                       ...\n2018-06-30 15:00:00  8.830340\n2018-06-30 16:00:00  9.291553\n2018-06-30 17:00:00  9.097666\n2018-06-30 18:00:00  8.905430\n2018-06-30 19:00:00  8.993793\n\n[96 rows x 1 columns]}\n</code></pre> <pre><code>paddlex --pipeline ts_ad --input https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_ad.csv --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result <pre><code>{'input_path': 'ts_ad.csv', 'anomaly':            label\ntimestamp\n220226         0\n220227         0\n220228         0\n220229         0\n220230         0\n...          ...\n220317         1\n220318         1\n220319         1\n220320         1\n220321         0\n\n[96 rows x 1 columns]}\n</code></pre> <pre><code>paddlex --pipeline ts_cls --input https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_cls.csv --device gpu:0 --save_path output\n</code></pre> <ul> <li>After running, the output results will be saved in the <code>output</code> folder. You can also customize the save path by modifying the <code>--save_path</code> parameter.</li> </ul> What's the result <pre><code>{'input_path': 'ts_cls.csv', 'classification':         classid     score\nsample\n0             0  0.617688}\n</code></pre>"},{"location":"en/index.html#python-usage","title":"\ud83d\udcdd Python Usage","text":"<p>A few lines of code can complete the quick inference of the production line, with a unified Python script format as follows:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=[production line name])\noutput = pipeline.predict([input image name])\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> The following steps were executed:</p> <ul> <li><code>create_pipeline()</code> instantiates the production line object</li> <li>Pass in the image and call the <code>predict</code> method of the production line object for inference prediction</li> <li>Process the prediction results</li> </ul> <p>OCR-related Python</p> OCRTable RecognitionLayout ParsingFormula RecognitionSeal Text Recognition <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"OCR\")\n\noutput = pipeline.predict(\"general_ocr_002.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"table_recognition\")\n\noutput = pipeline.predict(\"table_recognition.jpg\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_xlsx(\"./output/\")\n    res.save_to_html(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"layout_parsing\")\n\noutput = pipeline.predict(\"demo_paper.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_xlsx(\"./output/\")\n    res.save_to_html(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"formula_recognition\")\n\noutput = pipeline.predict(\"general_formula_recognition.png\")\nfor res in output:\n    res.print()\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"seal_recognition\")\n\noutput = pipeline.predict(\"seal_text_det.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> <p>Computer Vision Python</p> Image ClassificationObject DetectionInstance SegmentationSemantic SegmentationImage Multi-label ClassificationSmall Object DetectionImage Anomaly Detection <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"image_classification\")\n\noutput = pipeline.predict(\"general_image_classification_001.jpg\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"object_detection\")\n\noutput = pipeline.predict(\"general_object_detection_002.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"instance_segmentation\")\n\noutput = pipeline.predict(\"general_instance_segmentation_004.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"semantic_segmentation\")\n\noutput = pipeline.predict(\"makassaridn-road_demo.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"multi_label_image_classification\")\n\noutput = pipeline.predict(\"general_image_classification_001.jpg\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"small_object_detection\")\n\noutput = pipeline.predict(\"small_object_detection.jpg\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"anomaly_detection\")\n\noutput = pipeline.predict(\"uad_grid.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <p>Time Series-related Python</p> Time Series ForecastingTime Series Anomaly DetectionTime Series Classification <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"ts_fc\")\n\noutput = pipeline.predict(\"ts_fc.csv\")\nfor res in output:\n    res.print()\n    res.save_to_csv(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/ts_ad.yaml\")\noutput = pipeline.predict(\"ts_ad.cs\")\nfor res in output:\n    res.print()\n    res.save_to_csv(\"./output/\")\n</code></pre> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"ts_cls\")\n\noutput = pipeline.predict(\"ts_cls.csv\")\nfor res in output:\n    res.print()\n    res.save_to_csv(\"./output/\")\n</code></pre>"},{"location":"en/index.html#detailed-tutorials","title":"\ud83d\ude80 Detailed Tutorials","text":"<ul> <li> <p>Document Information Extraction</p> <p>Document scene information extraction v3 (PP-ChatOCRv3) is a document and image intelligent analysis solution with PaddlePaddle features, combining LLM and OCR technologies to solve complex document information extraction challenges such as layout analysis, rare character recognition, multi-page PDF, table, and seal recognition in one stop.</p> <p> Tutorial</p> </li> <li> <p>OCR</p> <p>The general OCR production line is used to solve text recognition tasks, extract text information from images, and output it in text form. Based on the end-to-end OCR system, it can achieve millisecond-level precise text content prediction on CPUs and reach open-source SOTA in general scenarios.</p> <p> Tutorial</p> </li> <li> <p>Image Classification</p> <p>Image classification can automatically extract image features and classify them accurately, recognizing various objects such as animals, plants, traffic signs, etc., and is widely used in object recognition, scene understanding, and automatic tagging fields.</p> <p> Tutorial</p> </li> <li> <p>Object Detection</p> <p>Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. This technology is widely used in fields such as autonomous driving, surveillance systems, and smart photo albums.</p> <p> Tutorial</p> </li> <li> <p>Small Object Detection</p> <p>Small object detection is a technology specifically designed to recognize smaller objects in images, widely used in surveillance, unmanned driving, and satellite image analysis fields. It can accurately locate and classify small-sized objects such as pedestrians, traffic signs, or small animals from complex scenes.</p> <p> Tutorial</p> </li> <li> <p>Time Series Forecasting</p> <p>Time series forecasting is a technique that uses historical data to predict future trends by analyzing the patterns of change in time series data. It is widely used in financial markets, weather forecasting, and sales forecasting fields.</p> <p> Tutorial</p> </li> </ul> <p> More</p>"},{"location":"en/index.html#discussion","title":"\ud83d\udcac Discussion","text":"<p>We warmly welcome and encourage community members to raise questions, share ideas, and feedback in the Discussions section. Whether you want to report a bug, discuss a feature request, seek help, or just want to keep up with the latest project news, this is a great platform.</p>"},{"location":"en/CHANGLOG.html","title":"Version Update Information","text":""},{"location":"en/CHANGLOG.html#latest-version-information","title":"Latest Version Information","text":""},{"location":"en/CHANGLOG.html#paddlex-v300beta2-11152024","title":"PaddleX v3.0.0beta2 (11.15/2024)","text":"<p>PaddleX 3.0 Beta2 is fully compatible with the PaddlePaddle 3.0b2 version. This update introduces new pipelines for general image recognition, face recognition, vehicle attribute recognition, and pedestrian attribute recognition. We have also developed 42 new models to fully support the Ascend 910B, with extensive documentation available on GitHub Pages. Here\u2019s a detailed look at the new features and improvements:</p> <ul> <li>New Pipelines:</li> <li>General Image Recognition: Introducing a powerful pipeline with enhanced feature extraction models. This allows for user-defined image database recognition of unknown categories, providing more customizable recognition options compared to existing open-domain object detection. Supports high-performance inference and service deployment.</li> <li>Face Recognition: This new pipeline enables the addition and removal of entries in the face database, with robust support for high-performance inference and service deployment.</li> <li>Vehicle Attribute Recognition: Detect and recognize vehicle attributes such as color and model in images. Supports high-performance inference and service deployment.</li> <li> <p>Pedestrian Attribute Recognition: Detect and recognize pedestrian attributes such as age, gender, and clothing in images. Supports high-performance inference and service deployment.</p> </li> <li> <p>New Capabilities:</p> </li> <li>Documentation Support: Comprehensive GitHub Pages is available, featuring search functionality and user commentary.</li> <li>Benchmarking: Print model inference benchmark information with related documentation.</li> <li> <p>Model Development: We have introduced 42 new model development processes specifically adapted for Ascend 910B. Check out the model list.</p> </li> <li> <p>Optimizations:</p> </li> <li>Formula Recognition: Now supports PDF input and visualization of results.</li> <li>Seal Recognition: Supports PDF format input.</li> <li>Layout Parsing: Improved the naming convention for saved images.</li> <li>Pretrained Models: Unified management of pretrained models, integrated into the default configuration file.</li> <li>Model Saving: Upgraded format to ensure high-performance inference.</li> <li> <p>Parameter Tuning: Optimized default parameters for certain models to enable higher accuracy training.</p> </li> <li> <p>Bug Fixes:</p> </li> <li>Addressed inaccuracies and inappropriate expressions in documentation, and fixed some invalid URLs.</li> <li>Resolved issues with the PP-LCNet_x1_0_doc_ori inference model.</li> <li>Fixed bugs related to high-performance inference and service deployment.</li> <li>Corrected training configuration issues in SLANet and SLANet_plus that led to low accuracy.</li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v300beta1-9302024","title":"PaddleX v3.0.0beta1 (9.30/2024)","text":"<p>PaddleX 3.0 Beta1 offers over 200 models accessible through a streamlined Python API for one-click deployment; realizes full-process model development based on unified commands, and opens source the foundational capabilities of the PP-ChatOCRv3 special model pipeline; supports high-performance inference and service-oriented deployment for over 100 models, as well as edge deployment for 7 key vision models; and fully adapts the development process of over 70 models to Huawei Ascend 910B, and over 15 models to XPU and MLU.</p> <ul> <li>Rich Models with One-click Deployment: Integrates over 200 PaddlePaddle models across key domains such as document image intelligent analysis, OCR, object detection, and time series prediction into 13 model pipelines, enabling rapid model experience through a streamlined Python API. Additionally, supports over 20 individual functional modules for convenient model combination.</li> <li>Enhanced Efficiency and Lowered Thresholds: Implements full-process model development based on a graphical interface and unified commands, creating 8 special model pipelines that combine large and small models, leverage large model semi-supervised learning, and multi-model fusion, significantly reducing iteration costs.</li> <li>Flexible Deployment Across Scenarios: Supports various deployment methods including high-performance, service-oriented, and edge deployment, ensuring efficient model operation and rapid response across different application scenarios.</li> <li>Efficient Support for Mainstream Hardware: Seamlessly switches between NVIDIA GPUs, XPU, Ascend, and MLU, ensuring efficient operation.</li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v300beta-6272024","title":"PaddleX v3.0.0beta (6.27/2024)","text":"<p>PaddleX 3.0beta integrates the advantages of the PaddlePaddle ecosystem, covering 7 major scenario tasks, constructs 16 model pipelines, and provides a low-code development mode to assist developers in realizing full-process model development on various mainstream hardware.</p> <ul> <li>Basic Model Pipelines (Rich Models, Comprehensive Scenarios): Selects 68 high-quality PaddlePaddle models, covering tasks such as image classification, object detection, image segmentation, OCR, text image layout analysis, and time series prediction.</li> <li>Special Model Pipelines (Significant Efficiency Improvement): Provides efficient solutions combining large and small models, large model semi-supervised learning, and multi-model fusion.</li> <li>Low-code Development Mode (Convenient Development and Deployment): Offers both zero-code and low-code development methods.</li> <li>Zero-code Development: Users can interactively submit background training tasks through a graphical user interface (GUI), bridging online and offline deployment, and supporting API-based online service invocation.</li> <li>Low-code Development: Achieves full-process development across 16 model pipelines through unified API interfaces, while supporting user-defined model process serialization.</li> <li>Multi-hardware Local Support (Strong Compatibility): Supports NVIDIA GPUs, XPU, Ascend, and MLU, enabling pure offline usage.</li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v210-12102021","title":"PaddleX v2.1.0 (12.10/2021)","text":"<p>Added the ultra-lightweight classification model PPLCNet, achieving approximately 5ms prediction speed for a single image on Intel CPUs, with a Top-1 Accuracy of 80.82% on the ImageNet-1K dataset, surpassing ResNet152's performance. Experience it now! Added the lightweight detection model PP-PicoDet, the first to surpass 30+ mAP(0.5:0.95) within 1M parameters (at 416px input), achieving up to 150FPS prediction on ARM CPUs. Experience it now! Upgraded PaddleX Restful API to support PaddlePaddle's dynamic graph development mode. Experience it now! Added negative sample training strategies for detection models. Experience it now! Added lightweight Python-based service deployment. Experience it now!</p>"},{"location":"en/CHANGLOG.html#paddlex-v200-9102021","title":"PaddleX v2.0.0 (9.10/2021)","text":"<ul> <li>PaddleX API</li> <li>Added visualization of prediction results for detection and instance segmentation tasks, as well as analysis of prediction errors to assist in model effect analysis</li> <li>Introduced negative sample optimization for detection tasks to suppress false detections in background regions</li> <li>Improved prediction results for semantic segmentation tasks, supporting the return of predicted categories and normalized prediction confidence</li> <li>Enhanced prediction results for image classification tasks, supporting the return of normalized prediction confidence</li> <li>Prediction Deployment</li> <li>Completed PaddleX Python prediction deployment, enabling rapid deployment with just 2 APIs</li> <li>Comprehensively upgraded PaddleX C++ deployment, supporting end-to-end unified deployment capabilities for PaddlePaddle vision suites including PaddleDetection, PaddleClas, PaddleSeg, and PaddleX</li> <li>Newly released Manufacture SDK, providing a pre-compiled PaddlePaddle deployment development kit (SDK) for industrial-grade multi-end and multi-platform deployment acceleration, enabling rapid inference deployment through configuring business logic flow files in a low-code manner</li> <li>PaddleX GUI</li> <li>Upgraded PaddleX GUI to support 30-series graphics cards</li> <li>Added PP-YOLO V2 model for object detection tasks, achieving 49.5% accuracy on the COCO test dataset and 68.9 FPS prediction speed on V100</li> <li>Introduced a 4.2MB ultra-lightweight model, PP-YOLO tiny, for object detection tasks</li> <li>Added real-time segmentation model BiSeNetV2 for semantic segmentation tasks</li> <li>Newly added the ability to export API training scripts for seamless switching to PaddleX API training</li> <li>Industrial Practice Cases</li> <li>Added tutorial cases for steel bar counting and defect detection, focusing on object detection tasks</li> <li>Added tutorial cases for robotic arm grasping, focusing on instance segmentation tasks</li> <li>Added tutorial cases for training and deployment of industrial meter readings, which combines object detection, semantic segmentation, and traditional vision algorithms</li> <li>Added a deployment case tutorial using C# language under Windows system</li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v200rc0-5192021","title":"PaddleX v2.0.0rc0 (5.19/2021)","text":"<ul> <li>Fully supports PaddlePaddle 2.0 dynamic graphs for an easier development mode</li> <li>Added PP-YOLOv2 for object detection tasks, achieving 49.5% accuracy on the COCO test dataset and 68.9 FPS prediction speed on V100</li> <li>Introduced a 4.2MB ultra-lightweight model, PP-YOLO tiny, for object detection tasks</li> <li>Added real-time segmentation model BiSeNetV2 for semantic segmentation tasks</li> <li>Comprehensive upgrade of C++ deployment module<ul> <li>PaddleInference deployment adapted to 2.0 prediction library (Usage Documentation)</li> <li>Supports deployment of models from PaddleDetection, PaddleSeg, PaddleClas, and PaddleX</li> <li>Added multi-GPU prediction based on PaddleInference (Usage Documentation)</li> <li>GPU deployment added TensorRT high-performance acceleration engine deployment method based on ONNX</li> <li>GPU deployment added Triton service-oriented deployment method based on ONNX (Docker Usage Documentation)</li> </ul> </li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v130-12192020","title":"PaddleX v1.3.0 (12.19/2020)","text":"<ul> <li> <p>Model Updates</p> <ul> <li>Image Classification model ResNet50_vd adds a pre-trained model with 100,000 categories.</li> <li>Object Detection model FasterRCNN adds model pruning support.</li> <li>Object Detection models now support multi-channel image training.</li> </ul> </li> <li> <p>Model Deployment Updates</p> <ul> <li>Fixed bugs in OpenVINO deployment C++ code.</li> <li>Raspberry Pi deployment adds Arm V8 support.</li> </ul> </li> <li> <p>Industry Case Updates</p> <ul> <li>Added an industrial quality inspection case, providing GPU and CPU deployment scenarios for industrial quality inspection, along with optimization strategies related to quality inspection.</li> </ul> </li> <li> <p>New RESTful API Module A new RESTful API module is added, enabling developers to quickly develop training platforms based on PaddleX.</p> <ul> <li>Added an HTML Demo based on RESTful API.</li> <li>Added a Remote version of the visualization client based on RESTful API. Added deployment solutions for models through OpenVINO.</li> </ul> </li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v120-992020","title":"PaddleX v1.2.0 (9.9/2020)","text":"<ul> <li> <p>Model Updates</p> <ul> <li>Added the object detection model PPYOLO.</li> <li>FasterRCNN, MaskRCNN, YOLOv3, DeepLabv3p, and other models now have pre-trained models on the COCO dataset.</li> <li>Object Detection models FasterRCNN and MaskRCNN add the backbone HRNet_W18.</li> <li>Semantic Segmentation model DeepLabv3p adds the backbone MobileNetV3_large_ssld.</li> </ul> </li> <li> <p>Model Deployment Updates</p> <ul> <li>Added deployment solutions for models through OpenVINO.</li> <li>Added deployment solutions for models on Raspberry Pi.</li> <li>Optimized data preprocessing and postprocessing code performance for PaddleLite Android deployment.</li> <li>Optimized Paddle Server-side C++ deployment code, added parameters such as use_mkl, significantly improving model prediction performance on CPUs through mkldnn.</li> </ul> </li> <li> <p>Industry Case Updates</p> <ul> <li>Added an RGB image remote sensing segmentation case.</li> <li>Added a multi-channel remote sensing segmentation case.</li> </ul> </li> <li> <p>Others</p> <ul> <li>Added a dataset splitting function, supporting command-line splitting of ImageNet, PascalVOC, MSCOCO, and semantic segmentation datasets.</li> </ul> </li> </ul> <p>### PaddleX v1.1.0 (7.13/2020) - Model Updates</p> <ul> <li>Added new semantic segmentation models: HRNet, FastSCNN</li> <li>Added HRNet backbone for object detection (FasterRCNN) and instance segmentation (MaskRCNN)</li> <li>Pre-trained models on COCO dataset for object detection and instance segmentation</li> <li>Integrated X2Paddle, enabling all PaddleX classification and semantic segmentation models to export to ONNX protocol</li> <li>Model Deployment Updates</li> <li>Added support for model encryption on Windows platform</li> <li>New deployment and prediction solutions for Jetson and PaddleLite</li> <li>C++ deployment code now supports batch prediction and utilizes OpenMP for parallel acceleration of preprocessing</li> <li>Added 2 PaddleX Industrial Cases</li> <li>Portrait segmentation case</li> <li>Industrial meter reading case</li> <li>New data format conversion feature, converting data annotated by LabelMe, Jingling Annotation Assistant, and EasyData platform to formats supported by PaddleX</li> <li>Updated PaddleX documentation, optimizing the document structure</li> </ul>"},{"location":"en/CHANGLOG.html#paddlex-v100-5212020","title":"PaddleX v1.0.0 (5.21/2020)","text":"<ul> <li>End-to-End Pipeline</li> <li>Data Preparation: Supports the EasyData Intelligent Data Service Platform data protocol, facilitating intelligent annotation and low-quality data cleaning through the platform. It is also compatible with mainstream annotation tool protocols, helping developers complete data preparation faster.</li> <li>Model Training: Integrates PaddleClas, PaddleDetection, PaddleSeg vision development kits, providing a rich selection of high-quality pre-trained models for faster achievement of industrial-grade model performance.</li> <li>Model Tuning: Built-in model interpretability modules and VisualDL visualization analysis components, providing abundant information for better understanding and optimizing models.</li> <li> <p>Secure Multi-platform Deployment: Integrated with PaddleSlim model compression tools and model encryption deployment modules, facilitating high-performance and secure multi-platform deployment in conjunction with Paddle Inference or Paddle Lite.</p> </li> <li> <p>Integrated Industrial Practices</p> </li> <li> <p>Selects mature model architectures from PaddlePaddle's industrial practices, opening up case study tutorials to accelerate developers' industrial implementation.</p> </li> <li> <p>Easy-to-Use and Easy-to-Integrate</p> </li> <li>Unified and user-friendly end-to-end APIs, enabling model training in 5 steps and high-performance Python/C++ deployment with just 10 lines of code.</li> <li>Provides PaddleX-GUI, a cross-platform visualization tool centered on PaddleX, for a quick experience of the full PaddlePaddle deep learning pipeline.</li> </ul>"},{"location":"en/FAQ.html","title":"FAQ","text":""},{"location":"en/FAQ.html#q-what-is-paddlex","title":"Q: What is PaddleX?","text":"<p>A: PaddleX is a low-code development tool featuring selected models and pipelines launched by the PaddlePaddle team. It aims to provide developers with a more convenient and efficient development environment. This tool supports model training and inference on multiple mainstream domestic and international hardware, and is compatible with various system configurations, thereby meeting users' needs in different application scenarios. PaddleX has a wide range of applications, covering industries such as industry, energy, finance, transportation, and education, providing professional support and solutions for these industries. By using PaddleX, developers can more easily apply deep learning technology to actual industrial practices, thereby realizing technology implementation and transformation, and promoting digital transformation and intelligent upgrading across industries.</p>"},{"location":"en/FAQ.html#q-what-is-a-pipeline-what-is-a-module-what-is-the-relationship-between-them","title":"Q: What is a Pipeline? What is a Module? What is the relationship between them?","text":"<p>A: In PaddleX, a module is defined as the smallest unit that implements basic functions, meaning each module undertakes a specific task, such as text detection. Within this framework, a pipeline is the actual functionality achieved by one or more modules working together, often forming more complex application scenarios, such as Optical Character Recognition (OCR) technology. Therefore, the relationship between modules and pipelines can be understood as the relationship between basics and applications. Modules, as the smallest units, provide the foundation for construction, while pipelines demonstrate the practical application effects of these foundational modules after reasonable combination and configuration. This design approach allows users to flexibly select and combine different modules to achieve the functions they need, significantly enhancing development flexibility and efficiency. The official pipelines also support users with high-performance inference, service-oriented deployment, and other deployment capabilities.</p>"},{"location":"en/FAQ.html#q-how-to-choose-between-the-wheel-package-installation-mode-and-the-plugin-installation-mode","title":"Q: How to choose between the Wheel package installation mode and the plugin installation mode?","text":"<p>A: If your application scenario in using PaddleX mainly focuses on model inference and integration, we highly recommend choosing a more convenient and lightweight installation method, namely the Wheel package installation mode. This installation mode aims to provide users with a quick and simple installation experience, especially suitable for scenarios requiring rapid deployment and integration of models. Installing with Wheel packages can significantly reduce the complexity of the installation process, avoid unnecessary configuration issues, and allow developers to focus more time and effort on the practical application and optimization of models. Whether you are a novice or an experienced developer, this lightweight installation method will greatly facilitate your workflow. Therefore, when performing model inference and integration, choosing the Wheel package installation mode will undoubtedly make your entire development process more efficient and smooth.</p>"},{"location":"en/FAQ.html#q-what-is-the-difference-between-paddlex-and-baidu-aistudio-communitys-zero-code-pipeline","title":"Q: What is the difference between PaddleX and Baidu AIStudio Community's Zero-Code Pipeline?","text":"<p>A: Baidu AIStudio Community's Zero-Code Pipeline is the cloud-based carrier of PaddleX, with its underlying code consistent with PaddleX, and can be considered as a cloud-based PaddleX. The design philosophy of Baidu AIStudio Community's Zero-Code Pipeline is to enable users to quickly build and deploy model applications without needing to delve deeply into programming and algorithm knowledge. On this basis, Baidu AIStudio Community's Zero-Code Pipeline also provides many special pipelines, such as training high-precision models with a small number of samples and solving complex time-series problems using multi-model fusion schemes. PaddleX, on the other hand, is a local development tool that provides users with powerful functions supporting more in-depth secondary development. This means developers can flexibly adjust and expand based on PaddleX to create solutions that better fit specific application scenarios. Additionally, PaddleX offers a rich set of model interfaces, supporting users in freely combining models for use.</p>"},{"location":"en/FAQ.html#q-when-i-encounter-problems-while-using-paddlex-how-should-i-provide-feedback","title":"Q: When I encounter problems while using PaddleX, how should I provide feedback?","text":"<p>A: Welcome to the Discussion Area to communicate with a vast number of developers! If you find errors or deficiencies in PaddleX, you are also welcome to submit an issue, and our on-duty team members will respond to your questions as soon as possible.</p>"},{"location":"en/blog/index.html","title":"Blog","text":""},{"location":"en/data_annotations/cv_modules/image_classification.html","title":"PaddleX Image Classification Task Module Data Annotation Tutorial","text":"<p>This document will introduce how to use the Labelme annotation tool to complete data annotation for image classification related single models. Click on the above link to refer to the homepage documentation for installing the data annotation tool and viewing detailed usage procedures.</p>"},{"location":"en/data_annotations/cv_modules/image_classification.html#1-labelme-annotation","title":"1. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/image_classification.html#11-introduction-to-labelme-annotation-tool","title":"1.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is a Python-based image annotation software with a graphical interface. It can be used for tasks such as image classification, object detection, and image segmentation. In instance segmentation annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/image_classification.html#12-labelme-installation","title":"1.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/image_classification.html#13-labelme-annotation-process","title":"1.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/image_classification.html#131-prepare-data-for-annotation","title":"1.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, such as <code>pets</code>.</li> <li>Create an <code>images</code> directory (must be named <code>images</code>) within <code>pets</code> and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul> <ul> <li>Create a category label file <code>flags.txt</code> for the dataset to be annotated in the <code>pets</code> folder, and write the categories of the dataset to be annotated into <code>flags.txt</code> line by line. Taking the <code>flags.txt</code> for a cat and dog classification dataset as an example, as shown below:</li> </ul>"},{"location":"en/data_annotations/cv_modules/image_classification.html#132-start-labelme","title":"1.3.2 Start Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and start the <code>labelme</code> annotation tool.</p> <p><pre><code>cd path/to/pets\nlabelme images --nodata --autosave --output annotations --flags flags.txt\n</code></pre> * <code>flags</code> creates classification labels for images, passing in the path to the labels. * <code>nodata</code> stops storing image data in JSON files. * <code>autosave</code> enables automatic saving. * <code>output</code> specifies the storage path for label files.</p>"},{"location":"en/data_annotations/cv_modules/image_classification.html#133-start-image-annotation","title":"1.3.3 Start Image Annotation","text":"<ul> <li>After starting <code>labelme</code>, it will look like this:</li> </ul> <p> * Select the category in the <code>Flags</code> interface.</p> <p></p> <ul> <li>After annotation, click Save. (If <code>output</code> is not specified when starting <code>labelme</code>, it will prompt to select a save path upon the first save. If <code>autosave</code> is specified, there is no need to click the Save button).</li> </ul> <p> * Then click <code>Next Image</code> to annotate the next image.</p> <p></p> <ul> <li>After annotating all images, use the convert_to_imagenet.py script to convert the annotated dataset to the <code>ImageNet-1k</code> dataset format, generating <code>train.txt</code>, <code>val.txt</code>, and <code>label.txt</code>.</li> </ul> <p><pre><code>python convert_to_imagenet.py --dataset_path /path/to/dataset\n</code></pre> <code>dataset_path</code> is the path to the annotated <code>labelme</code> format classification dataset.</p> <ul> <li>The final directory structure after organization is as follows:</li> </ul> <p></p>"},{"location":"en/data_annotations/cv_modules/image_classification.html#2-data-format","title":"2. Data Format","text":"<ul> <li>The dataset defined by PaddleX for image classification tasks is named ClsDataset, with the following organizational structure and annotation format:</li> </ul> <p><pre><code>dataset_dir    # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images     # Directory for saving images, the directory name can be changed, but note the correspondence with the content of train.txt and val.txt\n\u251c\u2500\u2500 label.txt  # Correspondence between annotation IDs and category names, the file name cannot```bash\nclassname1\nclassname2\nclassname3\n...\n</code></pre> Modified <code>label.txt</code>:</p> <p>```bash 0 classname1 1 classname2 2 classname3 ...</p>"},{"location":"en/data_annotations/cv_modules/image_feature.html","title":"PaddleX Image Feature Task Module Data Annotation Tutorial","text":"<p>This section will introduce how to use the Labelme annotation tool to complete data annotation for image feature-related single models. Click the link above to install the data annotation tool and view detailed usage instructions by referring to the homepage documentation.</p>"},{"location":"en/data_annotations/cv_modules/image_feature.html#1-labelme-annotation","title":"1. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/image_feature.html#11-introduction-to-labelme-annotation-tool","title":"1.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is a Python-based image annotation software with a graphical user interface. It can be used for tasks such as image classification, object detection, and image segmentation. In image feature annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/image_feature.html#12-labelme-installation","title":"1.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/image_feature.html#13-labelme-annotation-process","title":"1.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/image_feature.html#131-prepare-data-for-annotation","title":"1.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, e.g., <code>pets</code>.</li> <li>Create an <code>images</code> directory (must be named <code>images</code>) within <code>pets</code> and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul> <ul> <li>Create a category label file <code>flags.txt</code> in the <code>pets</code> folder for the dataset to be annotated, and write the categories of the dataset to be annotated into <code>flags.txt</code> line by line. Taking the <code>flags.txt</code> for a cat-dog classification dataset as an example, as shown below:</li> </ul>"},{"location":"en/data_annotations/cv_modules/image_feature.html#132-start-labelme","title":"1.3.2 Start Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and start the <code>labelme</code> annotation tool.</p> <p><pre><code>cd path/to/pets\nlabelme images --nodata --autosave --output annotations --flags flags.txt\n</code></pre> * <code>flags</code> creates classification labels for images, passing in the label path. * <code>nodata</code> stops storing image data in JSON files. * <code>autosave</code> enables automatic saving. * <code>output</code> specifies the storage path for label files.</p>"},{"location":"en/data_annotations/cv_modules/image_feature.html#133-start-image-annotation","title":"1.3.3 Start Image Annotation","text":"<ul> <li>After starting <code>labelme</code>, it will look like this:</li> </ul> <p> * Select the category in the <code>Flags</code> interface.</p> <p></p> <ul> <li>After annotation, click Save. (If <code>output</code> is not specified when starting <code>labelme</code>, it will prompt to select the save path upon the first save. If <code>autosave</code> is enabled, there is no need to click the Save button).</li> </ul> <p> * Then click <code>Next Image</code> to annotate the next image.</p> <p></p> <ul> <li>After annotating all images, use the convert_to_imagenet.py to convert the annotated dataset to <code>ImageNet-1k</code> dataset format, generating <code>train.txt</code>, <code>val.txt</code>, and <code>label.txt</code>.</li> </ul> <p><pre><code>python convert_to_imagenet.py --dataset_path /path/to/dataset\n</code></pre> <code>dataset_path</code> is the annotated <code>labelme</code> format classification dataset.</p> <ul> <li>The final directory structure after organization is as follows:</li> </ul> <p></p>"},{"location":"en/data_annotations/cv_modules/image_feature.html#134-data-format-conversion","title":"1.3.4 Data Format Conversion","text":"<p>After obtaining data in <code>LabelMe</code> format, the data format needs to be converted to <code>ShiTuRecDataset</code> format. Below is a code example that demonstrates how to convert the data labeled using <code>LabelMe</code> according to the previous tutorial.</p> <pre><code># Download and unzip the LabelMe format example dataset\ncd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/image_classification_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/image_classification_labelme_examples.tar -C ./dataset/\n# Convert the LabelMe example dataset\npython main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/image_classification_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre>"},{"location":"en/data_annotations/cv_modules/image_feature.html#3-data-format","title":"3. Data Format","text":"<p>The dataset defined by PaddleX for image classification tasks is named ShiTuRecDataset, with the following organizational structure and annotation format:</p> <pre><code>dataset_dir    # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images     # Directory where images are saved, the directory name can be changed, but should correspond to the content of train.txt, query.txt, gallery.txt\n\u251c\u2500\u2500 gallery.txt   # Annotation file for the gallery set, the file name cannot be changed. Each line gives the path of the image to be retrieved and its feature label, separated by a space. Example: images/WOMEN/Blouses_Shirts/id_00000001/02_2_side.jpg 3997\n\u2514\u2500\u2500 query.txt     # Annotation file for the query set, the file name cannot be changed. Each line gives the path of the database image and its feature label, separated by a space. Example: images/WOMEN/Blouses_Shirts/id_00000001/02_1_front.jpg 3997\n</code></pre> <p>The annotation files use an image feature format. Please refer to the above specifications to prepare your data. Additionally, you can refer to the example dataset.</p>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html","title":"PaddleX Instance Segmentation Task Module Data Annotation Tutorial","text":"<p>This document will introduce how to use the Labelme annotation tool to complete data annotation for a single model related to instance segmentation. Click on the link above to install the data annotation tool and view detailed usage instructions by referring to the homepage documentation.</p>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#1-annotation-data-example","title":"1. Annotation Data Example","text":"<p>This dataset is a fruit instance segmentation dataset, covering five different types of fruits, including photos taken from different angles of the targets. Image examples:</p>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#2-labelme-annotation","title":"2. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#21-introduction-to-labelme-annotation-tool","title":"2.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is a Python-based image annotation software with a graphical user interface. It can be used for tasks such as image classification, object detection, and image segmentation. For instance segmentation annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#22-labelme-installation","title":"2.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#23-labelme-annotation-process","title":"2.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#231-prepare-data-for-annotation","title":"2.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, such as <code>fruit</code>.</li> <li>Create an <code>images</code> directory (must be named <code>images</code>) within <code>fruit</code> and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul> <ul> <li>Create a category label file <code>label.txt</code> in the <code>fruit</code> folder for the dataset to be annotated, and write the categories of the dataset to be annotated into <code>label.txt</code> by line. Taking the fruit instance segmentation dataset's <code>label.txt</code> as an example, as shown below:</li> </ul>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#232-start-labelme","title":"2.3.2 Start Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and start the <code>labelme</code> annotation tool.</p> <p><pre><code>cd path/to/fruit\nlabelme images --labels label.txt --nodata --autosave --output annotations\n</code></pre> * <code>labels</code> is the path to the category labels. * <code>nodata</code> stops storing image data in the JSON file. * <code>autosave</code> enables automatic saving. * <code>output</code> specifies the path for storing label files.</p>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#233-begin-image-annotation","title":"2.3.3 Begin Image Annotation","text":"<ul> <li>After starting <code>labelme</code>, it will look like this:</li> </ul> <p> * Click <code>Edit</code> to select the annotation type, choose <code>Create Polygons</code>. * Create polygons on the image to outline the boundaries of the segmentation areas.</p> <p> * Click again to select the category of the segmentation area.</p> <p></p> <ul> <li>After annotation, click Save. (If <code>output</code> is not specified when starting <code>labelme</code>, it will prompt to select a save path upon the first save. If <code>autosave</code> is specified, there is no need to click the Save button).</li> </ul> <p></p> <ul> <li>Then click <code>Next Image</code> to annotate the next image.</li> </ul> <p> * The final annotated label file will look like this.</p> <p></p> <ul> <li> <p>Adjusting Directory Structure to Obtain a Standard <code>labelme</code> Format Dataset for Fruit Instance Segmentation</p> </li> <li> <p>Prepare the <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> text files in the root directory of your dataset. Populate these files with the paths of all <code>json</code> files in the <code>annotations</code> directory, distributing them into <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> at a specified ratio. Alternatively, you can include all paths in <code>train_anno_list.txt</code> and create an empty <code>val_anno_list.txt</code> file, intending to use a zero-code data splitting feature for re-partitioning upon upload. The specific format for filling <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> is illustrated as follows:</p> </li> </ul> <p></p> <ul> <li>The final directory structure after organization should resemble the following:</li> </ul> <p></p> <ul> <li>Compress the <code>fruit</code> directory into a <code>.tar</code> or <code>.zip</code> format archive to obtain the standard <code>labelme</code> format dataset for fruit instance segmentation.</li> </ul>"},{"location":"en/data_annotations/cv_modules/instance_segmentation.html#3-data-format","title":"3. Data Format","text":"<p>PaddleX defines a dataset named COCOInstSegDataset for instance segmentation tasks, with the following organizational structure and annotation format:</p> <pre><code>dataset_dir                  # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 annotations              # Directory for saving annotation files, the directory name cannot be changed\n\u2502   \u251c\u2500\u2500 instance_train.json  # Training set annotation file, the file name cannot be changed, using COCO annotation format\n\u2502   \u2514\u2500\u2500 instance_val.json    # Validation set annotation file, the file name cannot be changed, using COCO annotation format\n\u2514\u2500\u2500 images                   # Directory for saving images, the directory name cannot be changed\n</code></pre> <p>Annotation files adopt the <code>COCO</code> format. Please refer to the above specifications for data preparation. Additionally, refer to: Example Dataset.</p> <p>When using PaddleX 2.x version for instance segmentation datasets, please refer to the corresponding format conversion section in  Instance Segmentation Module Development Tutorial to convert VOC format datasets to COCO datasets. (Note in module development documentation)</p> <p>Note:</p> <ul> <li>Instance segmentation data requires the use of the <code>COCO</code> data format to annotate the pixel boundaries and categories of each target area in each image in the dataset. The polygon boundaries (segmentation) of objects are represented as <code>[x1,y1,x2,y2,...,xn,yn]</code>, where <code>(xn,yn)</code> denotes the coordinates of each corner point of the polygon. Annotation information is stored in <code>json</code> files under the <code>annotations</code> directory, with separate files for the training set (<code>instance_train.json</code>) and validation set (<code>instance_val.json</code>).</li> <li>If you have a batch of unlabeled data, we recommend using <code>LabelMe</code> for data annotation. PaddleX Pipelines support data format conversion for datasets annotated with <code>LabelMe</code>.</li> <li>To ensure successful format conversion, please strictly follow the file naming and organization of the example dataset: LabelMe Example Dataset.</li> </ul>"},{"location":"en/data_annotations/cv_modules/ml_classification.html","title":"PaddleX Multi-Label Classification Task Data Annotation Tutorial","text":"<p>This section will introduce how to use Labelme and PaddleLabel annotation tools to complete data annotation for multi-label classification tasks with a single model. Click on the above links to install the annotation tools and view detailed usage instructions by referring to the homepage documentation.</p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#1-annotation-data-example","title":"1. Annotation Data Example","text":"<p>This dataset is manually collected, covering two categories: safety helmets and human heads, with photos taken from different angles. Image examples:</p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#2-labelme-annotation","title":"2. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/ml_classification.html#21-introduction-to-labelme-annotation-tool","title":"2.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is a Python-based image annotation software with a graphical user interface. It can be used for image classification, object detection, image segmentation, and other tasks. In object detection annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#22-labelme-installation","title":"2.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#23-labelme-annotation-process","title":"2.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/ml_classification.html#231-prepare-data-for-annotation","title":"2.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, e.g., <code>helmet</code>.</li> <li>Create an <code>images</code> directory (must be named <code>images</code>) within <code>helmet</code> and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul> <p> * Create a category label file <code>label.txt</code> in the <code>helmet</code> folder and write the categories of the dataset to be annotated into <code>label.txt</code> by line. For example, for a helmet detection dataset, <code>label.txt</code> would look like this:</p> <p></p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#232-start-labelme","title":"2.3.2 Start Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and start the <code>Labelme</code> annotation tool: <pre><code>cd path/to/helmet\nlabelme images --labels label.txt --nodata --autosave --output annotations\n</code></pre> * <code>flags</code> creates classification labels for images, passing in the label path. * <code>nodata</code> stops storing image data in the <code>JSON</code> file. * <code>autosave</code> enables automatic saving. * <code>output</code> specifies the storage path for label files.</p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#233-begin-image-annotation","title":"2.3.3 Begin Image Annotation","text":"<ul> <li>After starting <code>Labelme</code>, it will look like this:</li> </ul> <p> * Click \"Edit\" to select the annotation type.</p> <p> * Choose to create a rectangular box.</p> <p> * Drag the crosshair to select the target area on the image.</p> <p> * Click again to select the category of the target box.</p> <p> * After labeling, click Save. (If the <code>output</code> field is not specified when starting <code>Labelme</code>, it will prompt you to select a save path the first time you save. If the <code>autosave</code> field is used for automatic saving, there is no need to click the Save button).</p> <p> * Then click <code>Next Image</code> to label the next image.</p> <p> * The final labeled tag file looks like this:</p> <p> * Adjust the directory to obtain the safety helmet detection dataset in the standard <code>Labelme</code> format   * Create two text files, <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code>, in the root directory of the dataset. Write the paths of all <code>json</code> files in the <code>annotations</code> directory into <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> at a certain ratio, or write all of them into <code>train_anno_list.txt</code> and create an empty <code>val_anno_list.txt</code> file. Use the data splitting function to re-split. The specific filling format of <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> is shown below:</p> <p>   * The final directory structure after organization is as follows:</p> <p></p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#234-format-conversion","title":"2.3.4 Format Conversion","text":"<p>After labeling with <code>Labelme</code>, the data format needs to be converted to <code>coco</code> format. Below is a code example for converting the data labeled using <code>Labelme</code> according to the above tutorial: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/det_labelme_examples.tar -C ./dataset/\n\npython main.py -c paddlex/configs/obeject_detection/PicoDet-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre></p>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#3-paddlelabel-annotation","title":"3. PaddleLabel Annotation","text":""},{"location":"en/data_annotations/cv_modules/ml_classification.html#31-installation-and-startup-of-paddlelabel","title":"3.1 Installation and Startup of PaddleLabel","text":"<ul> <li>To avoid environment conflicts, it is recommended to create a clean <code>conda</code> environment: <pre><code>conda create -n paddlelabel python=3.11\nconda activate paddlelabel\n</code></pre></li> <li>It can also be installed with <code>pip</code> in one step <pre><code>pip install --upgrade paddlelabel\npip install a2wsgi uvicorn==0.18.1\npip install connexion==2.14.1\npip install Flask==2.2.2\npip install Werkzeug==2.2.2\n</code></pre></li> <li>After successful installation, you can start PaddleLabel using one of the following commands in the terminal: <pre><code>paddlelabel  # Start paddlelabel\npdlabel # Abbreviation, identical to paddlelabel\n</code></pre> <code>PaddleLabel</code> will automatically open a webpage in the browser after starting. Next, you can start the annotation process based on the task.</li> </ul>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#32-annotation-process-with-paddlelabel","title":"3.2 Annotation Process with PaddleLabel","text":"<ul> <li>Open the automatically popped-up webpage, click on the sample project, and click on Object Detection.</li> </ul> <p> * Fill in the project name and dataset path. Note that the path is the absolute path on the local machine. Click Create after completion.</p> <p> * First, define the categories that need to be annotated. Taking layout analysis as an example, provide 10 categories, each with a unique corresponding id. Click Add Category to create the required category names. * Start annotating   * Select the label you want to annotate with.   * Click the rectangular selection button on the left.   * Draw a bounding box around the desired region in the image, ensuring semantic partitioning. If there are multiple columns, annotate each separately.   * After completing the annotation, the result will appear in the lower-right corner. Check if the annotation is correct.   * Once done, click Project Overview.</p> <p></p> <ul> <li> <p>Export Annotated Files</p> </li> <li> <p>In the Project Overview, segment the dataset as needed and click \"Export Dataset\".</p> </li> </ul> <p></p> <ul> <li>Fill in the export path and format. The export path should be an absolute path, and the format should be <code>coco</code>.</li> </ul> <p></p> <ul> <li>After successful export, the annotated files will be available in the specified path.</li> </ul> <p></p> <ul> <li> <p>Adjust directories to obtain COCO-formatted dataset for helmet detection</p> </li> <li> <p>Rename the three <code>json</code> files and the <code>image</code> directory as follows:</p> </li> </ul> Original File/Directory Name Renamed File/Directory Name <code>train.json</code> <code>instance_train.json</code> <p>|<code>val.json</code>|<code>instance_val.json</code>| </p> <p>|<code>test.json</code>|<code>instance_test.json</code>| |<code>image</code>|<code>images</code>|</p> <ul> <li>Create an <code>annotations</code> directory in the root of the dataset and move all <code>json</code> files into it. The final dataset structure should look like this:</li> </ul> <p></p> <ul> <li>Compress the <code>helmet</code> directory into a <code>.tar</code> or <code>.zip</code> file to obtain the COCO-formatted dataset for helmet detection.</li> </ul>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#4-image-multi-label-classification-data-format-conversion","title":"4. Image Multi-Label Classification Data Format Conversion","text":"<p>After obtaining data in COCO format, you need to convert the data format to <code>MLClsDataset</code> format. Below is a code example that follows the previous tutorial to use <code>LabelMe</code> or <code>PaddleLabel</code> annotated data and perform data format conversion:</p> <pre><code># Download and unzip the COCO example dataset\ncd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_coco_examples.tar -P ./dataset\ntar -xf ./dataset/det_coco_examples.tar -C ./dataset/\n# Convert the COCO example dataset to MLClsDataset\npython main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=COCO\n</code></pre>"},{"location":"en/data_annotations/cv_modules/ml_classification.html#5-data-format","title":"5. Data Format","text":"<p>The dataset defined by PaddleX for image multi-label classification tasks is named MLClsDataset, with the following directory structure and annotation format:</p> <pre><code>dataset_dir    # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images     # Directory where images are saved, the directory name can be changed, but note the correspondence with the content of train.txt and val.txt\n\u251c\u2500\u2500 label.txt  # Correspondence between annotation IDs and category names, the file name cannot be changed. Each line gives the category ID and category name, for example: 45 wallflower\n\u251c\u2500\u2500 train.txt  # Annotation file for the training set, the file name cannot be changed. Each line gives the image path and multi-label classification tags for the image, separated by spaces, for example: images/0041_2456602544.jpg 0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0\n\u2514\u2500\u2500 val.txt    # Annotation file for the validation set, the file name cannot be changed. Each line gives the image path and multi-label classification tags for the image, separated by spaces, for example: images/0045_845243484.jpg 0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0\n</code></pre> <p>The annotation files use the multi-label classification format. Please prepare your data according to the above specifications. Additionally, you can refer to the example dataset.</p>"},{"location":"en/data_annotations/cv_modules/object_detection.html","title":"PaddleX Object Detection Task Data Preparation Tutorial","text":"<p>This section will introduce how to use Labelme and PaddleLabel annotation tools to complete data annotation for single-model object detection tasks. Click the above links to install the annotation tools and view detailed usage instructions.</p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#1-annotation-data-examples","title":"1. Annotation Data Examples","text":""},{"location":"en/data_annotations/cv_modules/object_detection.html#2-labelme-annotation","title":"2. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/object_detection.html#21-introduction-to-labelme-annotation-tool","title":"2.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is a Python-based image annotation software with a graphical user interface. It can be used for tasks such as image classification, object detection, and image segmentation. For object detection annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#22-labelme-installation","title":"2.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/object_detection.html#23-labelme-annotation-process","title":"2.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/object_detection.html#231-prepare-data-for-annotation","title":"2.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, e.g., <code>helmet</code>.</li> <li>Create an <code>images</code> directory (must be named <code>images</code>) within <code>helmet</code> and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul> <p> * Create a category label file <code>label.txt</code> in the <code>helmet</code> folder and write the categories of the dataset to be annotated, line by line. For example, for a helmet detection dataset, <code>label.txt</code> would look like this:</p> <p></p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#232-start-labelme","title":"2.3.2 Start Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and start the <code>Labelme</code> annotation tool: <pre><code>cd path/to/helmet\nlabelme images --labels label.txt --nodata --autosave --output annotations\n</code></pre> * <code>flags</code> creates classification labels for images, passing in the path to the labels. * <code>nodata</code> stops storing image data in the <code>JSON</code> file. * <code>autosave</code> enables automatic saving. * <code>output</code> specifies the path for storing label files.</p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#233-begin-image-annotation","title":"2.3.3 Begin Image Annotation","text":"<ul> <li>After starting <code>Labelme</code>, it will look like this:</li> </ul> <p> * Click \"Edit\" to select the annotation type.</p> <p> * Choose to create a rectangular box.</p> <p> * Drag the crosshair to select the target area on the image.</p> <p> * Click again to select the category of the target box.</p> <p> * After annotation, click Save. (If <code>output</code> is not specified when starting <code>Labelme</code>, it will prompt to select a save path upon the first save. If <code>autosave</code> is enabled, no need to click Save.)</p> <p> * Click <code>Next Image</code> to annotate the next.</p> <p> * The final labeled tag file looks like this:</p> <p> * Adjust the directory to obtain the safety helmet detection dataset in the standard <code>Labelme</code> format   * Create two text files, <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code>, in the root directory of the dataset. Write the paths of all <code>json</code> files in the <code>annotations</code> directory into <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> at a certain ratio, or write all of them into <code>train_anno_list.txt</code> and create an empty <code>val_anno_list.txt</code> file. Use the data splitting function to re-split. The specific filling format of <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> is shown below:</p> <p>   * The final directory structure after organization is as follows:</p> <p></p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#234-format-conversion","title":"2.3.4 Format Conversion","text":"<p>After labeling with <code>Labelme</code>, the data format needs to be converted to <code>coco</code> format. Below is a code example for converting the data labeled using <code>Labelme</code> according to the above tutorial: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/det_labelme_examples.tar -C ./dataset/\n\npython main.py -c paddlex/configs/obeject_detection/PicoDet-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre></p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#3-paddlelabel-annotation","title":"3. PaddleLabel Annotation","text":""},{"location":"en/data_annotations/cv_modules/object_detection.html#31-installation-and-startup-of-paddlelabel","title":"3.1 Installation and Startup of PaddleLabel","text":"<ul> <li>To avoid environment conflicts, it is recommended to create a clean <code>conda</code> environment: <pre><code>conda create -n paddlelabel python=3.11\nconda activate paddlelabel\n</code></pre></li> <li>Alternatively, you can install it with <code>pip</code> in one command: <pre><code>pip install --upgrade paddlelabel\npip install a2wsgi uvicorn==0.18.1\npip install connexion==2.14.1\npip install Flask==2.2.2\npip install Werkzeug==2.2.2\n</code></pre></li> <li>After successful installation, you can start PaddleLabel using one of the following commands in the terminal: <pre><code>paddlelabel  # Start paddlelabel\npdlabel # Abbreviation, identical to paddlelabel\n</code></pre> PaddleLabel will automatically open a webpage in your browser after startup. You can then proceed with the annotation process based on your task.</li> </ul>"},{"location":"en/data_annotations/cv_modules/object_detection.html#32-annotation-process-of-paddlelabel","title":"3.2 Annotation Process of PaddleLabel","text":"<ul> <li>Open the automatically popped-up webpage, click on the sample project, and then click on Object Detection.</li> </ul> <p> * Fill in the project name and dataset path. Note that the path should be an absolute path on your local machine. Click Create when done.</p> <p> * First, define the categories that need to be annotated. Taking layout analysis as an example, provide 10 categories, each with a unique corresponding ID. Click Add Category to create the required category names. * Start Annotating   * First, select the label you need to annotate.   * Click the rectangular selection button on the left.   * Draw a bounding box around the desired area in the image, paying attention to semantic partitioning. If there are multiple columns, please annotate each separately.   * After completing the annotation, the annotation result will appear in the lower right corner. You can check if the annotation is correct.   * When all annotations are complete, click Project Overview.</p> <p> * Export Annotation Files   * In Project Overview, divide the dataset as needed, then click Export Dataset.</p> <p>   * Fill in the export path and export format. The export path should also be an absolute path, and the export format should be selected as <code>coco</code>.</p> <p>   * After successful export, you can obtain the annotation files in the specified path.</p> <p> * Adjust the directory to obtain the standard <code>coco</code> format dataset for helmet detection   * Rename the three <code>json</code> files and the <code>image</code> directory according to the following correspondence:</p> Original File (Directory) Name Renamed File (Directory) Name <code>train.json</code> <code>instance_train.json</code> <code>val.json</code> <code>instance_val.json</code> <code>test.json</code> <code>instance_test.json</code> <code>image</code> <code>images</code> <ul> <li>Create an <code>annotations</code> directory in the root directory of the dataset and move all <code>json</code> files to the <code>annotations</code> directory. The final dataset directory structure will look like this:</li> </ul> <p>   * Compress the <code>helmet</code> directory into a <code>.tar</code> or <code>.zip</code> format compressed package to obtain the standard <code>coco</code> format dataset for helmet detection.</p>"},{"location":"en/data_annotations/cv_modules/object_detection.html#4-data-format","title":"4. Data Format","text":"<p>The dataset defined by PaddleX for object detection tasks is named <code>COCODetDataset</code>, with the following organizational structure and annotation format: <pre><code>dataset_dir                  # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 annotations              # Directory for saving annotation files, the directory name cannot be changed\n\u2502   \u251c\u2500\u2500 instance_train.json  # Annotation file for the training set, the file name cannot be changed, using COCO annotation format\n\u2502   \u2514\u2500\u2500 instance_val.json    # Annotation file for the validation set, the file name cannot be changed, using COCO annotation format\n\u2514\u2500\u2500 images                   # Directory for saving images, the directory name cannot be changed\n</code></pre></p> <p>The annotation files use the COCO format. Please prepare your data according to the above specifications. Additionally, you can refer to the example dataset.</p>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html","title":"PaddleX Semantic Segmentation Task Module Data Annotation Tutorial","text":"<p>This document will introduce how to use the Labelme annotation tool to complete data annotation for a single model related to semantic segmentation. Click on the link above to install the data annotation tool and view detailed usage instructions by referring to the homepage documentation.</p>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#1-annotation-data-examples","title":"1. Annotation Data Examples","text":"<p>This dataset is a manually collected street scene dataset, covering two categories of vehicles and roads, including photos taken from different angles of the targets. Image examples:</p>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#2-labelme-annotation","title":"2. Labelme Annotation","text":""},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#21-introduction-to-labelme-annotation-tool","title":"2.1 Introduction to Labelme Annotation Tool","text":"<p><code>Labelme</code> is an image annotation software written in <code>python</code> with a graphical interface. It can be used for tasks such as image classification, object detection, and semantic segmentation. In semantic segmentation annotation tasks, labels are stored as <code>JSON</code> files.</p>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#22-labelme-installation","title":"2.2 Labelme Installation","text":"<p>To avoid environment conflicts, it is recommended to install in a <code>conda</code> environment.</p> <pre><code>conda create -n labelme python=3.10\nconda activate labelme\npip install pyqt5\npip install labelme\n</code></pre>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#23-labelme-annotation-process","title":"2.3 Labelme Annotation Process","text":""},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#231-prepare-data-for-annotation","title":"2.3.1 Prepare Data for Annotation","text":"<ul> <li>Create a root directory for the dataset, such as <code>seg_dataset</code></li> <li>Create an <code>images</code> directory within <code>seg_dataset</code> (the directory name can be modified, but ensure the subsequent command's image directory name is correct), and store the images to be annotated in the <code>images</code> directory, as shown below:</li> </ul>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#232-launch-labelme","title":"2.3.2 Launch Labelme","text":"<p>Navigate to the root directory of the dataset to be annotated in the terminal and launch the <code>labelme</code> annotation tool.</p> <p><pre><code># Windows\ncd C:\\path\\to\\seg_dataset\n# Mac/Linux\ncd path/to/seg_dataset\n</code></pre> <pre><code>labelme images --nodata --autosave --output annotations\n</code></pre> * <code>nodata</code> stops storing image data in the JSON file * <code>autosave</code> enables automatic saving * <code>output</code> specifies the path for storing label files</p>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#233-start-image-annotation","title":"2.3.3 Start Image Annotation","text":"<ul> <li>After launching <code>labelme</code>, it will look like this:</li> </ul> <p> * Click \"Edit\" to select the annotation type</p> <p> * Choose to create polygons</p> <p> * Draw the target contour on the image</p> <p></p> <ul> <li>When the contour line is closed as shown in the left image below, a category selection box will pop up, allowing you to input or select the target category</li> </ul> <p> </p> <p>Typically, only the foreground objects need to be labeled with their respective categories, while other pixels are automatically considered as background. If manual background labeling is required, the category must be set to background, otherwise errors may occur during dataset format conversion. For noisy parts or irrelevant sections in the image that should not participate in model training, the ignore class can be used, and the model will automatically skip those parts during training. For objects with holes, after outlining the main object, draw polygons along the edges of the holes and assign a specific category to the holes. If the hole represents background, assign it as background. An example is shown below:</p> <p></p> <ul> <li>After labeling, click \"Save\". (If the <code>output</code> field is not specified when starting <code>labelme</code>, it will prompt to select a save path upon the first save. If <code>autosave</code> is enabled, the save button is not required.)</li> </ul> <p> * Then click \"Next Image\" to proceed to the next image for labeling.</p> <p></p> <ul> <li>The final labeled file will look like this:</li> </ul> <p></p> <ul> <li> <p>Adjust the directory structure to obtain a standard LabelMe format dataset for safety helmet detection:     a. Download and execute the directory organization script in the root directory of your dataset, <code>seg_dataset</code>. After executing the script, the <code>train_anno_list.txt</code> and <code>val_anno_list.txt</code> files will contain content as shown:</p> <p><pre><code>python format_seg_labelme_dataset.py\n</code></pre>  b. The final directory structure after organization will look like this:</p> <p></p> </li> </ul>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#234-format-conversion","title":"2.3.4 Format Conversion","text":"<p>After labeling with <code>LabelMe</code>, the data format needs to be converted to the <code>Seg</code> data format. Below is a code example for converting data labeled using <code>LabelMe</code> according to the above tutorial.</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/seg_dataset_to_convert.tar -P ./dataset\ntar -xf ./dataset/seg_dataset_to_convert.tar -C ./dataset/\n\npython main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_dataset_to_convert \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre>"},{"location":"en/data_annotations/cv_modules/semantic_segmentation.html#data-format","title":"Data Format","text":"<p>The dataset defined by PaddleX for image segmentation tasks is named SegDataset, with the following organizational structure and annotation format:</p> <p><pre><code>dataset_dir         # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 annotations     # Directory for storing annotated images, the directory name can be changed, matching the content of the manifest files\n\u251c\u2500\u2500 images          # Directory for storing original images, the directory name can be changed, matching the content of the manifest files\n\u251c\u2500\u2500 train.txt       # Annotation file for the training set, the file name cannot be changed. Each line contains the path to the original image and the annotated image, separated by a space. Example: images/P0005.jpg annotations/P0005.png\n\u2514\u2500\u2500 val.txt         # Annotation file for the validation set, the file name cannot be changed. Each line contains the path to the original image and the annotated image, separated by a space. Example: images/N0139.jpg annotations/N0139.png\n</code></pre> Label images should be single-channel grayscale or single-channel pseudo-color images, and it is recommended to save them in <code>PNG</code> format. Each pixel value in the label image represents a category, and the categories should start from 0 and increase sequentially, for example, 0, 1, 2, 3 represent 4 categories. The pixel storage of the label image is 8bit, so a maximum of 256 categories are supported for labeling.</p> <p>Please prepare your data according to the above specifications. Additionally, you can refer to: Example Dataset</p>"},{"location":"en/data_annotations/ocr_modules/table_recognition.html","title":"PaddleX Table Structure Recognition Task Data Annotation Tutorial","text":""},{"location":"en/data_annotations/ocr_modules/table_recognition.html#1-data-annotation","title":"1. Data Annotation","text":"<p>For annotating table data, use the PPOCRLabelv2 tool. Detailed steps can be found in: \u3010Video Demonstration\u3011</p> <p>Table annotation focuses on structured extraction of table data, converting tables in images into Excel format. Therefore, annotation requires the use of an external software to open Excel simultaneously. In PPOCRLabel, complete the annotation of text information within the table (text and position), and in the Excel file, complete the annotation of table structure information. The recommended steps are:</p> <ol> <li>Table Recognition: Open the table image, click the <code>Table Recognition</code> button in the upper right corner of the software. The software will call the table recognition model in PP-Structure to automatically label the table, and simultaneously open an Excel file.</li> <li>Modify Annotation Results: Add annotation boxes with each cell as the unit (i.e., all text within a cell is marked as one box). Right-click on the annotation box and select <code>Cell Re-recognition</code> to automatically recognize the text within the cell using the model.</li> <li>Adjust Cell Order: Click <code>View - Show Box Number</code> to display the annotation box numbers. Drag all results under the <code>Recognition Results</code> column on the right side of the software interface to arrange the annotation box numbers in order from left to right and top to bottom, annotating by row.</li> <li>Annotate Table Structure: In an external Excel software, mark cells with text as any identifier (e.g., <code>1</code>), ensuring that the cell merging in Excel matches the original image (i.e., the text in Excel cells does not need to be identical to the text in the image).</li> <li>Export JSON Format: Close all Excel files corresponding to the table images, click <code>File - Export Table Annotation</code>, and generate the gt.txt annotation file.</li> </ol>"},{"location":"en/data_annotations/ocr_modules/table_recognition.html#2-data-format","title":"2. Data Format","text":"<p>The dataset structure and annotation format defined by PaddleX for table recognition tasks are as follows:</p> <p>```ruby dataset_dir    # Root directory of the dataset, the directory name can be changed \u251c\u2500\u2500 images     # Directory for saving images, the directory name can be changed, but note the correspondence with the content of train.txt and val.txt \u251c\u2500\u2500 train.txt  # Training set annotation file, the file name cannot be changed. Example content: {\"filename\": \"images/border.jpg\", \"html\": {\"structure\": {\"tokens\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]}, \"cells\": [{\"tokens\": [\"\u3001\", \"\u200b\u81ea\u200b\", \"\u200b\u6211\u200b\"], \"bbox\": [[[5, 2], [231, 2], [231, 35], [5, 35]]]}, {\"tokens\": [\"9\"], \"bbox\": [[[168, 68], [231, 68], [231, 98], [168, 98]]]}]}, \"gt\": \"\u3001\u200b\u81ea\u6211\u200bAghas\u200b\u5931\u5434\u200b\u6708\u200b\uff0clonwyCau9\"} \u2514\u2500\u2500 val.txt    # Validation set annotation file, the file name cannot be changed. Example content: {\"filename\": \"images/no_border.jpg\", \"html\": {\"structure\": {\"tokens\": [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]}, \"cells\": [{\"tokens\": [\"a"},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html","title":"PaddleX Text Detection/Text Recognition Task Module Data Annotation Tutorial","text":""},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#1-annotation-data-examples","title":"1. Annotation Data Examples","text":""},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#2-ppocrlabel-annotation","title":"2. PPOCRLabel Annotation","text":""},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#21-introduction-to-ppocrlabel-annotation-tool","title":"2.1 Introduction to PPOCRLabel Annotation Tool","text":"<p>PPOCRLabel is a semi-automatic graphical annotation tool tailored for OCR tasks, featuring automatic annotation and re-recognition of data using the built-in <code>PP-OCR</code> model. Written in <code>Python3</code> and <code>PyQT5</code>, it supports rectangular box annotation, table annotation, irregular text annotation, and key information annotation modes. In OCR annotation tasks, labels are stored as <code>txt</code> files.</p>"},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#22-installation-and-running-ppocrlabel","title":"2.2 Installation and Running PPOCRLabel","text":"<p>PPOCRLabel can be launched through whl packages or Python scripts. The whl package method is more convenient, and only this method is provided here:</p> <ul> <li>For Windows: <pre><code>pip install PPOCRLabel  # Installation\n\n# Start by selecting the labeling mode\nPPOCRLabel --lang ch  # Start in [Normal Mode] for [Detection + Recognition] scenarios\n</code></pre></li> </ul> <p>[!NOTE] Installing PPOCRLabel via whl packages will automatically download the <code>paddleocr</code> whl package. The shapely dependency may encounter a \"[winRrror 126] The specified module could not be found.\" error. It is recommended to download and install the shapely installation package separately.</p> <ul> <li>For MacOS: <pre><code>pip3 install PPOCRLabel\npip3 install opencv-contrib-python-headless==4.2.0.32 # Add \"-i https://mirror.baidu.com/pypi/simple\" for faster downloads\n\n# Start by selecting the labeling mode\nPPOCRLabel --lang ch  # Start in [Normal Mode] for [Detection + Recognition] scenarios\n</code></pre></li> </ul>"},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#23-annotation-process-for-text-detection-and-text-recognition","title":"2.3 Annotation Process for Text Detection and Text Recognition","text":"<ol> <li>Installation and Running: Use the above commands to install and run the program.</li> <li>Open Folder: Click \"File\" - \"Open Directory\" to select the folder containing images to be annotated.</li> </ol> <p> 3. Auto Annotation: Click \"Auto Annotation\" to use the PP-OCR ultra-lightweight model to automatically annotate images with a file status of \"X\".</p> <p> 4. Manual Annotation: Click \"Rectangle Annotation\" (recommended to press \"W\" directly in English mode), and users can manually draw bounding boxes for parts undetected by the model in the current image. Press \"Q\" on the keyboard to use the four-point annotation mode (or click \"Edit\" - \"Four-point Annotation\"), and users click 4 points in sequence, double-clicking the left mouse button to indicate completion. 5. After drawing the bounding box, click \"Confirm\", and the detection box will be pre-assigned a \"To Be Recognized\" label. 6. Re-Recognition: After drawing/adjusting all detection boxes in the image, click \"Re-Recognition\", and the PP-OCR model will re-recognize all detection boxes in the current image.</p> <p>  7. Content Modification: Click on the recognition result to manually modify inaccurate recognition results. 8. Confirm Annotation: Click \"Confirm\", the image status changes to \"\u221a\", and proceed to the next image. 9. Deletion: Click \"Delete Image\", and the image will be deleted to the Recycle Bin. 10. Export Results: Users can manually export results through \"File - Export Annotation Results\" or enable automatic export by clicking \"File - Auto Export Annotation Results\". Manually confirmed annotations will be stored in <code>Label.txt</code> under the opened image folder. Clicking \"File - Export Recognition Results\" in the menu bar will save the recognition training data of such images in the <code>crop_img</code> folder, and the recognition labels will be saved in <code>rec_gt.txt</code>.</p> <p> Notes:</p> <ul> <li>PPOCRLabel uses folders as the basic unit for labeling. After opening the folder containing images to be labeled, the images will not be displayed in the window bar. Instead, clicking \"Select Folder\" will directly import the images under the folder into the program.</li> <li>The image status indicates whether the current image has been manually saved by the user. An \"X\" indicates it has not been manually saved, while a \"\u221a\" indicates it has. After clicking the \"Auto Label\" button, PPOCRLabel will not relabel images with a status of \"\u221a\".</li> <li>Clicking \"Re-recognize\" will overwrite the recognition results in the image. Therefore, if manual changes have been made to the recognition results before this, they may be altered after re-recognition.</li> <li>Files generated by PPOCRLabel are placed in the folder containing the labeled images, including the following types. Do not manually modify their contents as it may cause program exceptions.</li> </ul> File Name Description <code>Label.txt</code> Detection labels, which can be directly used for PPOCR detection model training. The program automatically writes to this file every 5 confirmed detection results, or when the application is closed or the file path is changed. <code>fileState.txt</code> Image status marker file, which saves the names of images in the current folder that have been manually confirmed by the user. <code>Cache.cach</code> Cache file, which saves the results of automatic recognition by the model. <code>rec_gt.txt</code> Recognition labels. Can be directly used for PPOCR recognition model training. Generated by manually clicking \"Export Recognition Results\" in the menu bar. <code>crop_img</code> Recognition data. Images cropped according to the detection boxes. Generated simultaneously with <code>rec_gt.txt</code>. <p>If data partitioning is required, follow these steps:</p> <p><pre><code>cd ./PPOCRLabel # Switch to the PPOCRLabel folder\npython gen_ocr_train_val_test.py --trainValTestRatio 7:3:0 --datasetRootPath ../train_data\n</code></pre> * <code>trainValTestRatio</code> is the ratio for dividing images into training, validation, and test sets. Set it according to your needs. The default is <code>6:2:2</code>. * <code>datasetRootPath</code> is the full path to the dataset labeled by PPOCRLabel. The default path is <code>PaddleOCR/train_data</code>. Before partitioning the dataset, the structure should be as follows:</p> <p><pre><code>|-train_data\n  |-crop_img\n    |- word_001_crop_0.png\n    |- word_002_crop_0.jpg\n    |- word_003_crop_0.jpg\n    | ...\n  | Label.txt\n  | rec_gt.txt\n  |- word_001.png\n  |- word_002.jpg\n  |- word_003.jpg\n  | ...\n</code></pre> For more tool usage instructions, please refer to Detailed Instructions</p>"},{"location":"en/data_annotations/ocr_modules/text_detection_recognition.html#3-data-format","title":"3. Data Format","text":"<p>PaddleX defines a dataset named <code>TextDetDataset</code> specifically for text detection tasks. The organized and annotated data should follow the following file organization and naming conventions:</p> <p><pre><code>dataset_dir     # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images      # Directory to store images, the directory name can be changed but should correspond to the content in train.txt and val.txt\n\u251c\u2500\u2500 train.txt   # Annotation file for the training set, the file name cannot be changed. Example content: images/img_0.jpg \\t [{\"transcription\": \"MASA\", \"points\": [[310, 104], [416, 141], [418, 216], [312, 179]]}, {...}]\n\u2514\u2500\u2500 val.txt     # Annotation file for the validation set, the file name cannot be changed. Example content: images/img_61.jpg \\t [{\"transcription\": \"TEXT\", \"points\": [[31, 10], [310, 140], [420, 220], [310, 170]]}, {...}]\n</code></pre> Place the images in the <code>images</code> directory, and rename the generated <code>Label.txt</code> annotation file to <code>train.txt</code> for the training set. Similarly, rename the <code>Label.txt</code> annotation file for the validation set to <code>val.txt</code>. Both files should be placed in the <code>dataset_dir</code> (the name of which can be changed) directory. Note that the image paths in <code>train.txt</code>/<code>val.txt</code> should be in the format <code>images/xxx</code>.</p> <p>Each line in the annotation files contains the path to an image and a list of dictionaries. The path and the list must be separated by a tab character \u2018\\t\u2019, not spaces.</p> <p>For the list of dictionaries, the <code>points</code> key represents the coordinates <code>(x, y)</code> of the four vertices of the text box, starting from the top-left vertex and proceeding clockwise. The <code>transcription</code> key indicates the text within the text box. If the <code>transcription</code> content is \"###\", it indicates that the text box is invalid and will not be used for training. For reference, see the example dataset.</p> <p>If you use PPOCRLabel to annotate your data, simply rename <code>det_gt_train.txt</code> in the text detection (det) directory to <code>train.txt</code> and <code>det_gt_test.txt</code> to <code>val.txt</code> after dividing your dataset.</p>"},{"location":"en/data_annotations/time_series_modules/time_series_anomaly_detection.html","title":"PaddleX Time Series Anomaly Detection Task Module Data Annotation Tutorial","text":"<p>Time series anomaly detection is an unsupervised learning task, thus there is no need to annotate training data. The collected training samples should ideally consist solely of normal data, i.e., no anomalies (represented by 0 for no anomaly). In the training set, the label column indicating anomalies can be set to 0 or omitted entirely. For the validation set, to evaluate model accuracy, annotations are required. For points that are anomalous at a specific time, set the label for that time point to 1, and the labels for other normal time points to 0.</p>"},{"location":"en/data_annotations/time_series_modules/time_series_classification.html","title":"PaddleX Time Series Classification Task Data Annotation Tutorial","text":"<p>When annotating time series classification data, based on the collected real-world data, clearly define the classification objectives of the time series data and establish corresponding classification labels. In the <code>csv</code> file, set a <code>group_id</code> column to represent samples, where the same <code>group_id</code> indicates that the data points belong to the same sample. For example, in stock price prediction, labels might be \"Rise\" (0), \"Flat\" (1), or \"Fall\" (2). For a time series that exhibits an \"upward\" trend over a period, it can be considered as one sample (group), where each time point in this series shares the same <code>group_id</code>, and the label column is set to 0. Similarly, for a time series that exhibits a \"downward\" trend, it forms another sample (group), where each time point shares the same <code>group_id</code>, and the label column is set to 2. As shown in the figure below, the green box represents one sample (group_id=0) with a label of 1, while the red box represents another time series classification sample (group_id=1) with a label of 0. If there are n samples, you can set group_id=0,...,n-1; each sample has a length (time=0,...,9) of 10, and the feature dimensions (dim_0, dim_1) are 2.</p> <p></p>"},{"location":"en/data_annotations/time_series_modules/time_series_forecasting.html","title":"PaddleX Time Series Forecasting Task Data Annotation Tutorial","text":"<p>Data for time series forecasting tasks does not require annotation. Simply collect real-world data and arrange it in a csv file in chronological order. During training, the data will be automatically segmented into multiple time slices to form training samples, as shown in the figure below. The historical time series data and future sequences represent the input data for training the model and its corresponding prediction targets, respectively. To ensure data quality and completeness, missing values can be filled based on expert experience or statistical methods.</p> <p></p>"},{"location":"en/installation/installation.html","title":"PaddleX Local Installation Tutorial","text":"<p>\u2757Before installing PaddleX, please ensure you have a basic Python environment (Note: Currently supports Python 3.8 to Python 3.10, with more Python versions being adapted).</p>"},{"location":"en/installation/installation.html#1-quick-installation","title":"1. Quick Installation","text":"<p>Welcome to PaddleX, Baidu's low-code development tool for AI. Before we dive into the local installation process, please clarify your development needs and choose the appropriate installation mode.</p> <p>PaddleX offers two installation modes: Wheel Package Installation and Plugin Installation. Below, we introduce their respective application scenarios:</p>"},{"location":"en/installation/installation.html#11-wheel-package-installation-mode","title":"1.1 Wheel Package Installation Mode","text":"<p>If your use case for PaddleX involves model inference and integration, we recommend the more convenient and lightweight Wheel package installation mode.</p> <p>After installing PaddlePaddle (refer to the PaddlePaddle Local Installation Tutorial), you can quickly install the PaddleX Wheel package by executing the following commands:</p> <p>\u2757 Note: Please ensure that PaddlePaddle is successfully installed before proceeding to the next step.</p> <pre><code>pip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddlex-3.0.0b2-py3-none-any.whl\n</code></pre>"},{"location":"en/installation/installation.html#12-plugin-installation-mode","title":"1.2 Plugin Installation Mode","text":"<p>If your use case for PaddleX involves custom development (e.g. retraining models, fine-tuning models, customizing model structures, customizing inference codes, etc.), we recommend the more powerful plugin installation mode.</p> <p>After installing the PaddleX plugins you need, you can not only perform inference and integration with the supported models but also conduct advanced operations such as model training for custom development.</p> <p>The plugins supported by PaddleX are listed below. Please determine the name(s) of the plugin(s) you need based on your development requirements:</p> \ud83d\udc49 Plugin and Pipeline Correspondence (Click to Expand) Pipeline Module Corresponding Plugin General Image Classification Image Classification <code>PaddleClas</code> General Object Detection Object Detection <code>PaddleDetection</code> General Semantic Segmentation Semantic Segmentation <code>PaddleSeg</code> General Instance Segmentation Instance Segmentation <code>PaddleDetection</code> General OCR Text DetectionText Recognition <code>PaddleOCR</code> Table Recognition Layout Region DetectionTable Structure RecognitionText DetectionText Recognition <code>PaddleOCR</code><code>PaddleDetection</code> PP-ChatOCRv3-doc Table Structure RecognitionLayout Region DetectionText DetectionText RecognitionSeal Text DetectionText Image CorrectionDocument Image Orientation Classification <code>PaddleOCR</code><code>PaddleDetection</code><code>PaddleClas</code> Time Series Forecasting Time Series Forecasting Module <code>PaddleTS</code> Time Series Anomaly Detection Time Series Anomaly Detection Module <code>PaddleTS</code> Time Series Classification Time Series Classification Module <code>PaddleTS</code> Image Multi-Label Classification Image Multi-label Classification <code>PaddleClas</code> Small Object Detection Small Object Detection <code>PaddleDetection</code> Image Anomaly Detection Unsupervised Anomaly Detection <code>PaddleSeg</code> <p>If the plugin you need to install is <code>PaddleXXX</code>, after installing PaddlePaddle (refer to the PaddlePaddle Local Installation Tutorial), you can quickly install the corresponding PaddleX plugin by executing the following commands:</p> <pre><code>git clone https://github.com/PaddlePaddle/PaddleX.git\ncd PaddleX\npip install -e .\npaddlex --install PaddleXXX\n</code></pre> <p>\u2757 Note: The two installation methods are not mutually exclusive, and you can install both simultaneously.</p> <p>Next, we provide detailed installation tutorials for your reference. If you are using a Linux operating system, please refer to 2. Detailed Tutorial for Installing PaddleX on Linux.</p>"},{"location":"en/installation/installation.html#2-detailed-tutorial-for-installing-paddlex-on-linux","title":"2. Detailed Tutorial for Installing PaddleX on Linux","text":"<p>When installing PaddleX on Linux, we strongly recommend using the official PaddleX Docker image. Alternatively, you can use other custom installation methods.</p> <p>When using the official Docker image, PaddlePaddle, PaddleX (including the wheel package and all plugins), and the corresponding CUDA environment are already pre-installed. You can simply obtain the Docker image and start the container to begin using it.</p> <p>When using custom installation methods, you need to first install the PaddlePaddle framework, then obtain the PaddleX source code, and finally choose the PaddleX installation mode.</p>"},{"location":"en/installation/installation.html#21-get-paddlex-based-on-docker","title":"2.1 Get PaddleX based on Docker","text":"<p>Using the PaddleX official Docker image, create a container called 'paddlex' and map the current working directory to the '/paddle' directory inside the container by following the command.</p> <p>If your Docker version &gt;= 19.03, please use:</p> <pre><code># For CPU\ndocker run --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-cpu /bin/bash\n\n# For GPU\n# For CUDA11.8\ndocker run --gpus all --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-gpu-cuda11.8-cudnn8.6-trt8.5 /bin/bash\n\n# For CUDA12.3\ndocker run --gpus all --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6 /bin/bash\n</code></pre> <ul> <li>If your Docker version &lt;= 19.03 and &gt;= 17.06, please use:</li> </ul>  Click Here <pre><code># For CPU\ndocker run --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-cpu /bin/bash\n\n# For GPU\n# For CUDA11.8\nnvidia-docker run --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-gpu-cuda11.8-cudnn8.6-trt8.5 /bin/bash\n\n# For CUDA12.3\nnvidia-docker run --name paddlex -v $PWD:/paddle --shm-size=8g --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlex/paddlex:paddlex3.0.0b2-paddlepaddle3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6 /bin/bash\n</code></pre> <ul> <li> <p>If your Docker version &lt;= 17.06, please update your Docker.</p> </li> <li> <p>If you want to delve deeper into the principles or usage of Docker, please refer to the Docker Official Website or the Docker Official Tutorial.</p> </li> </ul>"},{"location":"en/installation/installation.html#22-custom-installation-of-paddlex","title":"2.2 Custom Installation of PaddleX","text":"<p>Before installation, please ensure you have completed the local installation of PaddlePaddle by referring to the PaddlePaddle Local Installation Tutorial.</p>"},{"location":"en/installation/installation.html#221-obtain-paddlex-source-code","title":"2.2.1 Obtain PaddleX Source Code","text":"<p>Next, use the following command to obtain the latest PaddleX source code from GitHub:</p> <p><pre><code>git clone https://github.com/PaddlePaddle/PaddleX.git\n</code></pre> If accessing GitHub is slow, you can download from Gitee instead, using the following command:</p> <pre><code>git clone https://gitee.com/paddlepaddle/PaddleX.git\n</code></pre>"},{"location":"en/installation/installation.html#222-install-paddlex","title":"2.2.2 Install PaddleX","text":"<p>After obtaining the latest PaddleX source code, you can choose between Wheel package installation mode or plugin installation mode.</p> <ul> <li>If you choose Wheel package installation mode, execute the following commands:</li> </ul> <pre><code>cd PaddleX\n\n# Install PaddleX whl\n# -e: Install in editable mode, so changes to the current project's code will directly affect the installed PaddleX Wheel\npip install -e .\n</code></pre> <ul> <li>If you choose plugin installation mode and the plugin you need is named PaddleXXX (there can be multiple), execute the following commands:</li> </ul> <pre><code>cd PaddleX\n\n# Install PaddleX whl\n# -e: Install in editable mode, so changes to the current project's code will directly affect the installed PaddleX Wheel\npip install -e .\n\n# Install PaddleX plugins\npaddlex --install PaddleXXX\n</code></pre> <p>For example, if you need to install the PaddleOCR and PaddleClas plugins, execute the following commands to install the plugins:</p> <pre><code># Install PaddleOCR and PaddleClas plugins\npaddlex --install PaddleOCR PaddleClas\n</code></pre> <p>If you need to install all plugins, you do not need to specify the plugin names, just execute the following command:</p> <pre><code># Install all PaddleX plugins\npaddlex --install\n</code></pre> <p>The default clone source for plugins is github.com, but it also supports gitee.com as a clone source. You can specify the clone source using <code>--platform</code>.</p> <p>For example, if you need to use gitee.com as the clone source to install all PaddleX plugins, just execute the following command:</p> <pre><code># Install PaddleX plugins\npaddlex --install --platform gitee.com\n</code></pre> <p>After installation, you will see the following prompt:</p> <pre><code>All packages are installed.\n</code></pre> <p>For PaddleX installation on more hardware environments, please refer to the PaddleX Multi-hardware Usage Guide</p>"},{"location":"en/installation/paddlepaddle_install.html","title":"PaddlePaddle Local Installation Tutorial","text":"<p>When installing PaddlePaddle, you can choose to install it via Docker or pip.</p>"},{"location":"en/installation/paddlepaddle_install.html#installing-paddlepaddle-via-docker","title":"Installing PaddlePaddle via Docker","text":"<p>If you choose to install via Docker, please refer to the following commands to use the official PaddlePaddle Docker image to create a container named <code>paddlex</code> and map the current working directory to the <code>/paddle</code> directory inside the container:</p> <p>If your Docker version &gt;= 19.03, please use:</p> <pre><code># For CPU users:\ndocker run --name paddlex -v $PWD:/paddle --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2 /bin/bash\n\n# For GPU users:\n# CUDA 11.8 users\ndocker run --gpus all --name paddlex -v $PWD:/paddle --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2-gpu-cuda11.8-cudnn8.6-trt8.5 /bin/bash\n\n# CUDA 12.3 users\ndocker run --gpus all --name paddlex -v $PWD:/paddle  --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6 /bin/bash\n</code></pre> <ul> <li>If your Docker version &lt;= 19.03 and &gt;= 17.06, please use:</li> </ul>  Click Here <pre><code># For CPU users:\ndocker run --name paddlex -v $PWD:/paddle --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2 /bin/bash\n\n# For GPU users:\n# CUDA 11.8 users\nnvidia-docker run --name paddlex -v $PWD:/paddle --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2-gpu-cuda11.8-cudnn8.6-trt8.5 /bin/bash\n\n# CUDA 12.3 users\nnvidia-docker run --name paddlex -v $PWD:/paddle  --shm-size=8G --network=host -it ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddle:3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6 /bin/bash\n</code></pre> <ul> <li> <p>If your Docker version &lt;= 17.06, please update your Docker.</p> </li> <li> <p>Note: For more official PaddlePaddle Docker images, please refer to the PaddlePaddle official website</p> </li> </ul>"},{"location":"en/installation/paddlepaddle_install.html#installing-paddlepaddle-via-pip","title":"Installing PaddlePaddle via pip","text":"<p>If you choose to install via pip, please refer to the following commands to install PaddlePaddle in your current environment using pip:</p> <p><pre><code># CPU\npython -m pip install paddlepaddle==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cpu/\n\n# GPU, this command is only suitable for machines with CUDA version 11.8\npython -m pip install paddlepaddle-gpu==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n\n# GPU, this command is only suitable for machines with CUDA version 12.3\npython -m pip install paddlepaddle-gpu==3.0.0b2 -i https://www.paddlepaddle.org.cn/packages/stable/cu123/\n</code></pre> Note: For more PaddlePaddle Wheel versions, please refer to the PaddlePaddle official website.</p> <p>For installing PaddlePaddle on other hardware, please refer to PaddleX Multi-hardware Usage Guide.</p> <p>After installation, you can verify if PaddlePaddle is successfully installed using the following command:</p> <p><pre><code>python -c \"import paddle; print(paddle.__version__)\"\n</code></pre> If the installation is successful, the following content will be output:</p> <pre><code>3.0.0-beta2\n</code></pre> <p>\u2757 Note: If you encounter any issues during the installation process, feel free to submit an issue in the Paddle repository.</p>"},{"location":"en/module_usage/instructions/benchmark.html","title":"Model Inference Benchmark","text":"<p>PaddleX support to benchmark model inference. Just set the related flags:</p> <ul> <li><code>PADDLE_PDX_INFER_BENCHMARK</code>: <code>True</code> means enable benchmark. <code>False</code> by default;</li> <li><code>PADDLE_PDX_INFER_BENCHMARK_WARMUP</code>: Number of warmup. Using random data to infer before testing benchmark if <code>input</code> is set to <code>None</code>. <code>0</code> by default;</li> <li><code>PADDLE_PDX_INFER_BENCHMARK_DATA_SIZE</code>: The size of randomly generated data. Valid only when <code>input</code> is set to <code>None</code>. <code>224</code> by default;</li> <li><code>PADDLE_PDX_INFER_BENCHMARK_ITER</code>: Number of benchmark testing using random data. Valid only when <code>input</code> is set to <code>None</code>. <code>10</code> by default;</li> <li><code>PADDLE_PDX_INFER_BENCHMARK_OUTPUT</code>: The directory to save benchmark result. <code>None</code> by default, that means not save.</li> </ul> <p>The example is as follows\uff1a</p> <pre><code>PADDLE_PDX_INFER_BENCHMARK=True \\\nPADDLE_PDX_INFER_BENCHMARK_WARMUP=5 \\\nPADDLE_PDX_INFER_BENCHMARK_DATA_SIZE=320 \\\nPADDLE_PDX_INFER_BENCHMARK_ITER=10 \\\nPADDLE_PDX_INFER_BENCHMARK_OUTPUT=./benchmark \\\npython main.py \\\n    -c ./paddlex/configs/object_detection/PicoDet-XS.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=None \\\n    -o Predict.batch_size=2 \\\n    -o Predict.input=None\n</code></pre> <p>The benchmark infomation would be print:</p> <pre><code>+----------------+-----------------+-----------------+------------------------+\n|   Component    | Total Time (ms) | Number of Calls | Avg Time Per Call (ms) |\n+----------------+-----------------+-----------------+------------------------+\n|    ReadCmp     |   99.60412979   |        10       |       9.96041298       |\n|     Resize     |   17.01641083   |        20       |       0.85082054       |\n|   Normalize    |   44.61312294   |        20       |       2.23065615       |\n|   ToCHWImage   |    0.03385544   |        20       |       0.00169277       |\n|    Copy2GPU    |   13.46874237   |        10       |       1.34687424       |\n|     Infer      |   71.31743431   |        10       |       7.13174343       |\n|    Copy2CPU    |    0.39076805   |        10       |       0.03907681       |\n| DetPostProcess |    0.36168098   |        20       |       0.01808405       |\n+----------------+-----------------+-----------------+------------------------+\n+-------------+-----------------+---------------------+----------------------------+\n|    Stage    | Total Time (ms) | Number of Instances | Avg Time Per Instance (ms) |\n+-------------+-----------------+---------------------+----------------------------+\n|  PreProcess |   161.26751900  |          20         |         8.06337595         |\n|  Inference  |   85.17694473   |          20         |         4.25884724         |\n| PostProcess |    0.36168098   |          20         |         0.01808405         |\n|   End2End   |   256.90770149  |          20         |        12.84538507         |\n|    WarmUp   |  5412.37807274  |          10         |        541.23780727        |\n+-------------+-----------------+---------------------+----------------------------+\n</code></pre> <p>The first table show the benchmark infomation by each component(<code>Component</code>), include <code>Total Time</code> (unit is \"ms\"), <code>Number of Calls</code> and <code>Avg Time Per Call</code>  (unit is \"ms\"). <code>Avg Time Per Call</code> is <code>Total Time</code> devide by <code>Number of Calls</code>. It should be noted that the <code>Number of Calls</code> is the number of times the component has been called.</p> <p>And the second table show the benchmark infomation by different stages: <code>WarmUp</code>, <code>PreProcess</code>, <code>Inference</code>, <code>PostProcess</code> and <code>End2End</code>. Different from the first table, <code>Number of Instances</code> is the number of instances (samples), not the number of calls.</p> <p>Meanwhile, the benchmark infomation would be saved to local files (<code>detail.csv</code> and <code>summary.csv</code>) if you set <code>PADDLE_PDX_INFER_BENCHMARK_OUTPUT</code>:</p> <pre><code>Component,Total Time (ms),Number of Calls,Avg Time Per Call (ms)\nReadCmp,99.60412979125977,10,9.960412979125977\nResize,17.01641082763672,20,0.8508205413818359\nNormalize,44.61312294006348,20,2.230656147003174\nToCHWImage,0.033855438232421875,20,0.0016927719116210938\nCopy2GPU,13.468742370605469,10,1.3468742370605469\nInfer,71.31743431091309,10,7.131743431091309\nCopy2CPU,0.39076805114746094,10,0.039076805114746094\nDetPostProcess,0.3616809844970703,20,0.018084049224853516\n</code></pre> <pre><code>Stage,Total Time (ms),Number of Instances,Avg Time Per Instance (ms)\nPreProcess,161.26751899719238,20,8.06337594985962\nInference,85.17694473266602,20,4.258847236633301\nPostProcess,0.3616809844970703,20,0.018084049224853516\nEnd2End,256.90770149230957,20,12.845385074615479\nWarmUp,5412.3780727386475,10,541.2378072738647\n</code></pre>"},{"location":"en/module_usage/instructions/config_parameters_common.html","title":"PaddleX Common Model Configuration File Parameter Explanation","text":""},{"location":"en/module_usage/instructions/config_parameters_common.html#global","title":"Global","text":"Parameter Name Data Type Description Default Value model str Specifies the model name Model name specified in the YAML file mode str Specifies the mode (check_dataset/train/evaluate/export/predict) check_dataset dataset_dir str Path to the dataset Dataset path specified in the YAML file device str Specifies the device to use Device ID specified in the YAML file output str Output path \"output\""},{"location":"en/module_usage/instructions/config_parameters_common.html#checkdataset","title":"CheckDataset","text":"Parameter Name Data Type Description Default Value convert.enable bool Whether to convert the dataset format; Image classification, pedestrian attribute recognition, vehicle attribute recognition, document orientation classification, object detection, pedestrian detection, vehicle detection, face detection, anomaly detection, text detection, seal text detection, text recognition, table recognition, image rectification, and layout area detection do not support data format conversion; Image multi-label classification supports COCO format conversion; Image feature, semantic segmentation, and instance segmentation support LabelMe format conversion; Object detection and small object detection support VOC and LabelMe format conversion; Formula recognition supports PKL format conversion; Time series prediction, time series anomaly detection, and time series classification support xlsx and xls format conversion False convert.src_dataset_type str The source dataset format to be converted null split.enable bool Whether to re-split the dataset False split.train_percent int Sets the percentage of the training set, an integer between 0-100, ensuring the sum with val_percent is 100; null split.val_percent int Sets the percentage of the validation set, an integer between 0-100, ensuring the sum with train_percent is 100; null split.gallery_percent int Sets the percentage of gallery samples in the validation set, an integer between 0-100, ensuring the sum with train_percent and query_percent is 100; This parameter is only used in the image feature module null split.query_percent int Sets the percentage of query samples in the validation set, an integer between 0-100, ensuring the sum with train_percent and gallery_percent is 100; This parameter is only used in the image feature module null"},{"location":"en/module_usage/instructions/config_parameters_common.html#train","title":"Train","text":"Parameter Name Data Type Description Default Value num_classes int Number of classes in the dataset; If you need to train on a private dataset, you need to set this parameter; Image rectification, text detection, seal text detection, text recognition, formula recognition, table recognition, time series prediction, time series anomaly detection, and time series classification do not support this parameter Number of classes specified in the YAML file epochs_iters int Number of times the model repeats learning the training data Number of iterations specified in the YAML file batch_size int Training batch size Training batch size specified in the YAML file learning_rate float Initial learning rate Initial learning rate specified in the YAML file pretrain_weight_path str Pre-trained weight path null warmup_steps int Warm-up steps Warm-up steps specified in the YAML file resume_path str Model resume path after interruption null log_interval int Training log printing interval Training log printing interval specified in the YAML file eval_interval int Model evaluation interval Model evaluation interval specified in the YAML file save_interval int Model saving interval; not supported for anomaly detection, semantic segmentation, image rectification, time series forecasting, time series anomaly detection, and time series classification Model saving interval specified in the YAML file"},{"location":"en/module_usage/instructions/config_parameters_common.html#evaluate","title":"Evaluate","text":"Parameter Name Data Type Description Default Value weight_path str Evaluation model path Default local path from training output, when specified as None, indicates using official weights log_interval int Evaluation log printing interval Evaluation log printing interval specified in the YAML file"},{"location":"en/module_usage/instructions/config_parameters_common.html#export","title":"Export","text":"Parameter Name Data Type Description Default Value weight_path str Dynamic graph weight path for exporting the model Default local path from training output, when specified as None, indicates using official weights"},{"location":"en/module_usage/instructions/config_parameters_common.html#predict","title":"Predict","text":"Parameter Name Data Type Description Default Value batch_size int Prediction batch size The prediction batch size specified in the YAML file model_dir str Path to the prediction model The default local inference model path produced by training. When specified as None, it indicates the use of official weights input str Path to the prediction input The prediction input path specified in the YAML file"},{"location":"en/module_usage/instructions/config_parameters_time_series.html","title":"PaddleX Time Series Task Model Configuration File Parameters Explanation","text":""},{"location":"en/module_usage/instructions/config_parameters_time_series.html#global","title":"Global","text":"Parameter Name Data Type Description Default Value model str Specifies the model name Model name specified in the YAML file mode str Specifies the mode (check_dataset/train/evaluate/export/predict) check_dataset dataset_dir str Path to the dataset Dataset path specified in the YAML file device str Specifies the device to use Device ID specified in the YAML file output str Output path \"output\""},{"location":"en/module_usage/instructions/config_parameters_time_series.html#checkdataset","title":"CheckDataset","text":"Parameter Name Data Type Description Default Value convert.enable bool Whether to convert the dataset format; time series prediction, anomaly detection, and classification support data conversion from xlsx and xls formats False convert.src_dataset_type str The source dataset format to be converted null split.enable bool Whether to re-split the dataset False split.train_percent int Sets the percentage of the training set, an integer between 0-100, ensuring the sum with val_percent is 100; null split.val_percent int Sets the percentage of the validation set, an integer between 0-100, ensuring the sum with train_percent is 100; null"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#train","title":"Train","text":""},{"location":"en/module_usage/instructions/config_parameters_time_series.html#common-parameters-for-time-series-tasks","title":"Common Parameters for Time Series Tasks","text":"Parameter Name Data Type Description Default Value epochs_iters int The number of times the model repeats learning the training data Number of iterations specified in the YAML file batch_size int Batch size Batch size specified in the YAML file learning_rate float Initial learning rate Initial learning rate specified in the YAML file time_col str Time column, set the column name of the time series dataset's time column based on your data. Time column specified in the YAML file freq str or int Frequency, set the time frequency based on your data, e.g., 1min, 5min, 1h. Frequency specified in the YAML file"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#time-series-forecasting-parameters","title":"Time Series Forecasting Parameters","text":"Parameter Name Data Type Description Default Value target_cols str Target variable column(s), set the column name(s) of the target variable(s) in the time series dataset, can be multiple, separated by commas OT input_len int For time series forecasting tasks, this parameter represents the length of historical time series input to the model; the input length should be considered in conjunction with the prediction length, generally, the larger the setting, the more historical information can be referenced, and the higher the model accuracy. 96 predict_len int The length of the future sequence that you want the model to predict; the prediction length should be considered in conjunction with the actual scenario, generally, the larger the setting, the longer the future sequence you want to predict, and the lower the model accuracy. 96 patience int Early stopping mechanism parameter, indicating how many times the model's performance on the validation set can be continuously unimproved before stopping training; a larger patience value generally results in longer training time. 10"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#time-series-anomaly-detection","title":"Time Series Anomaly Detection","text":"Parameter Name Data Type Description Default Value input_len int For time series anomaly detection tasks, this parameter represents the length of the time series input to the model, which will slice the time series according to this length to predict whether there is an anomaly in this segment of the time series; the input length should be considered in conjunction with the actual scenario. For example, an input length of 96 indicates that you want to predict whether there are anomalies in 96 time points. 96 feature_cols str Feature variables indicating variables related to whether the device is abnormal, e.g., whether the device is abnormal may be related to the heat dissipation during its operation. Set the column name(s) of the feature variable(s) based on your data, can be multiple, separated by commas. feature_0,feature_1 label_col str Represents the number indicating whether a time series point is abnormal, with 1 for abnormal points and 0 for normal points. label"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#time-series-classification","title":"Time Series Classification","text":"Parameter Name Data Type Description Default Value target_cols str Feature variable columns used for category discrimination. You need to set the column names of the target variables in the time series dataset based on your own data. It can be multiple, separated by commas. dim_0,dim_1,dim_2 freq str or int Frequency, which needs to be set based on your own data. Examples of time frequencies include: 1min, 5min, 1h. 1 group_id str A group ID represents a time series sample. Time series sequences with the same ID constitute a sample. Set the column name of the specified group ID based on your own data, e.g., group_id. group_id static_cov_cols str Represents the category number column of the time series. The labels of the same sample are the same. Set the column name of the category based on your own data, e.g., label. label"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#evaluate","title":"Evaluate","text":"Parameter Name Data Type Description Default Value weight_path str Evaluation model path Default local path from training output, when specified as None, indicates using official weights"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#export","title":"Export","text":"Parameter Name Data Type Description Default Value weight_path str Dynamic graph weight path for exporting the model Default local path from training output, when specified as None, indicates using official weights"},{"location":"en/module_usage/instructions/config_parameters_time_series.html#predict","title":"Predict","text":"Parameter Name Data Type Description Default Value batch_size int Prediction batch size The prediction batch size specified in the YAML file model_dir str Path to the prediction model The default local inference model path produced by training. When specified as None, it indicates the use of official weights input str Path to the prediction input The prediction input path specified in the YAML file"},{"location":"en/module_usage/instructions/model_python_API.html","title":"PaddleX Single Model Python Usage Instructions","text":"<p>Before using Python scripts for single model quick inference, please ensure you have completed the installation of PaddleX following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/instructions/model_python_API.html#i-usage-example","title":"I. Usage Example","text":"<p>Taking the image classification model as an example, the usage is as follows:</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0\")\noutput = model.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> In short, just three steps:</p> <ul> <li>Call the <code>create_model()</code> method to instantiate the prediction model object;</li> <li>Call the <code>predict()</code> method of the prediction model object to perform inference prediction;</li> <li>Call <code>print()</code>, <code>save_to_xxx()</code> and other related methods to visualize or save the prediction results.</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#ii-api-description","title":"II. API Description","text":""},{"location":"en/module_usage/instructions/model_python_API.html#1-instantiate-the-prediction-model-object-by-calling-the-create_model-method","title":"1. Instantiate the Prediction Model Object by Calling the <code>create_model()</code> Method","text":"<ul> <li><code>create_model</code>: Instantiate the prediction model object;</li> <li>Parameters:<ul> <li><code>model_name</code>: <code>str</code> type, model name or local inference model file path, such as \"PP-LCNet_x1_0\", \"/path/to/PP-LCNet_x1_0_infer/\";</li> <li><code>device</code>: <code>str</code> type, used to set the model inference device, such as \"cpu\", \"gpu:2\" for GPU settings;</li> <li><code>pp_option</code>: <code>PaddlePredictorOption</code> type, used to set the model inference backend;</li> </ul> </li> <li>Return Value: <code>BasePredictor</code> type.</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#2-perform-inference-prediction-by-calling-the-predict-method-of-the-prediction-model-object","title":"2. Perform Inference Prediction by Calling the <code>predict()</code> Method of the Prediction Model Object","text":"<ul> <li><code>predict</code>: Use the defined prediction model to predict the input data;</li> <li>Parameters:<ul> <li><code>input</code>: Any type, supports str type representing the path of the file to be predicted, or a directory containing files to be predicted, or a network URL; for CV models, supports numpy.ndarray representing image data; for TS models, supports pandas.DataFrame type data; also supports list types composed of the above types;</li> </ul> </li> <li>Return Value: <code>generator</code>, returns the prediction result of one sample per call;</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#3-visualize-the-prediction-results","title":"3. Visualize the Prediction Results","text":"<p>The prediction results support to be accessed, visualized, and saved, which can be achieved through corresponding attributes or methods, specifically as follows:</p>"},{"location":"en/module_usage/instructions/model_python_API.html#attributes","title":"Attributes:","text":"<ul> <li><code>str</code>: Representation of the prediction result in <code>str</code> type;</li> <li>Returns: A <code>str</code> type, the string representation of the prediction result.</li> <li><code>json</code>: The prediction result in JSON format;</li> <li>Returns: A <code>dict</code> type.</li> <li><code>img</code>: The visualization image of the prediction result;</li> <li>Returns: A <code>PIL.Image</code> type.</li> <li><code>html</code>: The HTML representation of the prediction result;</li> <li>Returns: A <code>str</code> type.</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#methods","title":"Methods:","text":"<ul> <li><code>print()</code>: Outputs the prediction result. Note that when the prediction result is not convenient for direct output, relevant content will be omitted;</li> <li>Parameters:<ul> <li><code>json_format</code>: <code>bool</code> type, default is <code>False</code>, indicating that json formatting is not used;</li> <li><code>indent</code>: <code>int</code> type, default is <code>4</code>, valid when <code>json_format</code> is <code>True</code>, indicating the indentation level for json formatting;</li> <li><code>ensure_ascii</code>: <code>bool</code> type, default is <code>False</code>, valid when <code>json_format</code> is <code>True</code>;</li> </ul> </li> <li>Return Value: None;</li> <li><code>save_to_json()</code>: Saves the prediction result as a JSON file. Note that when the prediction result contains data that cannot be serialized in JSON, automatic format conversion will be performed to achieve serialization and saving;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result;</li> <li><code>indent</code>: <code>int</code> type, default is <code>4</code>, valid when <code>json_format</code> is <code>True</code>, indicating the indentation level for json formatting;</li> <li><code>ensure_ascii</code>: <code>bool</code> type, default is <code>False</code>, valid when <code>json_format</code> is <code>True</code>;</li> </ul> </li> <li>Return Value: None;</li> <li><code>save_to_img()</code>: Visualizes the prediction result and saves it as an image;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_csv()</code>: Saves the prediction result as a CSV file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_html()</code>: Saves the prediction result as an HTML file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_xlsx()</code>: Saves the prediction result as an XLSX file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#4-inference-backend-configuration","title":"4. Inference Backend Configuration","text":"<p>PaddleX supports configuring the inference backend through <code>PaddlePredictorOption</code>. Relevant APIs are as follows:</p>"},{"location":"en/module_usage/instructions/model_python_API.html#attributes_1","title":"Attributes:","text":"<ul> <li><code>device</code>: Inference device;</li> <li>Supports setting the device type and card number represented by <code>str</code>. Device types include 'gpu', 'cpu', 'npu', 'xpu', 'mlu'. When using an accelerator card, you can specify the card number, e.g., 'gpu:0' for GPU 0. The default is 'gpu:0';</li> <li>Return value: <code>str</code> type, the currently set inference device.</li> <li><code>run_mode</code>: Inference backend;</li> <li>Supports setting the inference backend as a <code>str</code> type, options include 'paddle', 'trt_fp32', 'trt_fp16', 'trt_int8', 'mkldnn', 'mkldnn_bf16'. 'mkldnn' is only selectable when the inference device is 'cpu'. The default is 'paddle';</li> <li>Return value: <code>str</code> type, the currently set inference backend.</li> <li><code>cpu_threads</code>: Number of CPU threads for the acceleration library, only valid when the inference device is 'cpu';</li> <li>Supports setting an <code>int</code> type for the number of CPU threads for the acceleration library during CPU inference;</li> <li>Return value: <code>int</code> type, the currently set number of threads for the acceleration library.</li> </ul>"},{"location":"en/module_usage/instructions/model_python_API.html#methods_1","title":"Methods:","text":"<ul> <li><code>get_support_run_mode</code>: Get supported inference backend configurations;</li> <li>Parameters: None;</li> <li>Return value: List type, the available inference backend configurations.</li> <li><code>get_support_device</code>: Get supported device types for running;</li> <li>Parameters: None;</li> <li>Return value: List type, the available device types.</li> <li><code>get_device</code>: Get the currently set device;</li> <li>Parameters: None;</li> <li>Return value: <code>str</code> type. ```</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html","title":"Unsupervised Anomaly Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#i-overview","title":"I. Overview","text":"<p>Unsupervised anomaly detection is a technology that automatically identifies and detects anomalies or rare samples that are significantly different from the majority of data in a dataset, without labels or with a small amount of labeled data. This technology is widely used in many fields such as industrial manufacturing quality control and medical diagnosis.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link ROCAUC\uff08Avg\uff09 Model Size (M) Description STFPMInference Model/Trained Model 0.962 22.5 An unsupervised anomaly detection algorithm based on representation consists of a pre-trained teacher network and a student network with the same structure. The student network detects anomalies by matching its own features with the corresponding features in the teacher network. <p>The above model accuracy indicators are measured from the MVTec_AD dataset.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>Before quick integration, you need to install the PaddleX wheel package. For the installation method of the wheel package, please refer to the PaddleX Local Installation Tutorial. After installing the wheel package, a few lines of code can complete the inference of the unsupervised anomaly detection module. You can switch models under this module freely, and you can also integrate the model inference of the unsupervised anomaly detection module into your project. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"STFPM\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"uad_grid.png\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on the usage of PaddleX's single-model inference API, please refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better unsupervised anomaly detection models. Before using PaddleX to develop unsupervised anomaly detection models, ensure you have installed the PaddleDetection plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides a data validation function for each module, and only data that passes the validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development based on the official demos. If you wish to use private datasets for subsequent model training, refer to the PaddleX Semantic Segmentation Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/mvtec_examples.tar -P ./dataset\ntar -xf ./dataset/mvtec_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/mvtec_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/000.png\",\n      \"check_dataset/demo_img/001.png\",\n      \"check_dataset/demo_img/002.png\"\n    ],\n    \"train_samples\": 264,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/000.png\",\n      \"check_dataset/demo_img/001.png\",\n      \"check_dataset/demo_img/002.png\"\n    ],\n    \"val_samples\": 57,\n    \"num_classes\": 231\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/mvtec_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"SegDataset\"\n}\n</code></pre> <p>The verification results mentioned above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Details of other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 264;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 57;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of validation samples in this dataset;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command is sufficient to complete model training, taking the training of STFPM as an example:</p> <p><pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/mvtec_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file of the model (here it is <code>STFPM.yaml</code>\uff0cWhen training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/mvtec_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>STFPM.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer to PaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction. In PaddleX, model inference prediction can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"uad_grid.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it is <code>STFPM.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weight path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/anomaly_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own project.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The unsupervised anomaly detection module can be integrated into PaddleX pipelines such as Image_anomaly_detection. Simply replace the model path to update the unsupervised anomaly detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the unsupervised anomaly detection module. You can refer to the Python example code in Quick Integration, simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html","title":"Face Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#i-overview","title":"I. Overview","text":"<p>Face detection is a fundamental task in object detection, aiming to automatically identify and locate the position and size of faces in input images. It serves as the prerequisite and foundation for subsequent tasks such as face recognition and face analysis. Face detection accomplishes this by constructing deep neural network models that learn the feature representations of faces, enabling efficient and accurate face detection.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link AP (%)Easy/Medium/Hard GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description BlazeFaceInference Model/Trained Model 77.7/73.4/49.5 49.9 68.2 0.447 A lightweight and efficient face detection model BlazeFace-FPN-SSHInference Model/Trained Model 83.2/80.5/60.5 52.4 73.2 0.606 An improved model of BlazeFace, incorporating FPN and SSH structures PicoDet_LCNet_x2_5_faceInference Model/Trained Model 93.7/90.7/68.1 33.7 185.1 28.9 Face Detection model based on PicoDet_LCNet_x2_5 PP-YOLOE_plus-S_faceInference Model/Trained Model 93.9/91.8/79.8 25.8 159.9 26.5 Face Detection model based on PP-YOLOE_plus-S <p>Note: The above accuracy metrics are evaluated on the WIDER-FACE validation set with an input size of 640*640. GPU inference time is based on an NVIDIA V100 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 6271C CPU @ 2.60GHz and FP32 precision. </p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>Before quick integration, you need to install the PaddleX wheel package. For the installation method of the wheel package, please refer to the PaddleX Local Installation Tutorial. After installing the wheel package, a few lines of code can complete the inference of the face detection module. You can switch models under this module freely, and you can also integrate the model inference of the face detection module into your project. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"PicoDet_LCNet_x2_5_face\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"face_detection.png\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on the usage of PaddleX's single-model inference API, please refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better face detection models. Before using PaddleX to develop face detection models, ensure you have installed the PaddleDetection plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides a data validation function for each module, and only data that passes the validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development based on the official demos. If you wish to use private datasets for subsequent model training, refer to the PaddleX Object Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/widerface_coco_examples.tar -P ./dataset\ntar -xf ./dataset/widerface_coco_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerface_coco_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 1,\n    \"train_samples\": 500,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/0--Parade/0_Parade_marchingband_1_849.jpg\",\n      \"check_dataset/demo_img/0--Parade/0_Parade_Parade_0_904.jpg\",\n      \"check_dataset/demo_img/0--Parade/0_Parade_marchingband_1_799.jpg\"\n    ],\n    \"val_samples\": 100,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/1--Handshaking/1_Handshaking_Handshaking_1_384.jpg\",\n      \"check_dataset/demo_img/1--Handshaking/1_Handshaking_Handshaking_1_538.jpg\",\n      \"check_dataset/demo_img/1--Handshaking/1_Handshaking_Handshaking_1_429.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/widerface_coco_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>The verification results mentioned above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Details of other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 1;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 500;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 100;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of validation samples in this dataset;</li> </ul> <p>The dataset verification also analyzes the distribution of sample numbers across all classes and generates a histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Face detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> section in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerface_coco_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerface_coco_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command is sufficient to complete model training, taking the training of PicoDet_LCNet_x2_5_face as an example:</p> <p><pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/widerface_coco_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file of the model (here it is <code>PicoDet_LCNet_x2_5_face.yaml</code>\uff0cWhen training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/widerface_coco_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>PicoDet_LCNet_x2_5_face.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer to PaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction. In PaddleX, model inference prediction can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/face_detection/PicoDet_LCNet_x2_5_face.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"face_detection.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it is <code>PicoDet_LCNet_x2_5_face.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weight path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/face_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own project.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The face detection module can be integrated into PaddleX pipelines such as Face Recognition. Simply replace the model path to update the face detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the face detection module. You can refer to the Python example code in Quick Integration, simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html","title":"Face Feature Module Usage Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#i-overview","title":"I. Overview","text":"<p>Face feature models typically take standardized face images processed through detection, extraction, and keypoint correction as input. These models extract highly discriminative facial features from these images for subsequent modules, such as face matching and verification tasks.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#ii-supported-model-list","title":"II. Supported Model List","text":"\ud83d\udc49Details of Model List ModelModel Download Link Output Feature Dimension Acc (%)AgeDB-30/CFP-FP/LFW GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description MobileFaceNetInference Model/Trained Model 128 96.28/96.71/99.58 5.7 101.6 4.1 Face feature model trained on MobileFaceNet with MS1Mv3 dataset ResNet50_faceInference Model/Trained Model 512 98.12/98.56/99.77 8.7 200.7 87.2 Face feature model trained on ResNet50 with MS1Mv3 dataset <p>Note: The above accuracy metrics are Accuracy scores measured on the AgeDB-30, CFP-FP, and LFW datasets, respectively. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For details, refer to the PaddleX Local Installation Tutorial</p> <p>After installing the whl package, a few lines of code can complete the inference of the face feature module. You can switch models under this module freely, and you can also integrate the model inference of the face feature module into your project. Before running the following code, please download the example image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"MobileFaceNet\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"face_recognition_001.jpg\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on using the PaddleX single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you aim for higher accuracy with existing models, you can leverage PaddleX's custom development capabilities to develop better face feature models. Before developing face feature models with PaddleX, ensure you have installed the PaddleX PaddleClas plugin. The installation process can be found in the PaddleX Local Installation Tutorial</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the dataset for the corresponding task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, allowing you to complete subsequent development based on the official demo data. If you wish to use a private dataset for subsequent model training, the training dataset for the face feature module is organized in a general image classification dataset format. You can refer to the PaddleX Image Classification Task Module Data Annotation Tutorial. If you wish to use a private dataset for subsequent model evaluation, note that the validation dataset format for the face feature module differs from the training dataset format. Please refer to Section 4.1.4 Data Organization Face Feature Module</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/face_rec_examples.tar -P ./dataset\ntar -xf ./dataset/face_rec_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <pre><code>python main.py -c paddlex/configs/face_recognition/MobileFaceNet.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/face_rec_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_label_file\": \"../../dataset/face_rec_examples/train/label.txt\",\n    \"train_num_classes\": 995,\n    \"train_samples\": 1000,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/01378592.jpg\",\n      \"check_dataset/demo_img/04331410.jpg\",\n      \"check_dataset/demo_img/03485713.jpg\",\n      \"check_dataset/demo_img/02382123.jpg\",\n      \"check_dataset/demo_img/01722397.jpg\",\n      \"check_dataset/demo_img/02682349.jpg\",\n      \"check_dataset/demo_img/00272794.jpg\",\n      \"check_dataset/demo_img/03151987.jpg\",\n      \"check_dataset/demo_img/01725764.jpg\",\n      \"check_dataset/demo_img/02580369.jpg\"\n    ],\n    \"val_label_file\": \"../../dataset/face_rec_examples/val/pair_label.txt\",\n    \"val_num_classes\": 2,\n    \"val_samples\": 500,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/Don_Carcieri_0001.jpg\",\n      \"check_dataset/demo_img/Eric_Fehr_0001.jpg\",\n      \"check_dataset/demo_img/Harry_Kalas_0001.jpg\",\n      \"check_dataset/demo_img/Francis_Ford_Coppola_0001.jpg\",\n      \"check_dataset/demo_img/Amer_al-Saadi_0001.jpg\",\n      \"check_dataset/demo_img/Sergei_Ivanov_0001.jpg\",\n      \"check_dataset/demo_img/Erin_Runnion_0003.jpg\",\n      \"check_dataset/demo_img/Bill_Stapleton_0001.jpg\",\n      \"check_dataset/demo_img/Daniel_Bruehl_0001.jpg\",\n      \"check_dataset/demo_img/Clare_Short_0004.jpg\"\n    ]\n  },\n  \"analysis\": {},\n  \"dataset_path\": \"./dataset/face_rec_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"ClsDataset\"\n}\n</code></pre> <p>The verification results mentioned above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Details of other indicators are as follows:</p> <ul> <li><code>attributes.train_num_classes</code>: The number of classes in this training dataset is 995;</li> <li><code>attributes.val_num_classes</code>: The number of classes in this validation dataset is 2;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 1000;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 500;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of training samples in this dataset;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or adding hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>The face feature module does not support data format conversion or dataset splitting.</p>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#414-data-organization-for-face-feature-module","title":"4.1.4 Data Organization for Face Feature Module","text":"<p>The format of the validation dataset for the face feature module differs from the training dataset. If you need to evaluate model accuracy on private data, please organize your dataset as follows:</p> <pre><code>face_rec_dataroot      # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 train              # Directory for saving the training dataset, the directory name cannot be changed\n   \u251c\u2500\u2500 images          # Directory for saving images, the directory name can be changed but should correspond to the content in label.txt\n      \u251c\u2500\u2500 xxx.jpg      # Face image file\n      \u251c\u2500\u2500 xxx.jpg      # Face image file\n      ...\n   \u2514\u2500\u2500label.txt       # Training set annotation file, the file name cannot be changed. Each line gives the relative path of the image to `train` and the face image class (face identity) id, separated by a space. Example content: images/image_06765.jpg 0\n\u251c\u2500\u2500 val                # Directory for saving the validation dataset, the directory name cannot be changed\n   \u251c\u2500\u2500 images          # Directory for saving images, the directory name can be changed but should correspond to the content in pair_label.txt\n      \u251c\u2500\u2500 xxx.jpg      # Face image file\n      \u251c\u2500\u2500 xxx.jpg      # Face image file\n      ...\n   \u2514\u2500\u2500 pair_label.txt  # Validation dataset annotation file, the file name cannot be changed. Each line gives the paths of two images to be compared and a 0 or 1 label indicating whether the pair of images belong to the same person, separated by spaces.\n</code></pre> <p>Example content of the validation set annotation file <code>pair_label.txt</code>:</p> <pre><code># Face image 1.jpg Face image 2.jpg Label (0 indicates the two face images do not belong to the same person, 1 indicates they do)\nimages/Angela_Merkel_0001.jpg images/Angela_Merkel_0002.jpg 1\nimages/Bruce_Gebhardt_0001.jpg images/Masao_Azuma_0001.jpg 0\nimages/Francis_Ford_Coppola_0001.jpg images/Francis_Ford_Coppola_0002.jpg 1\nimages/Jason_Kidd_0006.jpg images/Jason_Kidd_0008.jpg 1\nimages/Miyako_Miyazaki_0002.jpg images/Munir_Akram_0002.jpg 0\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/face_feature.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command. Here is an example of training MobileFaceNet:</p> <p><pre><code>python main.py -c paddlex/configs/face_recognition/MobileFaceNet.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/face_rec_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model (here it is <code>MobileFaceNet.yaml</code>)</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file or by appending parameters in the command line. For example, to specify the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file instructions for the corresponding task module PaddleX Common Configuration Parameters for Model Tasks.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li>When training other models, specify the corresponding configuration file. The correspondence between models and configuration files can be found in the PaddleX Model List (CPU/GPU). After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>). Typically, the following outputs are included:</li> <li><code>train_result.json</code>: A file that records the training results, indicating whether the training task was successfully completed, and includes metrics, paths to related files, etc.</li> <li><code>train.log</code>: A log file that records changes in model metrics, loss variations, and other details during the training process.</li> <li><code>config.yaml</code>: A configuration file that logs the hyperparameter settings for the current training session.</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Files related to model weights, including network parameters, optimizer, EMA (Exponential Moving Average), static graph network parameters, and static graph network structure.</li> </ul>   ### 4.3 Model Evaluation After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:   <pre><code>python main.py -c paddlex/configs/face_detection/MobileFaceNet.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/face_rec_examples\n</code></pre>  Similar to model training, the process involves the following steps:  * Specify the path to the `.yaml` configuration file for the model\uff08here it's `MobileFaceNet.yaml`\uff09 * Set the mode to model evaluation: `-o Global.mode=evaluate` * Specify the path to the validation dataset: `-o Global.dataset_dir` Other related parameters can be configured by modifying the fields under `Global` and `Evaluate` in the `.yaml` configuration file. For detailed information, please refer to [PaddleX Common Configuration Parameters for Models](../../instructions/config_parameters_common.en.md)\u3002   \ud83d\udc49 More Details (Click to Expand)  During model evaluation, the path to the model weights file needs to be specified. Each configuration file has a default weight save path built in. If you need to change it, you can set it by appending a command line parameter, such as `-o Evaluate.weight_path=\"./output/best_model/best_model/model.pdparams\"`.  After completing the model evaluation, an `evaluate_result.json` file will be produced, which records the evaluation results. Specifically, it records whether the evaluation task was completed normally and the model's evaluation metrics, including Accuracy.  ### 4.4 Model Inference After completing model training and evaluation, you can use the trained model weights for inference predictions. In PaddleX, model inference predictions can be implemented through two methods: command line and wheel package.  #### 4.4.1 Model Inference * To perform inference predictions through the command line, you only need the following command. Before running the following code, please download the [example image](https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/face_recognition_001.jpg) to your local machine. <pre><code>python main.py -c paddlex/configs/face_recognition/MobileFaceNet.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"face_recognition_001.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:  * Specify the path to the model's `.yaml` configuration file (here it is `MobileFaceNet.yaml`) * Specify the mode as model inference prediction: `-o Global.mode=predict` * Specify the path to the model weights: `-o Predict.model_dir=\"./output/best_model/inference\"` * Specify the path to the input data: `-o Predict.input=\"...\"` Other related parameters can be set by modifying the fields under `Global` and `Predict` in the `.yaml` configuration file. For details, please refer to [PaddleX Common Model Configuration File Parameter Description](../../instructions/config_parameters_common.md).  #### 4.4.2 Model Integration The model can be directly integrated into the PaddleX pipeline or into your own project.  1. Pipeline Integration  The face feature module can be integrated into the PaddleX pipeline for [Face Recognition](../../../pipeline_usage/tutorials/face_recognition_pipelines/face_recognition.en.md). You only need to replace the model path to update the face feature module of the relevant pipeline. In pipeline integration, you can use high-performance deployment and service-oriented deployment to deploy the model you obtained.  2. Module Integration  The weights you produced can be directly integrated into the face feature module. You can refer to the Python example code in [Quick Integration](#III.-Quick-Integration) and only need to replace the model with the path to the model you trained."},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html","title":"Human Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#i-overview","title":"I. Overview","text":"<p>Human detection is a subtask of object detection, which utilizes computer vision technology to identify the presence of pedestrians in images or videos and provide the specific location information for each pedestrian. This information is crucial for various applications such as intelligent video surveillance, human behavior analysis, autonomous driving, and intelligent robots.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(0.5:0.95) mAP(0.5) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-YOLOE-L_humanInference Model/Trained Model 48.0 81.9 32.8 777.7 196.02 Human detection model based on PP-YOLOE PP-YOLOE-S_humanInference Model/Trained Model 42.5 77.9 15.0 179.3 28.79 <p>Note: The evaluation set for the above accuracy metrics is CrowdHuman dataset mAP(0.5:0.95). GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Guide</p> <p>After installing the wheel package, you can perform human detection with just a few lines of code. You can easily switch between models in this module and integrate the human detection model inference into your project. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"PP-YOLOE-S_human\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"human_detection.jpg\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on using PaddleX's single-model inference API, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you aim for higher accuracy with existing models, you can leverage PaddleX's custom development capabilities to develop better human detection models. Before using PaddleX to develop human detection models, ensure you have installed the PaddleDetection plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the specific task module. PaddleX provides a data validation function for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to PaddleX Object Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/widerperson_coco_examples.tar -P ./dataset\ntar -xf ./dataset/widerperson_coco_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>You can complete data validation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerperson_coco_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Details of validation results (click to expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 1,\n    \"train_samples\": 500,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/000041.jpg\",\n      \"check_dataset/demo_img/000042.jpg\",\n      \"check_dataset/demo_img/000044.jpg\"\n    ],\n    \"val_samples\": 100,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/001138.jpg\",\n      \"check_dataset/demo_img/001140.jpg\",\n      \"check_dataset/demo_img/001141.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/widerperson_coco_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. The explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>\uff1aThe number of classes in this dataset is 1.</li> <li><code>attributes.train_samples</code>\uff1aThe number of samples in the training set of this dataset is 500.</li> <li><code>attributes.val_samples</code>\uff1aThe number of samples in the validation set of this dataset is 100.</li> <li><code>attributes.train_sample_paths</code>\uff1aA list of relative paths to the visualized images of samples in the training set of this dataset.</li> <li><code>attributes.val_sample_paths</code>\uff1a A list of relative paths to the visualized images of samples in the validation set of this dataset.</li> </ul> <p>The dataset validation also analyzes the distribution of sample counts across all classes in the dataset and generates a histogram (histogram.png) to visualize this distribution. </p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Human detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Dataset splitting parameters can be set by modifying the <code>CheckDataset</code> section in the configuration file. Some example parameters in the configuration file are explained below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerperson_coco_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/widerperson_coco_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of <code>PP-YOLOE-S_human</code> as an example:</p> <p><pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/widerperson_coco_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-YOLOE-S_human.yaml</code>\uff0cWhen training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/widerperson_coco_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>PP-YOLOE-S_human.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer toPaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction. In PaddleX, model inference prediction can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/human_detection/PP-YOLOE-S_human.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"human_detection.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it is <code>PP-YOLOE-S_human.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/human_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The weights you produce can be directly integrated into the human detection module. You can refer to the Python sample code in Quick Integration and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html","title":"Image Classification Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#i-overview","title":"I. Overview","text":"<p>The image classification module is a crucial component in computer vision systems, responsible for categorizing input images. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. Typically, the image classification module receives an image as input and, through deep learning or other machine learning algorithms, classifies it into predefined categories based on its characteristics and content. For instance, in an animal recognition system, the image classification module might need to classify an input image as \"cat,\" \"dog,\" \"horse,\" etc. The classification results from the image classification module are then output for use by other modules or systems.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#ii-list-of-supported-models","title":"II. List of Supported Models","text":"ModelModel Download Link Top1 Acc(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Storage Size (M) CLIP_vit_base_patch16_224Inference Model/Trained Model 85.36 13.1957 285.493 306.5 M MobileNetV3_small_x1_0Inference Model/Trained Model 68.2 6.00993 12.9598 10.5 M PP-HGNet_smallInference Model/Trained Model 81.51 5.50661 119.041 86.5 M PP-HGNetV2-B0Inference Model/Trained Model 77.77 6.53694 23.352 21.4 M PP-HGNetV2-B4Inference Model/Trained Model 83.57 9.66407 54.2462 70.4 M PP-HGNetV2-B6Inference Model/Trained Model 86.30 21.226 255.279 268.4 M PP-LCNet_x1_0Inference Model/Trained Model 71.32 3.84845 9.23735 10.5 M ResNet50Inference Model/Trained Model 76.5 9.62383 64.8135 90.8 M SwinTransformer_tiny_patch4_window7_224Inference Model/Trained Model 81.10 8.54846 156.306 100.1 M <p>\u2757 The above list features the 9 core models that the image classification module primarily supports. In total, this module supports 80 models. The complete list of models is as follows:</p>  \ud83d\udc49Details of Model List ModelModel Download Link Top-1 Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description CLIP_vit_base_patch16_224Inference Model/Trained Model 85.36 13.1957 285.493 306.5 M CLIP is an image classification model based on the correlation between vision and language. It adopts contrastive learning and pre-training methods to achieve unsupervised or weakly supervised image classification, especially suitable for large-scale datasets. By mapping images and texts into the same representation space, the model learns general features, exhibiting good generalization ability and interpretability. With relatively good training errors, it performs well in many downstream tasks. CLIP_vit_large_patch14_224Inference Model/Trained Model 88.1 51.1284 1131.28 1.04 G ConvNeXt_base_224Inference Model/Trained Model 83.84 12.8473 1513.87 313.9 M The ConvNeXt series of models were proposed by Meta in 2022, based on the CNN architecture. This series of models builds upon ResNet, incorporating the advantages of SwinTransformer, including training strategies and network structure optimization ideas, to improve the pure CNN architecture network. It explores the performance limits of convolutional neural networks. The ConvNeXt series of models possesses many advantages of convolutional neural networks, including high inference efficiency and ease of migration to downstream tasks. ConvNeXt_base_384Inference Model/Trained Model 84.90 31.7607 3967.05 313.9 M ConvNeXt_large_224Inference Model/Trained Model 84.26 26.8103 2463.56 700.7 M ConvNeXt_large_384Inference Model/Trained Model 85.27 66.4058 6598.92 700.7 M ConvNeXt_smallInference Model/Trained Model 83.13 9.74075 1127.6 178.0 M ConvNeXt_tinyInference Model/Trained Model 82.03 5.48923 672.559 104.1 M FasterNet-LInference Model/Trained Model 83.5 23.4415 - 357.1 M FasterNet is a neural network designed to improve runtime speed. Its key improvements are as follows: 1. Re-examined popular operators and found that low FLOPS mainly stem from frequent memory accesses, especially in depthwise convolutions; 2. Proposed Partial Convolution (PConv) to extract image features more efficiently by reducing redundant computations and memory accesses; 3. Launched the FasterNet series of models based on PConv, a new design scheme that achieves significantly higher runtime speeds on various devices without compromising model task performance. FasterNet-MInference Model/Trained Model 83.0 21.8936 - 204.6 M FasterNet-SInference Model/Trained Model 81.3 13.0409 - 119.3 M FasterNet-T0Inference Model/Trained Model 71.9 12.2432 - 15.1 M FasterNet-T1Inference Model/Trained Model 75.9 11.3562 - 29.2 M FasterNet-T2Inference Model/Trained Model 79.1 10.703 - 57.4 M MobileNetV1_x0_5Inference Model/Trained Model 63.5 1.86754 7.48297 4.8 M MobileNetV1 is a network released by Google in 2017 for mobile devices or embedded devices. This network decomposes traditional convolution operations into depthwise separable convolutions, which are a combination of Depthwise convolution and Pointwise convolution. Compared to traditional convolutional networks, this combination can significantly reduce the number of parameters and computations. Additionally, this network can be used for image classification and other vision tasks. MobileNetV1_x0_25Inference Model/Trained Model 51.4 1.83478 4.83674 1.8 M MobileNetV1_x0_75Inference Model/Trained Model 68.8 2.57903 10.6343 9.3 M MobileNetV1_x1_0Inference Model/Trained Model 71.0 2.78781 13.98 15.2 M MobileNetV2_x0_5Inference Model/Trained Model 65.0 4.94234 11.1629 7.1 M MobileNetV2 is a lightweight network proposed by Google following MobileNetV1. Compared to MobileNetV1, MobileNetV2 introduces Linear bottlenecks and Inverted residual blocks as the basic structure of the network. By stacking these basic modules extensively, the network structure of MobileNetV2 is formed. Finally, it achieves higher classification accuracy with only half the FLOPs of MobileNetV1. MobileNetV2_x0_25Inference Model/Trained Model 53.2 4.50856 9.40991 5.5 M MobileNetV2_x1_0Inference Model/Trained Model 72.2 6.12159 16.0442 12.6 M MobileNetV2_x1_5Inference Model/Trained Model 74.1 6.28385 22.5129 25.0 M MobileNetV2_x2_0Inference Model/Trained Model 75.2 6.12888 30.8612 41.2 M MobileNetV3_large_x0_5Inference Model/Trained Model 69.2 6.31302 14.5588 9.6 M MobileNetV3 is a NAS-based lightweight network proposed by Google in 2019. To further enhance performance, relu and sigmoid activation functions are replaced with hard_swish and hard_sigmoid activation functions, respectively. Additionally, some improvement strategies specifically designed to reduce network computations are introduced. MobileNetV3_large_x0_35Inference Model/Trained Model 64.3 5.76207 13.9041 7.5 M MobileNetV3_large_x0_75Inference Model/Trained Model 73.1 8.41737 16.9506 14.0 M MobileNetV3_large_x1_0Inference Model/Trained Model 75.3 8.64112 19.1614 19.5 M MobileNetV3_large_x1_25Inference Model/Trained Model 76.4 8.73358 22.1296 26.5 M MobileNetV3_small_x0_5Inference Model/Trained Model 59.2 5.16721 11.2688 6.8 M MobileNetV3_small_x0_35Inference Model/Trained Model 53.0 5.22053 11.0055 6.0 M MobileNetV3_small_x0_75Inference Model/Trained Model 66.0 5.39831 12.8313 8.5 M MobileNetV3_small_x1_0Inference Model/Trained Model 68.2 6.00993 12.9598 10.5 M MobileNetV3_small_x1_25Inference Model/Trained Model 70.7 6.9589 14.3995 13.0 M MobileNetV4_conv_largeInference Model/Trained Model 83.4 12.5485 51.6453 125.2 M MobileNetV4 is an efficient architecture specifically designed for mobile devices. Its core lies in the introduction of the UIB (Universal Inverted Bottleneck) module, a unified and flexible structure that integrates IB (Inverted Bottleneck), ConvNeXt, FFN (Feed Forward Network), and the latest ExtraDW (Extra Depthwise) module. Alongside UIB, Mobile MQA, a customized attention block for mobile accelerators, was also introduced, achieving up to 39% significant acceleration. Furthermore, MobileNetV4 introduces a novel Neural Architecture Search (NAS) scheme to enhance the effectiveness of the search process. MobileNetV4_conv_mediumInference Model/Trained Model 79.9 9.65509 26.6157 37.6 M MobileNetV4_conv_smallInference Model/Trained Model 74.6 5.24172 11.0893 14.7 M MobileNetV4_hybrid_largeInference Model/Trained Model 83.8 20.0726 213.769 145.1 M MobileNetV4_hybrid_mediumInference Model/Trained Model 80.5 19.7543 62.2624 42.9 M PP-HGNet_baseInference Model/Trained Model 85.0 14.2969 327.114 249.4 M PP-HGNet (High Performance GPU Net) is a high-performance backbone network developed by Baidu PaddlePaddle's vision team, tailored for GPU platforms. This network combines the fundamentals of VOVNet with learnable downsampling layers (LDS Layer), incorporating the advantages of models such as ResNet_vd and PPHGNet. On GPU platforms, this model achieves higher accuracy compared to other SOTA models at the same speed. Specifically, it outperforms ResNet34-0 by 3.8 percentage points and ResNet50-0 by 2.4 percentage points. Under the same SLSD conditions, it ultimately surpasses ResNet50-D by 4.7 percentage points. Additionally, at the same level of accuracy, its inference speed significantly exceeds that of mainstream Vision Transformers. PP-HGNet_smallInference Model/Trained Model 81.51 5.50661 119.041 86.5 M PP-HGNet_tinyInference Model/Trained Model 79.83 5.22006 69.396 52.4 M PP-HGNetV2-B0Inference Model/Trained Model 77.77 6.53694 23.352 21.4 M PP-HGNetV2 (High Performance GPU Network V2) is the next-generation version of Baidu PaddlePaddle's PP-HGNet, featuring further optimizations and improvements upon its predecessor. It pushes the limits of NVIDIA's \"Accuracy-Latency Balance,\" significantly outperforming other models with similar inference speeds in terms of accuracy. It demonstrates strong performance across various label classification and evaluation scenarios. PP-HGNetV2-B1Inference Model/Trained Model 79.18 6.56034 27.3099 22.6 M PP-HGNetV2-B2Inference Model/Trained Model 81.74 9.60494 43.1219 39.9 M PP-HGNetV2-B3Inference Model/Trained Model 82.98 11.0042 55.1367 57.9 M PP-HGNetV2-B4Inference Model/Trained Model 83.57 9.66407 54.2462 70.4 M PP-HGNetV2-B5Inference Model/Trained Model 84.75 15.7091 115.926 140.8 M PP-HGNetV2-B6Inference Model/Trained Model 86.30 21.226 255.279 268.4 M PP-LCNet_x0_5Inference Model/Trained Model 63.14 3.67722 6.66857 6.7 M PP-LCNet is a lightweight backbone network developed by Baidu PaddlePaddle's vision team. It enhances model performance without increasing inference time, significantly surpassing other lightweight SOTA models. PP-LCNet_x0_25Inference Model/Trained Model 51.86 2.65341 5.81357 5.5 M PP-LCNet_x0_35Inference Model/Trained Model 58.09 2.7212 6.28944 5.9 M PP-LCNet_x0_75Inference Model/Trained Model 68.18 3.91032 8.06953 8.4 M PP-LCNet_x1_0Inference Model/Trained Model 71.32 3.84845 9.23735 10.5 M PP-LCNet_x1_5Inference Model/Trained Model 73.71 3.97666 12.3457 16.0 M PP-LCNet_x2_0Inference Model/Trained Model 75.18 4.07556 16.2752 23.2 M PP-LCNet_x2_5Inference Model/Trained Model 76.60 4.06028 21.5063 32.1 M PP-LCNetV2_baseInference Model/Trained Model 77.05 5.23428 19.6005 23.7 M The PP-LCNetV2 image classification model is the next-generation version of PP-LCNet, self-developed by Baidu PaddlePaddle's vision team. Based on PP-LCNet, it has undergone further optimization and improvements, primarily utilizing re-parameterization strategies to combine depthwise convolutions with varying kernel sizes and optimizing pointwise convolutions, Shortcuts, etc. Without using additional data, the PPLCNetV2_base model achieves over 77% Top-1 Accuracy on the ImageNet dataset for image classification, while maintaining an inference time of less than 4.4 ms on Intel CPU platforms. PP-LCNetV2_large Inference Model/Trained Model 78.51 6.78335 30.4378 37.3 M PP-LCNetV2_smallInference Model/Trained Model 73.97 3.89762 13.0273 14.6 M ResNet18_vdInference Model/Trained Model 72.3 3.53048 31.3014 41.5 M The ResNet series of models were introduced in 2015, winning the ILSVRC2015 competition with a top-5 error rate of 3.57%. This network innovatively proposed residual structures, which are stacked to construct the ResNet network. Experiments have shown that using residual blocks can effectively improve convergence speed and accuracy. ResNet18 Inference Model/Trained Model 71.0 2.4868 27.4601 41.5 M ResNet34_vdInference Model/Trained Model 76.0 5.60675 56.0653 77.3 M ResNet34Inference Model/Trained Model 74.6 4.16902 51.925 77.3 M ResNet50_vdInference Model/Trained Model 79.1 10.1885 68.446 90.8 M ResNet50Inference Model/Trained Model 76.5 9.62383 64.8135 90.8 M ResNet101_vdInference Model/Trained Model 80.2 20.0563 124.85 158.4 M ResNet101Inference Model/Trained Model 77.6 19.2297 121.006 158.4 M ResNet152_vdInference Model/Trained Model 80.6 29.6439 181.678 214.3 M ResNet152Inference Model/Trained Model 78.3 30.0461 177.707 214.2 M ResNet200_vdInference Model/Trained Model 80.9 39.1628 235.185 266.0 M StarNet-S1Inference Model/Trained Model 73.6 9.895 23.0465 11.2 M StarNet focuses on exploring the untapped potential of \"star operations\" (i.e., element-wise multiplication) in network design. It reveals that star operations can map inputs to high-dimensional, nonlinear feature spaces, a process akin to kernel tricks but without the need to expand the network size. Consequently, StarNet, a simple yet powerful prototype network, is further proposed, demonstrating exceptional performance and low latency under compact network structures and limited computational resources. StarNet-S2 Inference Model/Trained Model 74.8 7.91279 21.9571 14.3 M StarNet-S3Inference Model/Trained Model 77.0 10.7531 30.7656 22.2 M StarNet-S4Inference Model/Trained Model 79.0 15.2868 43.2497 28.9 M SwinTransformer_base_patch4_window7_224Inference Model/Trained Model 83.37 16.9848 383.83 310.5 M SwinTransformer is a novel vision Transformer network that can serve as a general-purpose backbone for computer vision tasks. SwinTransformer consists of a hierarchical Transformer structure represented by shifted windows. Shifted windows restrict self-attention computations to non-overlapping local windows while allowing cross-window connections, thereby enhancing network performance. SwinTransformer_base_patch4_window12_384Inference Model/Trained Model 84.17 37.2855 1178.63 311.4 M SwinTransformer_large_patch4_window7_224Inference Model/Trained Model 86.19 27.5498 689.729 694.8 M SwinTransformer_large_patch4_window12_384Inference Model/Trained Model 87.06 74.1768 2105.22 696.1 M SwinTransformer_small_patch4_window7_224Inference Model/Trained Model 83.21 16.3982 285.56 175.6 M SwinTransformer_tiny_patch4_window7_224Inference Model/Trained Model 81.10 8.54846 156.306 100.1 M <p>Note: The above accuracy metrics refer to Top-1 Accuracy on the ImageNet-1k validation set. All model GPU inference times are based on NVIDIA Tesla T4 machines, with precision type FP32. CPU inference speeds are based on Intel\u00ae Xeon\u00ae Gold 5117 CPU @ 2.00GHz, with 8 threads and precision type FP32.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide.</p> <p>After installing the wheel package, you can complete image classification module inference with just a few lines of code. You can switch between models in this module freely, and you can also integrate the model inference of the image classification module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0\")\noutput = model.predict(\"general_image_classification_001.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, please refer to the PaddleX Single-Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you are seeking higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better image classification models. Before using PaddleX to develop image classification models, please ensure that you have installed the relevant model training plugins for image classification in PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the dataset for the corresponding task module. PaddleX provides data validation functionality for each module, and only data that passes data validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use your own private dataset for subsequent model training, please refer to the PaddleX Image Classification Task Module Data Annotation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following command to download the demo dataset to a specified folder: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/cls_flowers_examples.tar -P ./dataset\ntar -xf ./dataset/cls_flowers_examples.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>One command is all you need to complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/cls_flowers_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Validation Results Details (Click to Expand) <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": \"dataset/label.txt\",\n    \"num_classes\": 102,\n    \"train_samples\": 1020,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/image_01904.jpg\",\n      \"check_dataset/demo_img/image_06940.jpg\"\n    ],\n    \"val_samples\": 1020,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/image_01937.jpg\",\n      \"check_dataset/demo_img/image_06958.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/cls_flowers_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"ClsDataset\"\n}\n</code></pre> <p>The above validation results, with check_pass being True, indicate that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 102;</li> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 1020;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 1020;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visual samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visual samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset validation analyzes the sample number distribution across all classes in the dataset and generates a distribution histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format or re-split the training/validation ratio of the dataset by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Dataset Format Conversion/Dataset Splitting Details (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Image classification does not currently support data conversion.</p> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. The following are example explanations for some of the parameters in the configuration file:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, the dataset format will be converted. The default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, you need to set the percentage of the training set, which should be an integer between 0-100, ensuring that the sum with <code>val_percent</code> equals 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, you need to modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/cls_flowers_examples\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>These parameters also support being set through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/cls_flowers_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command can complete the model training. Taking the training of the image classification model PP-LCNet_x1_0 as an example: <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml  \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/cls_flowers_examples\n</code></pre></p> <p>the following steps are required:</p> <ul> <li>Specify the path of the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0.yaml</code>. When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path of the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first 2 GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file parameter instructions for the corresponding task module of the model PaddleX Common Model Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. Using PaddleX for model evaluation, a single command can complete the model evaluation: <pre><code>python main.py -c  paddlex/configs/image_classification/PP-LCNet_x1_0.yaml  \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/cls_flowers_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path of the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path of the validation dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results. Specifically, it records whether the evaluation task was completed successfully and the model's evaluation metrics, including val.top1, val.top5;</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_image_classification_001.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-LCNet_x1_0.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weight path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/image_classification.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipelines or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The image classification module can be integrated into the General Image Classification Pipeline of PaddleX. Simply replace the model path to update the image classification module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your obtained model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the image classification module. You can refer to the Python example code in Quick Integration  and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html","title":"Image Feature Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#i-overview","title":"I. Overview","text":"<p>The image feature module is one of the important tasks in computer vision, primarily referring to the automatic extraction of useful features from image data using deep learning methods, to facilitate subsequent image retrieval tasks. The performance of this module directly affects the accuracy and efficiency of the subsequent tasks. In practical applications, image features typically output a set of feature vectors, which can effectively represent the content, structure, texture, and other information of the image, and will be passed as input to the subsequent retrieval module for processing.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Recall@1 (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-ShiTuV2_recInference Model/Trained Model 84.2 5.23428 19.6005 16.3 M PP-ShiTuV2 is a general image feature system consisting of three modules: object detection, feature extraction, and vector retrieval. These models are part of the feature extraction module and can be selected based on system requirements. PP-ShiTuV2_rec_CLIP_vit_baseInference Model/Trained Model 88.69 13.1957 285.493 306.6 M PP-ShiTuV2_rec_CLIP_vit_largeInference Model/Trained Model 91.03 51.1284 1131.28 1.05 G <p>Note: The above accuracy metrics are Recall@1 from AliProducts. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>After installing the wheel package, a few lines of code can complete the inference of the image feature module. You can switch between models under this module freely, and you can also integrate the model inference of the image feature module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-ShiTuV2_rec\")\noutput = model.predict(\"general_image_recognition_001.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better image feature models. Before developing image feature models with PaddleX, ensure you have installed the classification-related model training plugins for PaddleX. The installation process can be found in the PaddleX Local Installation Guide</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training.  Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Image Feature Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder: <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/Inshop_examples.tar -P ./dataset\ntar -xf ./dataset/Inshop_examples.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation: <pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/Inshop_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 1000,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/05_1_front.jpg\",\n      \"check_dataset/demo_img/02_1_front.jpg\",\n      \"check_dataset/demo_img/02_3_back.jpg\",\n      \"check_dataset/demo_img/04_3_back.jpg\",\n      \"check_dataset/demo_img/04_2_side.jpg\",\n      \"check_dataset/demo_img/12_1_front.jpg\",\n      \"check_dataset/demo_img/07_2_side.jpg\",\n      \"check_dataset/demo_img/04_7_additional.jpg\",\n      \"check_dataset/demo_img/04_4_full.jpg\",\n      \"check_dataset/demo_img/01_1_front.jpg\"\n    ],\n    \"gallery_samples\": 110,\n    \"gallery_sample_paths\": [\n      \"check_dataset/demo_img/06_2_side.jpg\",\n      \"check_dataset/demo_img/01_4_full.jpg\",\n      \"check_dataset/demo_img/04_7_additional.jpg\",\n      \"check_dataset/demo_img/02_1_front.jpg\",\n      \"check_dataset/demo_img/02_3_back.jpg\",\n      \"check_dataset/demo_img/02_3_back.jpg\",\n      \"check_dataset/demo_img/02_4_full.jpg\",\n      \"check_dataset/demo_img/03_4_full.jpg\",\n      \"check_dataset/demo_img/02_2_side.jpg\",\n      \"check_dataset/demo_img/03_2_side.jpg\"\n    ],\n    \"query_samples\": 125,\n    \"query_sample_paths\": [\n      \"check_dataset/demo_img/08_7_additional.jpg\",\n      \"check_dataset/demo_img/01_7_additional.jpg\",\n      \"check_dataset/demo_img/02_4_full.jpg\",\n      \"check_dataset/demo_img/04_4_full.jpg\",\n      \"check_dataset/demo_img/09_7_additional.jpg\",\n      \"check_dataset/demo_img/04_3_back.jpg\",\n      \"check_dataset/demo_img/02_1_front.jpg\",\n      \"check_dataset/demo_img/06_2_side.jpg\",\n      \"check_dataset/demo_img/02_7_additional.jpg\",\n      \"check_dataset/demo_img/02_2_side.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/Inshop_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"ShiTuRecDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows: * <code>attributes.train_samples</code>: The number of training samples in this dataset is 1000; * <code>attributes.gallery_samples</code>: The number of gallery (or reference) samples in this dataset is 110; * <code>attributes.query_samples</code>: The number of query samples in this dataset is 125; * <code>attributes.train_sample_paths</code>: A list of relative paths to the visual images of training samples in this dataset; * <code>attributes.gallery_sample_paths</code>: A list of relative paths to the visual images of gallery (or reference) samples in this dataset; * <code>attributes.query_sample_paths</code>: A list of relative paths to the visual images of query samples in this dataset;</p> <p>Additionally, the dataset verification also analyzes the number of images and image categories within the dataset, and generates a distribution histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the data verification, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>The image feature task supports converting <code>LabelMe</code> format datasets to <code>ShiTuRecDataset</code> format. The parameters for dataset format conversion can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. The image feature task supports converting <code>LabelMe</code> format datasets to <code>ShiTuRecDataset</code> format, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set, default is <code>null</code>, optional value is <code>LabelMe</code>;</li> </ul> <p>For example, if you want to convert a <code>LabelMe</code> format dataset to <code>ShiTuRecDataset</code> format, you need to modify the configuration file as follows:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/image_classification_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/image_classification_labelme_examples.tar -C ./dataset/\n</code></pre> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: LabelMe\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/image_classification_labelme_examples\n</code></pre> <p>After the data conversion is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/image_classification_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. When <code>True</code>, the dataset will be re-split, default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is re-split, the percentage of the training set needs to be set, the type is any integer between 0-100, and it needs to ensure that the sum of <code>gallery_percent</code> and <code>query_percent</code> values is 100;</li> </ul> <p>For example, if you want to re-split the dataset with 70% training set, 20% gallery set, and 10% query set, you need to modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 70\n    gallery_percent: 20\n    query_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/Inshop_examples\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/Inshop_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=70 \\\n    -o CheckDataset.split.gallery_percent=20 \\\n    -o CheckDataset.split.query_percent=10\n</code></pre> <p>\u2757Note: Due to the specificity of image feature model evaluation, data partitioning is meaningful only when the train, query, and gallery sets belong to the same category system. During the evaluation of recognition models, it is imperative that the gallery and query sets belong to the same category system, which may or may not be the same as the train set. If the gallery and query sets do not belong to the same category system as the train set, the evaluation after data partitioning becomes meaningless. It is recommended to proceed with caution.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of the image feature model PP-ShiTuV2_rec as an example:</p> <p><pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/Inshop_examples\n</code></pre> The following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-ShiTuV2_rec.yaml</code>\uff0cWhen training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file instructions for the corresponding task module of the model PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/Inshop_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-ShiTuV2_rec.yaml</code>)</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file, detailed instructions can be found in PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including recall1\u3001recall5\u3001mAP\uff1b</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/general_recognition/PP-ShiTuV2_rec.yaml  \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_image_recognition_001.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-ShiTuV2_rec.yaml</code>)</li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> <p>\u2757 Note: The inference result of the recognition model is a set of vectors, which requires a retrieval module to complete image feature.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_feature.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The image feature module can be integrated into the General Image Recognition Pipeline (comming soon) of PaddleX. Simply replace the model path to update the image feature module of the relevant pipeline. In pipeline integration, you can use service-oriented deployment to deploy your trained model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the image feature module. Refer to the Python example code in Quick Integration, and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html","title":"Image Multi-Label Classification Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#i-overview","title":"I. Overview","text":"<p>The image multi-label classification module is a crucial component in computer vision systems, responsible for assigning multiple labels to input images. Unlike traditional image classification tasks that assign a single category to an image, multi-label classification tasks require assigning multiple relevant categories to an image. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. The image multi-label classification module typically takes an image as input and, through deep learning or other machine learning algorithms, classifies it into multiple predefined categories based on its characteristics and content. For example, an image containing both a cat and a dog might be labeled as both \"cat\" and \"dog\" by the image multi-label classification module. These classification labels are then output for subsequent processing and analysis by other modules or systems.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(%) Model Size (M) Description CLIP_vit_base_patch16_448_MLInference Model/Trained Model 89.15 325.6 M CLIP_ML is an image multi-label classification model based on CLIP, which significantly improves accuracy on multi-label classification tasks by incorporating an ML-Decoder. PP-HGNetV2-B0_MLInference Model/Trained Model 80.98 39.6 M PP-HGNetV2_ML is an image multi-label classification model based on PP-HGNetV2, which significantly improves accuracy on multi-label classification tasks by incorporating an ML-Decoder. PP-HGNetV2-B4_MLInference Model/Trained Model 87.96 88.5 M PP-HGNetV2-B6_MLInference Model/Trained Model 91.25 286.5 M PP-LCNet_x1_0_MLInference Model/Trained Model 77.96 29.4 M PP-LCNet_ML is an image multi-label classification model based on PP-LCNet, which significantly improves accuracy on multi-label classification tasks by incorporating an ML-Decoder. ResNet50_MLInference Model/Trained Model 83.50 108.9 M ResNet50_ML is an image multi-label classification model based on ResNet50, which significantly improves accuracy on multi-label classification tasks by incorporating an ML-Decoder. <p>Note: The above accuracy metrics are mAP for the multi-label classification task on COCO2017.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>After installing the wheel package, you can complete multi-label classification module inference with just a few lines of code. You can switch between models in this module freely, and you can also integrate the model inference of the multi-label classification module into your project. Before running the following code, please download the demo image to your local machine. <pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0_ML\")\noutput = model.predict(\"multilabel_classification_005.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, please refer to the PaddleX Single-Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you are seeking higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better multi-label classification models. Before using PaddleX to develop multi-label classification models, please ensure that you have installed the relevant model training plugins for image classification in PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the dataset for the corresponding task module. PaddleX provides data validation functionality for each module, and only data that passes data validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use your own private dataset for subsequent model training, please refer to the PaddleX Image Multi-Label Classification Task Module Data Annotation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following command to download the demo dataset to a specified folder: <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/mlcls_nus_examples.tar -P ./dataset\ntar -xf ./dataset/mlcls_nus_examples.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/mlcls_nus_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": \"../../dataset/mlcls_nus_examples/label.txt\",\n    \"num_classes\": 33,\n    \"train_samples\": 17463,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/0543_4338693.jpg\",\n      \"check_dataset/demo_img/0272_347806939.jpg\",\n      \"check_dataset/demo_img/0069_2291994812.jpg\",\n      \"check_dataset/demo_img/0012_1222850604.jpg\",\n      \"check_dataset/demo_img/0238_53773041.jpg\",\n      \"check_dataset/demo_img/0373_541261977.jpg\",\n      \"check_dataset/demo_img/0567_519506868.jpg\",\n      \"check_dataset/demo_img/0023_289621557.jpg\",\n      \"check_dataset/demo_img/0581_484524659.jpg\",\n      \"check_dataset/demo_img/0325_120753036.jpg\"\n    ],\n    \"val_samples\": 17463,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/0546_130758157.jpg\",\n      \"check_dataset/demo_img/0284_2230710138.jpg\",\n      \"check_dataset/demo_img/0090_1491261559.jpg\",\n      \"check_dataset/demo_img/0013_392798436.jpg\",\n      \"check_dataset/demo_img/0246_2248376356.jpg\",\n      \"check_dataset/demo_img/0377_1349296474.jpg\",\n      \"check_dataset/demo_img/0570_2457645006.jpg\",\n      \"check_dataset/demo_img/0027_309333946.jpg\",\n      \"check_dataset/demo_img/0584_132639537.jpg\",\n      \"check_dataset/demo_img/0329_206031527.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/mlcls_nus_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"MLClsDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 33;</li> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 17463;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 17463;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visual samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visual samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset validation analyzes the sample number distribution across all classes in the dataset and generates a distribution histogram (histogram.png): </p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)(2) Dataset Splitting","text":"<p>After completing data validation, you can convert the dataset format or re-split the training/validation ratio of the dataset by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Dataset Format Conversion/Dataset Splitting Details (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>The multi-label image classification supports the conversion of <code>COCO</code> format datasets to <code>MLClsDataset</code> format. The parameters for dataset format conversion can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. Multi-label image classification supports converting <code>COCO</code> format datasets to <code>MLClsDataset</code> format. Default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set. Default is <code>null</code>, with the optional value of <code>COCO</code>; </li> </ul> <p>For example, if you want to convert a <code>COCO</code> format dataset to <code>MLClsDataset</code> format, you need to modify the configuration file as follows:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_coco_examples.tar -P ./dataset\ntar -xf ./dataset/det_coco_examples.tar -C ./dataset/\n</code></pre> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: COCO\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> <p>After the data conversion is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=COCO\n</code></pre> <p>The dataset splitting parameters can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. An example of part of the configuration file is shown below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to perform dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set, an integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If re-splitting the dataset, set the percentage of the validation set, an integer between 0-100, ensuring the sum with <code>train_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>These parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command can complete the model training. Taking the training of the image multi-label classification model PP-LCNet_x1_0_ML as an example: <pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/mlcls_nus_examples\n</code></pre> the following steps are required:</p> <ul> <li>Specify the path of the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_ML.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path of the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first 2 GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file parameter instructions for the corresponding task module of the model PaddleX Common Model Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/mlcls_nus_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-LCNet_x1_0_ML.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter to set it, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including MultiLabelMAP;</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li>Inference predictions can be performed through the command line with just one command. Before running the following code, please download the demo image to your local machine.</li> </ul> <p><pre><code>python main.py -c paddlex/configs/multilabel_classification/PP-LCNet_x1_0_ML.yaml  \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"multilabel_classification_005.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-LCNet_x1_0_ML.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/image_multilabel_classification.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The image multi-label classification module can be integrated into the General Image Multi-label Classification Pipeline of PaddleX. Simply replace the model path to update the image multi-label classification module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the image multi-label classification module. Refer to the Python example code in Quick Integration and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html","title":"Instance Segmentation Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#i-overview","title":"I. Overview","text":"<p>The instance segmentation module is a crucial component in computer vision systems, responsible for identifying and marking pixels that contain specific object instances in images or videos. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. The instance segmentation module typically outputs pixel-level masks (masks) for each target instance, which are then passed as input to the object recognition module for subsequent processing.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Mask AP GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Mask-RT-DETR-HInference Model/Trained Model 50.6 132.693 4896.17 449.9 M Mask-RT-DETR is an instance segmentation model based on RT-DETR. By adopting the high-performance PP-HGNetV2 as the backbone network and constructing a MaskHybridEncoder encoder, along with introducing IOU-aware Query Selection technology, it achieves state-of-the-art (SOTA) instance segmentation accuracy with the same inference time. Mask-RT-DETR-LInference Model/Trained Model 45.7 46.5059 2575.92 113.6 M <p>\u2757 The above list features the 2 core models that the image classification module primarily supports. In total, this module supports 15 models. The complete list of models is as follows:</p>  \ud83d\udc49Model List Details ModelModel Download Link Mask AP GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Cascade-MaskRCNN-ResNet50-FPNInference Model/Trained Model 36.3 - - 254.8 M Cascade-MaskRCNN is an improved Mask RCNN instance segmentation model that utilizes multiple detectors in a cascade, optimizing segmentation results by leveraging different IOU thresholds to address the mismatch between detection and inference stages, thereby enhancing instance segmentation accuracy. Cascade-MaskRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 39.1 - - 254.7 M Mask-RT-DETR-HInference Model/Trained Model 50.6 132.693 4896.17 449.9 M Mask-RT-DETR is an instance segmentation model based on RT-DETR. By adopting the high-performance PP-HGNetV2 as the backbone network and constructing a MaskHybridEncoder encoder, along with introducing IOU-aware Query Selection technology, it achieves state-of-the-art (SOTA) instance segmentation accuracy with the same inference time. Mask-RT-DETR-LInference Model/Trained Model 45.7 46.5059 2575.92 113.6 M Mask-RT-DETR-MInference Model/Trained Model 42.7 36.8329 - 66.6 M Mask-RT-DETR-SInference Model/Trained Model 41.0 33.5007 - 51.8 M Mask-RT-DETR-XInference Model/Trained Model 47.5 75.755 3358.04 237.5 M MaskRCNN-ResNet50-FPNInference Model/Trained Model 35.6 - - 157.5 M Mask R-CNN is a full-task deep learning model from Facebook AI Research (FAIR) that can perform object classification and localization in a single model, combined with image-level masks to complete segmentation tasks. MaskRCNN-ResNet50-vd-FPNInference Model/Trained Model 36.4 - - 157.5 M MaskRCNN-ResNet50Inference Model/Trained Model 32.8 - - 128.7 M MaskRCNN-ResNet101-FPNInference Model/Trained Model 36.6 - - 225.4 M MaskRCNN-ResNet101-vd-FPNInference Model/Trained Model 38.1 - - 225.1 M MaskRCNN-ResNeXt101-vd-FPNInference Model/Trained Model 39.5 - - 370.0 M PP-YOLOE_seg-SInference Model/Trained Model 32.5 - - 31.5 M PP-YOLOE_seg is an instance segmentation model based on PP-YOLOE. This model inherits PP-YOLOE's backbone and head, significantly enhancing instance segmentation performance and inference speed through the design of a PP-YOLOE instance segmentation head. SOLOv2Inference Model/Trained Model 35.5 - - 179.1 M  SOLOv2 is a real-time instance segmentation algorithm that segments objects by location. This model is an improved version of SOLO, achieving a good balance between accuracy and speed through the introduction of mask learning and mask NMS. <p>Note: The above accuracy metrics are based on the Mask AP of the COCO2017 validation set. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Tutorial</p> <p>After installing the wheel package, a few lines of code can complete the inference of the instance segmentation module. You can switch models under this module freely, and you can also integrate the model inference of the instance segmentation module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"Mask-RT-DETR-L\")\noutput = model.predict(\"general_instance_segmentation_004.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, please refer to the PaddleX Single-Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you are seeking higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better instance segmentation models. Before using PaddleX to develop instance segmentation models, please ensure that you have installed the relevant model training plugins for segmentation in PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, it is necessary to prepare the corresponding dataset for each task module. PaddleX provides data verification functionality for each module, and only data that passes the verification can be used for model training. Additionally, PaddleX provides demo datasets for each module, allowing you to complete subsequent development based on the officially provided demo data. If you wish to use a private dataset for subsequent model training, you can refer to the PaddleX Instance Segmentation Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#411-download-demo-data","title":"4.1.1 Download Demo Data","text":"<p>You can download the demo dataset to a specified folder using the following command:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/instance_seg_coco_examples.tar -P ./dataset\ntar -xf ./dataset/instance_seg_coco_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#412-data-verification","title":"4.1.2 Data Verification","text":"<p>Data verification can be completed with a single command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/instance_seg_coco_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 2,\n    \"train_samples\": 79,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/pexels-photo-634007.jpeg\",\n      \"check_dataset/demo_img/pexels-photo-59576.png\"\n    ],\n    \"val_samples\": 19,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/peasant-farmer-farmer-romania-botiza-47862.jpeg\",\n      \"check_dataset/demo_img/pexels-photo-715546.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/instance_seg_coco_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCOInstSegDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 2;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 79;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 19;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized validation samples in this dataset; Additionally, the dataset verification also analyzes the distribution of sample numbers across all categories in the dataset and generates a distribution histogram (<code>histogram.png</code>):</li> </ul> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion/Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>The instance segmentation task supports converting <code>LabelMe</code> format to <code>COCO</code> format. The parameters for dataset format conversion can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Below are some example explanations for some of the parameters in the configuration file:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. Set to <code>True</code> to enable dataset format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set. The available source format is <code>LabelMe</code>; For example, if you want to convert a <code>LabelMe</code> dataset to <code>COCO</code> format, you need to modify the configuration file as follows:</li> </ul> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/instance_seg_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/instance_seg_labelme_examples.tar -C ./dataset/\n</code></pre> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: LabelMe\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml\\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/instance_seg_labelme_examples\n</code></pre> <p>After the data conversion is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml\\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/instance_seg_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example explanations for the parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, the dataset will be re-split. The default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is to be re-split, the percentage of the training set needs to be set. The type is any integer between 0-100, and the sum with <code>val_percent</code> must be 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, you need to modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/instance_seg_labelme_examples\n</code></pre> <p>After data splitting, the original annotation files will be renamed as <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/instance_seg_labelme_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command can complete model training. Taking the training of the instance segmentation model Mask-RT-DETR-L as an example:</p> <p><pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/instance_seg_coco_examples\n</code></pre> The following steps are required:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file of the model (here it is <code>Mask-RT-DETR-L.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify the first 2 GPUs for training: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration File Parameters Instructions.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/instance_seg_coco_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>Mask-RT-DETR-L.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction via the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-L.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_instance_segmentation_004.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>Mask-RT-DETR-L.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/instance_segmentation.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX Pipeline or into your own project.</p> <p>1.Pipeline Integration</p> <p>The instance segmentation module can be integrated into the General Instance Segmentation Pipeline of PaddleX. Simply replace the model path to update the instance segmentation module of the relevant pipeline.</p> <p>2.Module Integration The weights you produce can be directly integrated into the instance segmentation module. Refer to the Python example code in Quick Integration , and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html","title":"Mainbody detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#i-overview","title":"I. Overview","text":"<p>Mainbody detection is a fundamental task in object detection, aiming to identify and extract the location and size of specific target objects, people, or entities from images and videos. By constructing deep neural network models, mainbody detection learns the feature representations of image subjects to achieve efficient and accurate detection.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(0.5:0.95) mAP(0.5) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-ShiTuV2_detInference Model/Trained Model 41.5 62.0 33.7 537.0 27.54 A mainbody detection model based on PicoDet_LCNet_x2_5, which may detect multiple common subjects simultaneously. <p>Note: The evaluation set for the above accuracy metrics is  PaddleClas mainbody detection dataset mAP(0.5:0.95). GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Guide</p> <p>After installing the wheel package, you can perform mainbody detection inference with just a few lines of code. You can easily switch between models under this module, and integrate the mainbody detection model inference into your project. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"PP-ShiTuV2_det\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"general_object_detection_002.png\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on using PaddleX's single-model inference APIs, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better mainbody detection models. Before developing mainbody detection models with PaddleX, ensure you have installed the PaddleDetection plugin for PaddleX. The installation process can be found in PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the specific task module. PaddleX provides a data validation function for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to PaddleX Object Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/mainbody_det_examples.tar -P ./dataset\ntar -xf ./dataset/mainbody_det_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>You can complete data validation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/mainbody_det_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Details of validation results (click to expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 1,\n    \"train_samples\": 701,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/road839.png\",\n      \"check_dataset/demo_img/road363.png\",\n      \"check_dataset/demo_img/road148.png\"\n    ],\n    \"val_samples\": 176,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/road218.png\",\n      \"check_dataset/demo_img/road681.png\",\n      \"check_dataset/demo_img/road138.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/mainbody_det_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. The explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>\uff1aThe number of classes in this dataset is 1.</li> <li><code>attributes.train_samples</code>\uff1aThe number of samples in the training set of this dataset is 701.</li> <li><code>attributes.val_samples</code>\uff1aThe number of samples in the validation set of this dataset is 176.</li> <li><code>attributes.train_sample_paths</code>\uff1aA list of relative paths to the visualized images of samples in the training set of this dataset.</li> <li><code>attributes.val_sample_paths</code>\uff1a A list of relative paths to the visualized images of samples in the validation set of this dataset.</li> </ul> <p>The dataset validation also analyzes the distribution of sample counts across all classes in the dataset and generates a histogram (histogram.png) to visualize this distribution.</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Mainbody detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Dataset splitting parameters can be set by modifying the <code>CheckDataset</code> section in the configuration file. Some example parameters in the configuration file are explained below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/mainbody_det_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/mainbody_det_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of <code>PP-ShiTuV2_det</code> as an example:</p> <p><pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/mainbody_det_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-ShiTuV2_det.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/mainbody_det_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>PP-ShiTuV2_det.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer to PaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions. In PaddleX, model inference predictions can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference predictions through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/mainbody_detection/PP-ShiTuV2_det.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_object_detection_002.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it is <code>PP-ShiTuV2_det.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/mainbody_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The main body detection module can be integrated into PaddleX pipelines such as General Object Detection (comming soon). Simply replace the model path to update the main body detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your trained model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the main body detection module. You can refer to the Python example code in Quick Integration, simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html","title":"Object Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#i-overview","title":"I. Overview","text":"<p>The object detection module is a crucial component in computer vision systems, responsible for locating and marking regions containing specific objects in images or videos. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. The object detection module typically outputs bounding boxes for the target regions, which are then passed as input to the object recognition module for further processing.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#ii-list-of-supported-models","title":"II. List of Supported Models","text":"ModelModel Download Link mAP(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Storage Size (M) Description PicoDet-LInference Model/Trained Model 42.6 16.6715 169.904 20.9 M PP-PicoDet is a lightweight object detection algorithm for full-size, wide-angle targets, considering the computational capacity of mobile devices. Compared to traditional object detection algorithms, PP-PicoDet has a smaller model size and lower computational complexity, achieving higher speed and lower latency while maintaining detection accuracy. PicoDet-SInference Model/Trained Model 29.1 14.097 37.6563 4.4 M PP-YOLOE_plus-LInference Model/Trained Model 52.9 33.5644 814.825 185.3 M PP-YOLOE_plus is an upgraded version of the high-precision cloud-edge integrated model PP-YOLOE, developed by Baidu's PaddlePaddle vision team. By using the large-scale Objects365 dataset and optimizing preprocessing, it significantly enhances the model's end-to-end inference speed. PP-YOLOE_plus-SInference Model/Trained Model 43.7 16.8884 223.059 28.3 M RT-DETR-HInference Model/Trained Model 56.3 114.814 3933.39 435.8 M RT-DETR is the first real-time end-to-end object detector. The model features an efficient hybrid encoder to meet both model performance and throughput requirements, efficiently handling multi-scale features, and proposes an accelerated and optimized query selection mechanism to optimize the dynamics of decoder queries. RT-DETR supports flexible end-to-end inference speeds by using different decoders. RT-DETR-LInference Model/Trained Model 53.0 34.5252 1454.27 113.7 M <p>\u2757 The above list features the 6 core models that the image classification module primarily supports. In total, this module supports 37 models. The complete list of models is as follows:</p>  \ud83d\udc49Details of Model List ModelModel Download Link mAP(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Cascade-FasterRCNN-ResNet50-FPNInference Model/Trained Model 41.1 - - 245.4 M Cascade-FasterRCNN is an improved version of the Faster R-CNN object detection model. By coupling multiple detectors and optimizing detection results using different IoU thresholds, it addresses the mismatch problem between training and prediction stages, enhancing the accuracy of object detection. Cascade-FasterRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 45.0 - - 246.2 M CenterNet-DLA-34Inference Model/Trained Model 37.6 - - 75.4 M CenterNet is an anchor-free object detection model that treats the keypoints of the object to be detected as a single point\u2014the center point of its bounding box, and performs regression through these keypoints. CenterNet-ResNet50Inference Model/Trained Model 38.9 - - 319.7 M DETR-R50Inference Model/Trained Model 42.3 59.2132 5334.52 159.3 M DETR is a transformer-based object detection model proposed by Facebook. It achieves end-to-end object detection without the need for predefined anchor boxes or NMS post-processing strategies. FasterRCNN-ResNet34-FPNInference Model/Trained Model 37.8 - - 137.5 M Faster R-CNN is a typical two-stage object detection model that first generates region proposals and then performs classification and regression on these proposals. Compared to its predecessors R-CNN and Fast R-CNN, Faster R-CNN's main improvement lies in the region proposal aspect, using a Region Proposal Network (RPN) to provide region proposals instead of traditional selective search. RPN is a Convolutional Neural Network (CNN) that shares convolutional features with the detection network, reducing the computational overhead of region proposals. FasterRCNN-ResNet50-FPNInference Model/Trained Model 38.4 - - 148.1 M FasterRCNN-ResNet50-vd-FPNInference Model/Trained Model 39.5 - - 148.1 M FasterRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 41.4 - - 148.1 M FasterRCNN-ResNet50Inference Model/Trained Model 36.7 - - 120.2 M FasterRCNN-ResNet101-FPNInference Model/Trained Model 41.4 - - 216.3 M FasterRCNN-ResNet101Inference Model/Trained Model 39.0 - - 188.1 M FasterRCNN-ResNeXt101-vd-FPNInference Model/Trained Model 43.4 - - 360.6 M FasterRCNN-Swin-Tiny-FPNInference Model/Trained Model 42.6 - - 159.8 M FCOS-ResNet50Inference Model/Trained Model 39.6 103.367 3424.91 124.2 M FCOS is an anchor-free object detection model that performs dense predictions. It uses the backbone of RetinaNet and directly regresses the width and height of the target object on the feature map, predicting the object's category and centerness (the degree of offset of pixels on the feature map from the object's center), which is eventually used as a weight to adjust the object score. PicoDet-LInference Model/Trained Model 42.6 16.6715 169.904 20.9 M PP-PicoDet is a lightweight object detection algorithm designed for full-size and wide-aspect-ratio targets, with a focus on mobile device computation. Compared to traditional object detection algorithms, PP-PicoDet boasts smaller model sizes and lower computational complexity, achieving higher speeds and lower latency while maintaining detection accuracy. PicoDet-MInference Model/Trained Model 37.5 16.2311 71.7257 16.8 M PicoDet-SInference Model/Trained Model 29.1 14.097 37.6563 4.4 M PicoDet-XSInference Model/Trained Model 26.2 13.8102 48.3139 5.7 M PP-YOLOE_plus-LInference Model/Trained Model 52.9 33.5644 814.825 185.3 M PP-YOLOE_plus is an iteratively optimized and upgraded version of PP-YOLOE, a high-precision cloud-edge integrated model developed by Baidu PaddlePaddle's Vision Team. By leveraging the large-scale Objects365 dataset and optimizing preprocessing, it significantly enhances the end-to-end inference speed of the model. PP-YOLOE_plus-MInference Model/Trained Model 49.8 19.843 449.261 82.3 M PP-YOLOE_plus-SInference Model/Trained Model 43.7 16.8884 223.059 28.3 M PP-YOLOE_plus-XInference Model/Trained Model 54.7 57.8995 1439.93 349.4 M RT-DETR-HInference Model/Trained Model 56.3 114.814 3933.39 435.8 M RT-DETR is the first real-time end-to-end object detector. It features an efficient hybrid encoder that balances model performance and throughput, efficiently processes multi-scale features, and introduces an accelerated and optimized query selection mechanism to dynamize decoder queries. RT-DETR supports flexible end-to-end inference speeds through the use of different decoders. RT-DETR-LInference Model/Trained Model 53.0 34.5252 1454.27 113.7 M RT-DETR-R18Inference Model/Trained Model 46.5 19.89 784.824 70.7 M RT-DETR-R50Inference Model/Trained Model 53.1 41.9327 1625.95 149.1 M RT-DETR-XInference Model/Trained Model 54.8 61.8042 2246.64 232.9 M YOLOv3-DarkNet53Inference Model/Trained Model 39.1 40.1055 883.041 219.7 M YOLOv3 is a real-time end-to-end object detector that utilizes a unique single Convolutional Neural Network (CNN) to frame the object detection problem as a regression task, enabling real-time detection. The model employs multi-scale detection to enhance performance across different object sizes. YOLOv3-MobileNetV3Inference Model/Trained Model 31.4 18.6692 267.214 83.8 M YOLOv3-ResNet50_vd_DCNInference Model/Trained Model 40.6 31.6276 856.047 163.0 M YOLOX-LInference Model/Trained Model 50.1 185.691 1250.58 192.5 M Building upon YOLOv3's framework, YOLOX significantly boosts detection performance in complex scenarios by incorporating Decoupled Head, Data Augmentation, Anchor Free, and SimOTA components. YOLOX-MInference Model/Trained Model 46.9 123.324 688.071 90.0 M YOLOX-NInference Model/Trained Model 26.1 79.1665 155.59 3.4 M YOLOX-SInference Model/Trained Model 40.4 184.828 474.446 32.0 M YOLOX-TInference Model/Trained Model 32.9 102.748 212.52 18.1 M YOLOX-XInference Model/Trained Model 51.8 227.361 2067.84 351.5 M <p>Note: The precision metrics mentioned are based on the COCO2017 validation set mAP(0.5:0.95). All model GPU inference times are measured on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before proceeding with quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide.</p> <p>After installing the wheel package, you can perform object detection inference with just a few lines of code. You can easily switch between models within the module and integrate the object detection inference into your projects. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\nmodel = create_model(\"PicoDet-S\")\noutput = model.predict(\"general_object_detection_002.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on using PaddleX's single-model inference APIs, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher precision from existing models, you can leverage PaddleX's custom development capabilities to develop better object detection models. Before developing object detection models with PaddleX, ensure you have installed the object detection related training plugins. For installation instructions, refer to the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, prepare the corresponding dataset for the task module. PaddleX provides a data validation feature for each module, and only datasets that pass validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to the PaddleX Object Detection Task Module Data Annotation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#411-download-demo-data","title":"4.1.1 Download Demo Data","text":"<p>You can download the demo dataset to a specified folder using the following command:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_coco_examples.tar -P ./dataset\ntar -xf ./dataset/det_coco_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Validate your dataset with a single command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 4,\n    \"train_samples\": 701,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/road839.png\",\n      \"check_dataset/demo_img/road363.png\",\n      \"check_dataset/demo_img/road148.png\",\n      \"check_dataset/demo_img/road237.png\",\n      \"check_dataset/demo_img/road733.png\",\n      \"check_dataset/demo_img/road861.png\",\n      \"check_dataset/demo_img/road762.png\",\n      \"check_dataset/demo_img/road515.png\",\n      \"check_dataset/demo_img/road754.png\",\n      \"check_dataset/demo_img/road173.png\"\n    ],\n    \"val_samples\": 176,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/road218.png\",\n      \"check_dataset/demo_img/road681.png\",\n      \"check_dataset/demo_img/road138.png\",\n      \"check_dataset/demo_img/road544.png\",\n      \"check_dataset/demo_img/road596.png\",\n      \"check_dataset/demo_img/road857.png\",\n      \"check_dataset/demo_img/road203.png\",\n      \"check_dataset/demo_img/road589.png\",\n      \"check_dataset/demo_img/road655.png\",\n      \"check_dataset/demo_img/road245.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/det_coco_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 4;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 704;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 176;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of validation samples in this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all classes in the dataset and generates a histogram (histogram.png) for visualization:</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Object detection supports converting datasets in <code>VOC</code> and <code>LabelMe</code> formats to <code>COCO</code> format.</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. Object detection supports converting <code>VOC</code> and <code>LabelMe</code> format datasets to <code>COCO</code> format. Default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set. Default is <code>null</code>, with optional values <code>VOC</code>, <code>LabelMe</code>, <code>VOCWithUnlabeled</code>, <code>LabelMeWithUnlabeled</code>; For example, if you want to convert a <code>LabelMe</code> format dataset to <code>COCO</code> format, taking the following <code>LabelMe</code> format dataset as an example, you need to modify the configuration as follows:</li> </ul> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_labelme_examples.tar -P ./dataset\ntar -xf ./dataset/det_labelme_examples.tar -C ./dataset/\n</code></pre> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: LabelMe\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_labelme_examples\n</code></pre> <p>Of course, the above parameters also support being set by appending command line arguments. Taking a <code>LabelMe</code> format dataset as an example:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_labelme_examples \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. When <code>True</code>, dataset splitting is performed. Default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is re-split, the percentage of the training set needs to be set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If the dataset is re-split, the percentage of the validation set needs to be set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>train_percent</code> is 100; For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, you need to modify the configuration file as follows:</li> </ul> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> <p>After dataset splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_coco_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of the object detection model PicoDet-S as an example:</p> <p><pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> The following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PicoDet-S.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file instructions for the corresponding task module of the model PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/det_coco_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PicoDet-S.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference predictions through the command line, use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-S.yaml  \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_object_detection_002.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PicoDet-S.yaml</code>)</p> </li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/object_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipelines or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The object detection module can be integrated into the General Object Detection Pipeline of PaddleX. Simply replace the model path to update the object detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the object detection module. Refer to the Python example code in Quick Integration, and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html","title":"Pedestrian Attribute Recognition Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#i-overview","title":"I. Overview","text":"<p>Pedestrian attribute recognition is a crucial component in computer vision systems, responsible for locating and labeling specific attributes of pedestrians in images or videos, such as gender, age, clothing color, and type. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. The pedestrian attribute recognition module typically outputs attribute information for each pedestrian, which is then passed as input to other modules (e.g., pedestrian tracking, pedestrian re-identification) for subsequent processing.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_pedestrian_attributeInference Model/Trained Model 92.2 3.84845 9.23735 6.7 M PP-LCNet_x1_0_pedestrian_attribute is a lightweight pedestrian attribute recognition model based on PP-LCNet, covering 26 categories <p>Note: The above accuracy metrics are mA on PaddleX's internal self-built dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>After installing the wheel package, a few lines of code can complete the inference of the pedestrian attribute recognition module. You can easily switch models under this module and integrate the model inference of pedestrian attribute recognition into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0_pedestrian_attribute\")\noutput = model.predict(\"pedestrian_attribute_006.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p> <p>Note: The index of the <code>output</code> value represents the following attributes: index 0 indicates whether a hat is worn, index 1 indicates whether glasses are worn, indexes 2-7 represent the style of the upper garment, indexes 8-13 represent the style of the lower garment, index 14 indicates whether boots are worn, indexes 15-17 represent the type of bag carried, index 18 indicates whether an object is held in front, indexes 19-21 represent age, index 22 represents gender, and indexes 23-25 represent orientation. Specifically, the attributes include the following types:</p> <pre><code>- Gender: Male, Female\n- Age: Under 18, 18-60, Over 60\n- Orientation: Front, Back, Side\n- Accessories: Glasses, Hat, None\n- Holding Object in Front: Yes, No\n- Bag: Backpack, Shoulder Bag, Handbag\n- Upper Garment Style: Striped, Logo, Plaid, Patchwork\n- Lower Garment Style: Striped, Patterned\n- Short-sleeved Shirt: Yes, No\n- Long-sleeved Shirt: Yes, No\n- Long Coat: Yes, No\n- Pants: Yes, No\n- Shorts: Yes, No\n- Skirt: Yes, No\n- Boots: Yes, No\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better pedestrian attribute recognition models. Before developing pedestrian attribute recognition with PaddleX, ensure you have installed the classification-related model training plugins for PaddleX.  The installation process can be found in the custom development section of the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the specific task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to the PaddleX Multi-Label Classification Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/pedestrian_attribute_examples.tar -P ./dataset\ntar -xf ./dataset/pedestrian_attribute_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Run a single command to complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/pedestrian_attribute_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": \"../../dataset/pedestrian_attribute_examples/label.txt\",\n    \"num_classes\": 26,\n    \"train_samples\": 1000,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/020907.jpg\",\n      \"check_dataset/demo_img/004274.jpg\",\n      \"check_dataset/demo_img/009412.jpg\",\n      \"check_dataset/demo_img/026873.jpg\",\n      \"check_dataset/demo_img/030560.jpg\",\n      \"check_dataset/demo_img/022846.jpg\",\n      \"check_dataset/demo_img/009055.jpg\",\n      \"check_dataset/demo_img/015399.jpg\",\n      \"check_dataset/demo_img/006435.jpg\",\n      \"check_dataset/demo_img/055307.jpg\"\n    ],\n    \"val_samples\": 500,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/080381.jpg\",\n      \"check_dataset/demo_img/080469.jpg\",\n      \"check_dataset/demo_img/080146.jpg\",\n      \"check_dataset/demo_img/080003.jpg\",\n      \"check_dataset/demo_img/080283.jpg\",\n      \"check_dataset/demo_img/080104.jpg\",\n      \"check_dataset/demo_img/080149.jpg\",\n      \"check_dataset/demo_img/080313.jpg\",\n      \"check_dataset/demo_img/080131.jpg\",\n      \"check_dataset/demo_img/080412.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/pedestrian_attribute_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"MLClsDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 26;</li> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 1000;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 500;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of the length and width of all images in the dataset and plots a histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format or re-split the training/validation ratio of the dataset by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Dataset Format Conversion/Dataset Splitting Details (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Pedestrian attribute recognition does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>The dataset splitting parameters can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. An example of part of the configuration file is shown below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/pedestrian_attribute_examples\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/pedestrian_attribute_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command. Taking the training of the PP-LCNet pedestrian attribute recognition model (PP-LCNet_x1_0_pedestrian_attribute) as an example:</p> <p><pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/pedestrian_attribute_examples\n</code></pre> the following steps are required:</p> <ul> <li>Specify the path of the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_pedestrian_attribute.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path of the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first 2 GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file parameter instructions for the corresponding task module of the model PaddleX Common Model Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/pedestrian_attribute_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_pedestrian_attribute.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including MultiLabelMAP;</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/pedestrian_attribute/PP-LCNet_x1_0_pedestrian_attribute.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"pedestrian_attribute_006.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_pedestrian_attribute.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> . Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The pedestrian attribute recognition module can be integrated into the General Image Multi-label Classification Pipeline of PaddleX. Simply replace the model path to update the pedestrian attribute recognition module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the pedestrian attribute recognition module. Refer to the Python example code in Quick Integration  and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html","title":"Semantic Segmentation Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#i-overview","title":"I. Overview","text":"<p>Semantic segmentation is a technique in computer vision that classifies each pixel in an image, dividing the image into distinct semantic regions, with each region corresponding to a specific category. This technique generates detailed segmentation maps, clearly revealing objects and their boundaries in the image, providing powerful support for image analysis and understanding.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) OCRNet_HRNet-W48Inference Model/Trained Model 82.15 78.9976 2226.95 249.8 M PP-LiteSeg-TInference Model/Trained Model 73.10 7.6827 138.683 28.5 M <p>\u2757 The above list features the 2 core models that the image classification module primarily supports. In total, this module supports 18 models. The complete list of models is as follows:</p>  \ud83d\udc49Model List Details Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Deeplabv3_Plus-R50Inference Model/Trained Model 80.36 61.0531 1513.58 94.9 M Deeplabv3_Plus-R101Inference Model/Trained Model 81.10 100.026 2460.71 162.5 M Deeplabv3-R50Inference Model/Trained Model 79.90 82.2631 1735.83 138.3 M Deeplabv3-R101Inference Model/Trained Model 80.85 121.492 2685.51 205.9 M OCRNet_HRNet-W18Inference Model/Trained Model 80.67 48.2335 906.385 43.1 M OCRNet_HRNet-W48Inference Model/Trained Model 82.15 78.9976 2226.95 249.8 M PP-LiteSeg-TInference Model/Trained Model 73.10 7.6827 138.683 28.5 M PP-LiteSeg-BInference Model/Trained Model 75.25 10.9935 194.727 47.0 M SegFormer-B0 (slice)Inference Model/Trained Model 76.73 11.1946 268.929 13.2 M SegFormer-B1 (slice)Inference Model/Trained Model 78.35 17.9998 403.393 48.5 M SegFormer-B2 (slice)Inference Model/Trained Model 81.60 48.0371 1248.52 96.9 M SegFormer-B3 (slice)Inference Model/Trained Model 82.47 64.341 1666.35 167.3 M SegFormer-B4 (slice)Inference Model/Trained Model 82.38 82.4336 1995.42 226.7 M SegFormer-B5 (slice)Inference Model/Trained Model 82.58 97.3717 2420.19 229.7 M <p>The accuracy metrics of the above models are measured on the Cityscapes dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time Model Size (M) SeaFormer_base(slice)Inference Model/Trained Model 40.92 24.4073 397.574 30.8 M SeaFormer_large (slice)Inference Model/Trained Model 43.66 27.8123 550.464 49.8 M SeaFormer_small (slice)Inference Model/Trained Model 38.73 19.2295 358.343 14.3 M SeaFormer_tiny (slice)Inference Model/Trained Model 34.58 13.9496 330.132 6.1M <p>The accuracy metrics of the SeaFormer series models are measured on the ADE20k dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>Just a few lines of code can complete the inference of the Semantic Segmentation module, allowing you to easily switch between models under this module. You can also integrate the model inference of the the Semantic Segmentation module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LiteSeg-T\")\noutput = model.predict(\"general_semantic_segmentation_002.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy, you can leverage PaddleX's custom development capabilities to develop better Semantic Segmentation models. Before developing a Semantic Segmentation model with PaddleX, ensure you have installed PaddleClas plugin for PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#41-dataset-preparation","title":"4.1 Dataset Preparation","text":"<p>Before model training, you need to prepare a dataset for the task. PaddleX provides data validation functionality for each module. Only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Semantic Segmentation Task Module Data Preparation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/seg_optic_examples.tar -P ./dataset\ntar -xf ./dataset/seg_optic_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Data validation can be completed with a single command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_optic_examples\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Once the command runs successfully, a message saying <code>Check dataset passed !</code> will be printed in the log. The verification results will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be stored in the <code>./output/check_dataset</code> directory, including visual examples of sample images and a histogram of sample distribution.</p> \ud83d\udc49 Verification Result Details (click to expand) <p>The specific content of the verification result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/P0005.jpg\",\n      \"check_dataset/demo_img/P0050.jpg\"\n    ],\n    \"train_samples\": 267,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/N0139.jpg\",\n      \"check_dataset/demo_img/P0137.jpg\"\n    ],\n    \"val_samples\": 76,\n    \"num_classes\": 2\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/seg_optic_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"SegDataset\"\n}\n</code></pre> <p>The verification results above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 2;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 267;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 76;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of validation samples in this dataset;</li> </ul> <p>The dataset verification also analyzes the distribution of sample numbers across all classes and plots a histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#413-dataset-format-conversiondataset-splitting-optional-click-to-expand","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional) (Click to Expand)","text":"\ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> <p>(1) Dataset Format Conversion</p> <p>Semantic segmentation supports converting <code>LabelMe</code> format datasets to the required format.</p> <p>Parameters related to dataset verification can be set by modifying the <code>CheckDataset</code> fields in the configuration file. Example explanations for some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to enable dataset format conversion, supporting <code>LabelMe</code> format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, the source dataset format needs to be set, default is <code>null</code>, and the supported source dataset format is <code>LabelMe</code>;</li> </ul> <p>For example, if you want to convert a <code>LabelMe</code> format dataset, you can download a sample <code>LabelMe</code> format dataset as follows:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/seg_dataset_to_convert.tar -P ./dataset\ntar -xf ./dataset/seg_dataset_to_convert.tar -C ./dataset/\n</code></pre> <p>After downloading, modify the <code>paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml</code> configuration as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: LabelMe\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_dataset_to_convert\n</code></pre> <p>Of course, the above parameters also support being set by appending command-line arguments. For a <code>LabelMe</code> format dataset, the command is:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_dataset_to_convert \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> fields in the configuration file. Example explanations for some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to enable re-splitting the dataset, set to <code>True</code> to perform dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set, which should be an integer between 0 and 100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_optic_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/seg_optic_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the semantic segmentation model (PP-LiteSeg-T) as an example:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/seg_optic_examples\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-LiteSeg-T.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to train using the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, with the default path being <code>output</code>. To specify a different save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced, and static graph weights are used by default for model inference.</li> <li> <p>After model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, including whether the training task completed successfully, produced weight metrics, and related file paths.</p> </li> <li><code>train.log</code>: Training log file, recording model metric changes, loss changes, etc.</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameters used for this training session.</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, and static graph network structure.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After model training, you can evaluate the specified model weights on the validation set to verify model accuracy. Using PaddleX for model evaluation requires just one command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/seg_optic_examples\n</code></pre> <p>Similar to model training, follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-LiteSeg-T.yaml</code>).</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the validation dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For more details, refer to the PaddleX Common Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After model evaluation, the following outputs are typically produced:</p> <ul> <li><code>evaluate_result.json</code>: Records the evaluation results, specifically whether the evaluation task completed successfully and the model's evaluation metrics, including mIoU.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions via the command line, use the following command. Before running the following code, please download the demo image to your local machine.</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"general_semantic_segmentation_002.png\"\n</code></pre> <p>Similar to model training and evaluation, the following steps are required:</p> <ul> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>PP-LCNet_x1_0_doc_ori.yaml</code>)</p> </li> <li> <p>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></p> </li> <li> <p>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></p> </li> </ul> <p>Specify the input data path: <code>-o Predict.inputh=\"...\"</code> Other related parameters can be set by modifying the fields under Global and Predict in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</p> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</p>"},{"location":"en/module_usage/tutorials/cv_modules/semantic_segmentation.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own projects.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The document semantic segmentation module can be integrated into PaddleX pipelines such as the Semantic Segmentation Pipeline (Seg). Simply replace the model path to update the The document semantic segmentation module's model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the semantic segmentation module. You can refer to the Python sample code in Quick Integration and just replace the model with the path to the model you trained.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html","title":"Small Object Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#i-overview","title":"I. Overview","text":"<p>Small object detection typically refers to accurately detecting and locating small-sized target objects in images or videos. These objects often have a small pixel size in images, typically less than 32x32 pixels (as defined by datasets like MS COCO), and may be obscured by the background or other objects, making them difficult to observe directly by the human eye. Small object detection is an important research direction in computer vision, aiming to precisely detect small objects with minimal visual features in images.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(0.5:0.95) mAP(0.5) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description (VisDrone) PP-YOLOE_plus_SOD-LInference Model/Trained Model 31.9 52.1 57.1 1007.0 324.93 PP-YOLOE_plus small object detection model trained on VisDrone. VisDrone is a benchmark dataset specifically for unmanned aerial vehicle (UAV) visual data, which is used for small object detection due to the small size of the targets and the inherent challenges they pose. PP-YOLOE_plus_SOD-SInference Model/Trained Model 25.1 42.8 65.5 324.4 77.29 PP-YOLOE_plus_SOD-largesize-LInference Model/Trained Model 42.7 65.9 458.5 11172.7 340.42 <p>Note: The evaluation set for the above accuracy metrics is VisDrone-DET dataset mAP(0.5:0.95). GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>After installing the wheel package, you can complete the inference of the small object detection module with just a few lines of code. You can switch models under this module freely, and you can also integrate the model inference of the small object detection module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\n\nmodel_name = \"PP-YOLOE_plus_SOD-S\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"small_object_detection.jpg\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better small object detection models. Before using PaddleX to develop small object detection models, ensure you have installed PaddleX's Detection-related model training capabilities. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the specific task module. PaddleX provides a data validation function for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to PaddleX Object Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/small_det_examples.tar -P ./dataset\ntar -xf ./dataset/small_det_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>You can complete data validation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/small_det_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Details of validation results (click to expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 10,\n    \"train_samples\": 1610,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/9999938_00000_d_0000352.jpg\",\n      \"check_dataset/demo_img/9999941_00000_d_0000014.jpg\",\n      \"check_dataset/demo_img/9999973_00000_d_0000043.jpg\"\n    ],\n    \"val_samples\": 548,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/0000330_00801_d_0000804.jpg\",\n      \"check_dataset/demo_img/0000103_00180_d_0000026.jpg\",\n      \"check_dataset/demo_img/0000291_04001_d_0000888.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/small_det_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. The explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>\uff1aThe number of classes in this dataset is 10.</li> <li><code>attributes.train_samples</code>\uff1aThe number of samples in the training set of this dataset is 1610.</li> <li><code>attributes.val_samples</code>\uff1aThe number of samples in the validation set of this dataset is 548.</li> <li><code>attributes.train_sample_paths</code>\uff1aA list of relative paths to the visualized images of samples in the training set of this dataset.</li> <li><code>attributes.val_sample_paths</code>\uff1a A list of relative paths to the visualized images of samples in the validation set of this dataset.</li> </ul> <p>The dataset validation also analyzes the distribution of sample counts across all classes in the dataset and generates a histogram (histogram.png) to visualize this distribution. </p> <p></p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Small object detection supports converting datasets in <code>VOC</code> and <code>LabelMe</code> formats to <code>COCO</code> format.</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. Small object detection supports converting <code>VOC</code> and <code>LabelMe</code> format datasets to <code>COCO</code> format. Default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set. Default is <code>null</code>, with optional values <code>VOC</code>, <code>LabelMe</code>, <code>VOCWithUnlabeled</code>, <code>LabelMeWithUnlabeled</code>; For example, if you want to convert a <code>LabelMe</code> format dataset to <code>COCO</code> format, taking the following <code>LabelMe</code> format dataset as an example, you need to modify the configuration as follows:</li> </ul> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: LabelMe\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./path/to/your_smallobject_labelme_dataset\n</code></pre> <p>Of course, the above parameters also support being set by appending command line arguments. Taking a <code>LabelMe</code> format dataset as an example:</p> <pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./path/to/your_smallobject_labelme_dataset \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=LabelMe\n</code></pre> <p>(2) Dataset Splitting</p> <p>Dataset splitting parameters can be set by modifying the <code>CheckDataset</code> section in the configuration file. Some example parameters in the configuration file are explained below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/small_det_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/small_det_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of <code>PP-YOLOE_plus_SOD-S</code> as an example:</p> <p><pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/small_det_examples \\\n    -o Train.num_classes=10\n</code></pre> The steps required are:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-YOLOE_plus_SOD-S.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/small_det_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>PP-YOLOE_plus_SOD-S.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer to PaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions. In PaddleX, model inference predictions can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference predictions through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/small_object_detection/PP-YOLOE_plus_SOD-S.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"small_object_detection.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-YOLOE_plus_SOD-S.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weight path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Explanation.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/small_object_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipelines or directly into your own projects.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The small object detection module can be integrated into the Small Object Detection Pipeline of PaddleX. Simply replace the model path to update the small object detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your obtained model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the small object detection module. You can refer to the Python example code in Quick Integration, simply replacing the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html","title":"Vehicle Attribute Recognition Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#i-overview","title":"I. Overview","text":"<p>Vehicle attribute recognition is a crucial component in computer vision systems. Its primary task is to locate and label specific attributes of vehicles in images or videos, such as vehicle type, color, license plate number, etc. The performance of this module directly impacts the accuracy and efficiency of the entire computer vision system. The vehicle attribute recognition module typically outputs bounding boxes (Bounding Boxes) containing vehicle attribute information, which are then passed as input to other modules (e.g., vehicle tracking, vehicle re-identification) for subsequent processing.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_vehicle_attributeInference Model/Trained Model 91.7 3.84845 9.23735 6.7 M PP-LCNet_x1_0_vehicle_attribute is a lightweight vehicle attribute recognition model based on PP-LCNet. <p>Note: The above accuracy metrics are mA on the VeRi dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Guide</p> <p>After installing the wheel package, a few lines of code can complete the inference of the vehicle attribute recognition module. You can easily switch models under this module, and you can also integrate the model inference of the vehicle attribute recognition module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0_vehicle_attribute\")\noutput = model.predict(\"vehicle_attribute_007.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to PaddleX Single Model Python Script Usage Instructions.</p> <p>Note: In the <code>output</code>, values indexed from 0-9 represent color attributes, corresponding to the following colors respectively: yellow, orange, green, gray, red, blue, white, golden, brown, black. Indices 10-18 represent vehicle type attributes, corresponding to the following vehicle types: sedan, suv, van, hatchback, mpv, pickup, bus, truck, estate.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better vehicle attribute recognition models. Before using PaddleX to develop vehicle attribute recognition models, ensure you have installed the classification-related model training plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides a data validation function for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Multi-Label Classification Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/vehicle_attribute_examples.tar -P ./dataset\ntar -xf ./dataset/vehicle_attribute_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_attribute_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": \"../../dataset/vehicle_attribute_examples/label.txt\",\n    \"num_classes\": 19,\n    \"train_samples\": 1200,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/0018_c017_00033140_0.jpg\",\n      \"check_dataset/demo_img/0010_c019_00034275_0.jpg\",\n      \"check_dataset/demo_img/0015_c019_00068660_0.jpg\",\n      \"check_dataset/demo_img/0016_c017_00049590_1.jpg\",\n      \"check_dataset/demo_img/0018_c016_00052280_0.jpg\",\n      \"check_dataset/demo_img/0023_c001_00006995_0.jpg\",\n      \"check_dataset/demo_img/0022_c004_00065910_0.jpg\",\n      \"check_dataset/demo_img/0007_c019_00048655_1.jpg\",\n      \"check_dataset/demo_img/0022_c007_00072970_0.jpg\",\n      \"check_dataset/demo_img/0022_c008_00065785_0.jpg\"\n    ],\n    \"val_samples\": 300,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/0025_c003_00054095_0.jpg\",\n      \"check_dataset/demo_img/0023_c013_00006350_1.jpg\",\n      \"check_dataset/demo_img/0024_c003_00046320_0.jpg\",\n      \"check_dataset/demo_img/0025_c005_00054795_2.jpg\",\n      \"check_dataset/demo_img/0024_c012_00041770_0.jpg\",\n      \"check_dataset/demo_img/0024_c007_00060845_1.jpg\",\n      \"check_dataset/demo_img/0023_c017_00013150_0.jpg\",\n      \"check_dataset/demo_img/0024_c014_00040410_0.jpg\",\n      \"check_dataset/demo_img/0025_c002_00050685_1.jpg\",\n      \"check_dataset/demo_img/0025_c005_00032645_0.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/vehicle_attribute_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"MLClsDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 19;</li> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 1200;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 300;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of the length and width of all images in the dataset and plots a histogram (histogram.png): </p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Vehicle attribute recognition does not support dataset format conversion.</p> <p>(2) Dataset Splitting</p> <p>The dataset splitting parameters can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. An example of part of the configuration file is shown below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The value should be an integer between 0 and 100, and the sum with <code>val_percent</code> should be 100;</li> </ul> <p>For example, if you want to re-split the dataset with 90% training set and 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_attribute_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_attribute_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>Training a model can be done with a single command, taking the training of the PP-LCNet vehicle attribute recognition model (<code>PP-LCNet_x1_0_vehicle_attribute</code>) as an example:</p> <p><pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/vehicle_attribute_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>PP-LCNet_x1_0_vehicle_attribute.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml  \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/vehicle_attribute_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_vehicle_attribute.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including MultiLabelMAP;</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/vehicle_attribute/PP-LCNet_x1_0_vehicle_attribute.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"vehicle_attribute_007.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-LCNet_x1_0_vehicle_attribute.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The vehicle attribute recognition module can be integrated into the General Image Multi-label Classification Pipeline of PaddleX. Simply replace the model path to update the vehicle attribute recognition module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the vehicle attribute recognition module. Refer to the Python example code in  Quick Integration  and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_detection.html","title":"Vehicle Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/cv_modules/vehicle_detection.html#i-overview","title":"I. Overview","text":"<p>Vehicle detection is a subtask of object detection, specifically referring to the use of computer vision technology to determine the presence of vehicles in images or videos and provide specific location information for each vehicle (such as the coordinates of the bounding box). This information is of great significance for various fields such as intelligent transportation systems, autonomous driving, and video surveillance.</p>"},{"location":"en/module_usage/tutorials/cv_modules/vehicle_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model mAP 0.5:0.95 GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-YOLOE-S_vehicle 61.3 15.4 178.4 28.79 Vehicle detection model based on PP-YOLOE PP-YOLOE-L_vehicle 63.9 32.6 775.6 196.02 Note: The evaluation set for the above accuracy metrics is PPVehicle dataset mAP(0.5:0.95). GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.   ## III. Quick Integration &gt; \u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the [PaddleX Local Installation Guide](../../../installation/installation.en.md)  After installing the wheel package, you can complete the inference of the vehicle detection module with just a few lines of code. You can switch models under this module freely, and you can also integrate the model inference of the vehicle detection module into your project. Before running the following code, please download the [demo image](https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_detection.jpg) to your local machine.  <pre><code>from paddlex import create_model\n\nmodel_name = \"PP-YOLOE-S_vehicle\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"vehicle_detection.jpg\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the [PaddleX Single-Model Python Script Usage Instructions](../../instructions/model_python_API.en.md).  ## IV. Custom Development If you are seeking higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better vehicle detection models. Before using PaddleX to develop vehicle detection models, please ensure that you have installed the PaddleDetection plugin for PaddleX. The installation process can be found in the [PaddleX Local Installation Guide](../../../installation/installation.en.md).  ### 4.1 Data Preparation Before model training, you need to prepare a dataset for the specific task module. PaddleX provides a data validation function for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to [PaddleX Object Detection Task Module Data Annotation Tutorial](../../../data_annotations/cv_modules/object_detection.en.md).  #### 4.1.1 Demo Data Download You can download the demo dataset to a specified folder using the following commands:  <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/vehicle_coco_examples.tar -P ./dataset\ntar -xf ./dataset/vehicle_coco_examples.tar -C ./dataset/\n</code></pre>  #### 4.1.2 Data Validation You can complete data validation with a single command:  <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_coco_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message `Check dataset passed !`. The validation result file will be saved in `./output/check_dataset_result.json`, and related outputs will be saved in the `./output/check_dataset` directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.  \ud83d\udc49 Details of validation results (click to expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 4,\n    \"train_samples\": 500,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/MVI_20011__img00001.jpg\",\n      \"check_dataset/demo_img/MVI_20011__img00005.jpg\",\n      \"check_dataset/demo_img/MVI_20011__img00009.jpg\"\n    ],\n    \"val_samples\": 100,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/MVI_20032__img00401.jpg\",\n      \"check_dataset/demo_img/MVI_20032__img00405.jpg\",\n      \"check_dataset/demo_img/MVI_20032__img00409.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/vehicle_coco_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. The explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>\uff1aThe number of classes in this dataset is 4.</li> <li><code>attributes.train_samples</code>\uff1aThe number of samples in the training set of this dataset is 500.</li> <li><code>attributes.val_samples</code>\uff1aThe number of samples in the validation set of this dataset is 100.</li> <li><code>attributes.train_sample_paths</code>\uff1aA list of relative paths to the visualized images of samples in the training set of this dataset.</li> <li><code>attributes.val_sample_paths</code>\uff1a A list of relative paths to the visualized images of samples in the validation set of this dataset.</li> </ul> <p>The dataset validation also analyzes the distribution of sample counts across all classes in the dataset and generates a histogram (histogram.png) to visualize this distribution. </p> <p></p>  #### 4.1.3 Dataset Format Conversion / Dataset Splitting (Optional) After completing the dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.  \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Vehicle detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Dataset splitting parameters can be set by modifying the <code>CheckDataset</code> section in the configuration file. Some example parameters in the configuration file are explained below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_coco_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/vehicle_coco_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>  ### 4.2 Model Training Model training can be completed with a single command, taking the training of `PP-YOLOE-S_vehicle` as an example:  <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/vehicle_coco_examples\n</code></pre> The steps required are:  * Specify the `.yaml` configuration file path for the model (here it is `PP-YOLOE-S_vehicle.yaml`,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the [PaddleX Model List (CPU/GPU)](../../../support_list/models_list.en.md)) * Specify the mode as model training: `-o Global.mode=train` * Specify the training dataset path: `-o Global.dataset_dir` Other related parameters can be set by modifying the `Global` and `Train` fields in the `.yaml` configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: `-o Global.device=gpu:0,1`; to set the number of training epochs to 10: `-o Train.epochs_iters=10`. For more modifiable parameters and their detailed explanations, refer to the [PaddleX Common Configuration Parameters for Model Tasks](../../instructions/config_parameters_common.en.md).  \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>  ### 4.3 Model Evaluation After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:  <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/vehicle_coco_examples\n</code></pre> Similar to model training, the process involves the following steps:  * Specify the path to the `.yaml` configuration file for the model\uff08here it's `PP-YOLOE-S_vehicle.yaml`\uff09 * Set the mode to model evaluation: `-o Global.mode=evaluate` * Specify the path to the validation dataset: `-o Global.dataset_dir` Other related parameters can be configured by modifying the fields under `Global` and `Evaluate` in the `.yaml` configuration file. For detailed information, please refer to[PaddleX Common Configuration Parameters for Models](../../instructions/config_parameters_common.en.md)\u3002  \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>  ### 4.4 Model Inference After completing model training and evaluation, you can use the trained model weights for inference predictions. In PaddleX, model inference predictions can be achieved through two methods: command line and wheel package.  #### 4.4.1 Model Inference The model can be directly integrated into the PaddleX pipeline or into your own project.  1. Pipeline Integration  The object detection module can be integrated into the [General Object Detection Pipeline](../../../pipeline_usage/tutorials/cv_pipelines/object_detection.en.md) of PaddleX. Simply replace the model path to update the object detection module of the relevant pipeline. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your trained model.  2. Module Integration  The weights you produced can be directly integrated into the object detection module. You can refer to the Python example code in [Quick Integration](#\u200b\u4e09\u200b\u5feb\u901f\u200b\u96c6\u6210\u200b), simply replace the model with the path to your trained model.  * To perform inference predictions through the command line, simply use the following command. Before running the following code, please download the [demo image](https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_detection.jpg) to your local machine. <pre><code>python main.py -c paddlex/configs/vehicle_detection/PP-YOLOE-S_vehicle.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"vehicle_detection.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:  * Specify the `.yaml` configuration file path of the model (here it is `PP-YOLOE-S_vehicle.yaml`) * Set the mode to model inference prediction: `-o Global.mode=predict` * Specify the model weight path: `-o Predict.model_dir=\"./output/best_model/inference\"` * Specify the input data path: `-o Predict.input=\"...\"` Other related parameters can be set by modifying the fields under `Global` and `Predict` in the `.yaml` configuration file. For details, please refer to [PaddleX Common Model Configuration File Parameter Description](../../instructions/config_parameters_common.en.md).  #### 4.4.2 Model Integration The weights you produced can be directly integrated into the vehicle detection module. You can refer to the Python example code in [Quick Integration](#iii-quick-integration), simply replace the model with the path to your trained model."},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html","title":"Document Image Orientation Classification Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#i-overview","title":"I. Overview","text":"<p>The document image orientation classification module is aim to distinguish the orientation of document images and correct them through post-processing. In processes such as document scanning and ID card photography, capturing devices are sometimes rotated to obtain clearer images, resulting in images with varying orientations. Standard OCR pipelines cannot effectively handle such data. By utilizing image classification technology, we can pre-judge the orientation of document or ID card images containing text regions and adjust their orientations, thereby enhancing the accuracy of OCR processing.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Top-1 Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Trained Model 99.06 3.84845 9.23735 7 A document image classification model based on PP-LCNet_x1_0, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 <p>Note: The above accuracy metrics are evaluated on a self-built dataset covering various scenarios such as IDs and documents, containing 1000 images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Tutorial</p> <p>Just a few lines of code can complete the inference of the document image orientation classification module, allowing you to easily switch between models under this module. You can also integrate the model inference of the the document image orientation classification module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-LCNet_x1_0_doc_ori\")\noutput = model.predict(\"img_rot180_demo.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/demo.png\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single model inference API, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy, you can leverage PaddleX's custom development capabilities to develop better document image orientation classification models. Before developing a document image orientation classification model with PaddleX, ensure you have installed PaddleClas plugin for PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the task. PaddleX provides data validation functionality for each module. Only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Image Classification Task Module Data Preparation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/text_image_orientation.tar -P ./dataset\ntar -xf ./dataset/text_image_orientation.tar  -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Data validation can be completed with a single command:</p> <pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/text_image_orientation\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Once the command runs successfully, a message saying <code>Check dataset passed !</code> will be printed in the log. The verification results will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be stored in the <code>./output/check_dataset</code> directory, including visual examples of sample images and a histogram of sample distribution.</p> \ud83d\udc49 Verification Result Details (click to expand) <p>The specific content of the verification result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": \"..\\/..\\/text_image_orientation\\/label.txt\",\n    \"num_classes\": 4,\n    \"train_samples\": 1553,\n    \"train_sample_paths\": [\n      \"check_dataset\\/demo_img\\/img_rot270_10351.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot0_3908.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot180_7712.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot0_7480.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot270_9599.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_10323.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_4885.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot180_3939.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_7153.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot180_1747.jpg\"\n    ],\n    \"val_samples\": 2593,\n    \"val_sample_paths\": [\n      \"check_dataset\\/demo_img\\/img_rot270_3190.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot0_10272.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot0_9930.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_918.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot180_2079.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_8574.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_7595.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_1751.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot180_1573.jpg\",\n      \"check_dataset\\/demo_img\\/img_rot90_4401.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/text_image_orientation\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"ClsDataset\"\n}\n</code></pre> <p>In the verification results above, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations of other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 4;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 1552;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 2593;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to visual sample images for the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visual samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset validation analyzes the sample number distribution across all classes in the dataset and generates a distribution histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Document image orientation classification does not currently support dataset format conversion.</p> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/text_image_orientation\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/text_image_orientation \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the document image orientation classification model (PP-LCNet_x1_0_doc_ori) as an example:</p> <pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/text_image_orientation\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here, <code>PP-LCNet_x1_0_doc_ori.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code>.</li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code>.</li> </ul> <p>Other relevant parameters can be set by modifying fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or by appending arguments to the command line. For example, to specify the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and detailed explanations, refer to the PaddleX General Model Configuration File Parameters.</p> \ud83d\udc49 More Information (click to expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, defaulting to <code>output</code>. If you want to specify a different save path, you can set it using the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts away the concept of dynamic graph weights and static graph weights. During model training, it produces both dynamic and static graph weights. For model inference, it defaults to using static graph weights.</li> <li> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, which records whether the training task was completed normally, as well as the output weight metrics and related file paths.</p> </li> <li><code>train.log</code>: Training log file, which records changes in model metrics and loss during training.</li> <li><code>config.yaml</code>: Training configuration file, which records the hyperparameter configuration for this training.</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. With PaddleX, model evaluation can be done with just one command:</p> <p><pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/text_image_orientation\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it is <code>PP-LCNet_x1_0_doc_ori.yaml</code>).</li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code>.</li> <li>Specify the path to the model weights: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code>.</li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code>. Other relevant parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX General Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Information (click to expand) <ul> <li> <p>When conducting model evaluation, it is necessary to specify the model weight file path. Each configuration file has a built-in default path for saving weights. If you need to change this path, you can simply append a command line argument to set it, for example: <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> </li> <li> <p>After the model evaluation is completed, typically, the following outputs are generated:</p> </li> <li> <p>Upon finishing the model evaluation, an evaluate_result.json file is produced, which records the results of the evaluation. Specifically, it logs whether the evaluation task was successfully completed and the evaluation metrics of the model, including <code>Top1 Accuracy (Top1 Acc)</code>.</p> </li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions via the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <pre><code>python main.py -c paddlex/configs/doc_text_orientation/PP-LCNet_x1_0_doc_ori.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"img_rot180_demo.jpg\"\n</code></pre> <p>Similar to model training and evaluation, the following steps are required:</p> <ul> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>PP-LCNet_x1_0_doc_ori.yaml</code>)</p> </li> <li> <p>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></p> </li> <li> <p>Specify the model weights path: -o Predict.model_dir=\"./output/best_accuracy/inference\"</p> </li> </ul> <p>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under Global and Predict in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</p> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own projects.</p> <p>1.Pipeline Integration</p> <p>The document image classification module can be integrated into PaddleX pipelines such as the Document Scene Information Extraction Pipeline (PP-ChatOCRv3). Simply replace the model path to update the The document image classification module's model.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the document image orientation classification module. You can refer to the Python sample code in Quick Integration and just replace the model with the path to the model you trained.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html","title":"Formula Recognition Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#i-overview","title":"I. Overview","text":"<p>The formula recognition module is a crucial component of OCR (Optical Character Recognition) systems, responsible for converting mathematical formulas in images into editable text or computer-readable formats. The performance of this module directly impacts the accuracy and efficiency of the entire OCR system. The module typically outputs LaTeX or MathML codes of mathematical formulas, which are then passed on to the text understanding module for further processing.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Normed Edit Distance BLEU Score ExpRate (%) Model Size (M) Description LaTeX_OCR_recInference Model/Trained Model 0.8821 0.0823 40.01 89.7 M LaTeX-OCR is a formula recognition algorithm based on an autoregressive large model. By adopting Hybrid ViT as the backbone network and transformer as the decoder, it significantly improves the accuracy of formula recognition. <p>Note: The above accuracy metrics are measured on the LaTeX-OCR formula recognition test set.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide.</p> <p>After installing the wheel package, a few lines of code can complete the inference of the formula recognition module. You can switch models under this module freely, and you can also integrate the model inference of the formula recognition module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"LaTeX_OCR_rec\")\noutput = model.predict(\"general_formula_rec_001.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you aim for higher accuracy with existing models, you can leverage PaddleX's custom development capabilities to develop better formula recognition models. Before developing formula recognition models with PaddleX, ensure you have installed the PaddleOCR-related model training plugins for PaddleX. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides a data validation function for each module, and only data that passes the validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to the LaTeX-OCR Formula Recognition Project.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following command:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_latexocr_dataset_example.tar -P ./dataset\ntar -xf ./dataset/ocr_rec_latexocr_dataset_example.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 9452,\n    \"train_sample_paths\": [\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0109284.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0217434.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0166758.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0022294.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/val_0071799.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0017043.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0026204.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0209202.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/val_0157332.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0232582.png\"\n    ],\n    \"val_samples\": 1050,\n    \"val_sample_paths\": [\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0070221.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0157901.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0085392.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0196480.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0096180.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0136149.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0143310.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0004560.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0115191.png\",\n      \"../dataset/ocr_rec_latexocr_dataset_example/images/train_0015323.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/ocr_rec_latexocr_dataset_example\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"LaTeXOCRDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows: * <code>attributes.train_samples</code>: The number of training samples in this dataset is 9452; * <code>attributes.val_samples</code>: The number of validation samples in this dataset is 1050; * <code>attributes.train_sample_paths</code>: A list of relative paths to the visualized training samples in this dataset; * <code>attributes.val_sample_paths</code>: A list of relative paths to the visualized validation samples in this dataset;</p> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all categories in the dataset and generates a distribution histogram (<code>histogram.png</code>): </p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the data verification, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>The formula recognition supports converting <code>MSTextRecDataset</code> format datasets to <code>LaTeXOCRDataset</code> format ( <code>PKL</code> format ). The parameters for dataset format conversion can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion. Formula recognition supports converting <code>MSTextRecDataset</code> format datasets to <code>LaTeXOCRDataset</code> format, default is <code>True</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format needs to be set, default is <code>MSTextRecDataset</code>;</li> </ul> <p>For example, if you want to convert a <code>MSTextRecDataset</code> format dataset to <code>LaTeXOCRDataset</code> format, you need to modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: MSTextRecDataset\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example\n</code></pre> <p>After the data conversion is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c  paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example \\\n    -o CheckDataset.convert.enable=True \\\n    -o CheckDataset.convert.src_dataset_type=MSTextRecDataset\n</code></pre> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, dataset splitting is performed, default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is re-split, the percentage of the training set needs to be set, which is an integer between 0 and 100, and the sum with <code>val_percent</code> should be 100;</li> </ul> <p>For example, if you want to re-split the dataset with 90% for the training set and 10% for the validation set, you need to modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support being set by appending command line arguments:</p> <pre><code>python main.py -c  paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command, taking the training of the formula recognition model LaTeX_OCR_rec as an example:</p> <p><pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml  \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example\n</code></pre> The following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>LaTeX_OCR_rec.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file instructions for the corresponding task module of the model PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml  \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ocr_rec_latexocr_dataset_example\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>LaTeX_OCR_rec.yaml</code>)</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file, detailed instructions can be found in PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including recall1\u3001recall5\u3001mAP\uff1b</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/formula_recognition/LaTeX_OCR_rec.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_accuracy/inference\" \\\n    -o Predict.input=\"general_formula_rec_001.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>LaTeX_OCR_rec.yaml</code>)</li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_accuracy/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/formula_recognition.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The weights you produce can be directly integrated into the formula recognition module. Refer to the Python example code in Quick Integration, and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html","title":"Layout Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#i-overview","title":"I. Overview","text":"<p>The core task of structure analysis is to parse and segment the content of input document images. By identifying different elements in the image (such as text, charts, images, etc.), they are classified into predefined categories (e.g., pure text area, title area, table area, image area, list area, etc.), and the position and size of these regions in the document are determined.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1xInference Model/Trained Model 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_tableInference Model/Trained Model 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3clsInference Model/Trained Model 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17clsInference Model/Trained Model 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3clsInference Model/Trained Model 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17clsInference Model/Trained Model 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3clsInference Model/Trained Model 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout region analysis dataset, containing 10,000 images of common document types, including English and Chinese papers, magazines, research reports, etc. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Tutorial</p> <p>After installing the wheel package, a few lines of code can complete the inference of the structure analysis module. You can switch models under this module freely, and you can also integrate the model inference of the structure analysis module into your project. Before running the following code, please download the demo image to your local machine.</p> <pre><code>from paddlex import create_model\n\nmodel_name = \"PicoDet-L_layout_3cls\"\n\nmodel = create_model(model_name)\noutput = model.predict(\"layout.jpg\", batch_size=1)\n\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> <p>For more information on using PaddleX's single-model inference API, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better structure analysis models. Before developing a structure analysis model with PaddleX, ensure you have installed PaddleX's Detection-related model training capabilities. The installation process can be found in PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides a data validation function for each module, and only data that passes the validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development based on the official demos. If you wish to use private datasets for subsequent model training, refer to the PaddleX Object Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_layout_examples.tar -P ./dataset\ntar -xf ./dataset/det_layout_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_layout_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and collect its basic information. Upon successful execution, the log will print the message <code>Check dataset passed !</code>. The validation result file will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be saved in the <code>./output/check_dataset</code> directory of the current directory. The output directory includes visualized example images and histograms of sample distributions.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 11,\n    \"train_samples\": 90,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/JPEGImages/train_0077.jpg\",\n      \"check_dataset/demo_img/JPEGImages/train_0028.jpg\",\n      \"check_dataset/demo_img/JPEGImages/train_0012.jpg\"\n    ],\n    \"val_samples\": 20,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/JPEGImages/val_0007.jpg\",\n      \"check_dataset/demo_img/JPEGImages/val_0019.jpg\",\n      \"check_dataset/demo_img/JPEGImages/val_0010.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/example_data/det_layout_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>The verification results mentioned above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Details of other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 11;</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 90;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 20;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of validation samples in this dataset;</li> </ul> <p>The dataset verification also analyzes the distribution of sample numbers across all classes and generates a histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Layout detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> section in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_layout_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_layout_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command is sufficient to complete model training, taking the training of PicoDet-L_layout_3cls as an example:</p> <p><pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/det_layout_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file of the model (here it is <code>PicoDet-L_layout_3cls.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters for Model Tasks.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation, you can complete the evaluation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/det_layout_examples\n</code></pre> Similar to model training, the process involves the following steps:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file for the model\uff08here it's <code>PicoDet-L_layout_3cls.yaml</code>\uff09</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be configured by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For detailed information, please refer to PaddleX Common Configuration Parameters for Models\u3002</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be generated, which records the evaluation results, specifically whether the evaluation task was completed successfully, and the model's evaluation metrics, including AP.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#44-model-inference","title":"4.4 Model Inference","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions. In PaddleX, model inference predictions can be achieved through two methods: command line and wheel package.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>To perform inference predictions through the command line, simply use the following command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/structure_analysis/PicoDet-L_layout_3cls.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_model/inference\" \\\n    -o Predict.input=\"layout.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it is <code>PicoDet-L_layout_3cls.yaml</code>)</p> </li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_model/inference\"</code></li> <li> <p>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX Common Model Configuration File Parameter Description.</p> </li> <li> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own project. To integrate, simply add the <code>model_dir=\"/output/best_model/inference\"</code> parameter to the <code>create_model(model_name=model_name, kernel_option=kernel_option)</code> function in the quick integration method from Step 3.</p> </li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/layout_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into PaddleX pipelines or into your own projects.</p> <ol> <li> <p>Pipeline Integration The structure analysis module can be integrated into PaddleX pipelines such as the General Table Recognition Pipeline and the Document Scene Information Extraction Pipeline v3 (PP-ChatOCRv3). Simply replace the model path to update the layout area localization module. In pipeline integration, you can use high-performance inference and service-oriented deployment to deploy your model.</p> </li> <li> <p>Module Integration The weights you produce can be directly integrated into the layout area localization module. You can refer to the Python example code in the Quick Integration section, simply replacing the model with the path to your trained model.</p> </li> </ol>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html","title":"Seal Text Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#i-overview","title":"I. Overview","text":"<p>The seal text detection module typically outputs multi-point bounding boxes around text regions, which are then passed as inputs to the distortion correction and text recognition modules for subsequent processing to identify the textual content of the seal. Recognizing seal text is an integral part of document processing and finds applications in various scenarios such as contract comparison, inventory access auditing, and invoice reimbursement verification. The seal text detection module serves as a subtask within OCR (Optical Character Recognition), responsible for locating and marking the regions containing seal text within an image. The performance of this module directly impacts the accuracy and efficiency of the entire seal text OCR system.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link Hmean\uff08%\uff09 GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Trained Model 98.21 84.341 2425.06 109 M The server-side seal text detection model of PP-OCRv4 boasts higher accuracy and is suitable for deployment on better-equipped servers. PP-OCRv4_mobile_seal_detInference Model/Trained Model 96.47 10.5878 131.813 4.6 M The mobile-side seal text detection model of PP-OCRv4, on the other hand, offers greater efficiency and is suitable for deployment on end devices. <p>Note: The evaluation set for the above accuracy metrics is a self-built dataset containing 500 circular seal images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>Just a few lines of code can complete the inference of the Seal Text Detection module, allowing you to easily switch between models under this module. You can also integrate the model inference of the the Seal Text Detection module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-OCRv4_server_seal_det\")\noutput = model.predict(\"seal_text_det.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy, you can leverage PaddleX's custom development capabilities to develop better Seal Text Detection models. Before developing a Seal Text Detection model with PaddleX, ensure you have installed PaddleOCR plugin for PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#41-dataset-preparation","title":"4.1 Dataset Preparation","text":"<p>Before model training, you need to prepare a dataset for the task. PaddleX provides data validation functionality for each module. Only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Text Detection and Recognition Task Module Data Preparation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_curve_det_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/ocr_curve_det_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Data validation can be completed with a single command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_curve_det_dataset_examples\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Once the command runs successfully, a message saying <code>Check dataset passed !</code> will be printed in the log. The verification results will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be stored in the <code>./output/check_dataset</code> directory, including visual examples of sample images and a histogram of sample distribution.</p> \ud83d\udc49 Verification Result Details (click to expand) <p>The specific content of the verification result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 606,\n    \"train_sample_paths\": [\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug07834.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug09943.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug04079.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug05701.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug08324.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug07451.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug09562.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug08237.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug01788.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug06481.png\"\n    ],\n    \"val_samples\": 152,\n    \"val_sample_paths\": [\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug03724.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug06456.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug04029.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug03603.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug05454.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug06269.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug00624.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug02818.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug00538.png\",\n      \"..\\/ocr_curve_det_dataset_examples\\/images\\/circle_Aug04935.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/ocr_curve_det_dataset_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"TextDetDataset\"\n}\n</code></pre> <p>The verification results above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 606;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 152;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of validation samples in this dataset;</li> </ul> <p>The dataset verification also analyzes the distribution of sample numbers across all classes and plots a histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"\ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> <p>(1) Dataset Format Conversion</p> <p>Seal text detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> fields in the configuration file. Example explanations for some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to enable re-splitting the dataset, set to <code>True</code> to perform dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set, which should be an integer between 0 and 100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_curve_det_dataset_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_curve_det_dataset_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the Seal Text Detection model (PP-OCRv4_server_seal_det) as an example:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ocr_curve_det_dataset_examples\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-OCRv4_server_seal_det.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to train using the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters Documentation. </p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, with the default path being <code>output</code>. To specify a different save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced, and static graph weights are used by default for model inference.</li> <li> <p>After model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, including whether the training task completed successfully, produced weight metrics, and related file paths.</p> </li> <li><code>train.log</code>: Training log file, recording model metric changes, loss changes, etc.</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameters used for this training session.</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, and static graph network structure.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After model training, you can evaluate the specified model weights on the validation set to verify model accuracy. Using PaddleX for model evaluation requires just one command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ocr_curve_det_dataset_examples\n</code></pre> <p>Similar to model training, follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-OCRv4_server_seal_det.yaml</code>).</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the validation dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For more details, refer to the PaddleX Common Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After model evaluation, the following outputs are typically produced:</p> <ul> <li><code>evaluate_result.json</code>: Records the evaluation results, specifically whether the evaluation task completed successfully and the model's evaluation metrics, including precision, recall and Hmean.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions via the command line, use the following command. Before running the following code, please download the demo image to your local machine.</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_accuracy/inference\" \\\n    -o Predict.input=\"seal_text_det.png\"\n</code></pre> <p>Similar to model training and evaluation, the following steps are required:</p> <ul> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>PP-OCRv4_server_seal_det.yaml</code>)</p> </li> <li> <p>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></p> </li> <li> <p>Specify the model weights path: -o Predict.model_dir=\"./output/best_accuracy/inference\"</p> </li> </ul> <p>Specify the input data path: <code>-o Predict.inputh=\"...\"</code> Other related parameters can be set by modifying the fields under Global and Predict in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</p> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/seal_text_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own projects.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The document Seal Text Detection module can be integrated into PaddleX pipelines such as the General OCR Pipeline and Document Scene Information Extraction Pipeline v3 (PP-ChatOCRv3). Simply replace the model path to update the text detection module of the relevant pipeline.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the Seal Text Detection module. You can refer to the Python sample code in Quick Integration and just replace the model with the path to the model you trained.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html","title":"Table Structure Recognition Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#i-overview","title":"I. Overview","text":"<p>Table structure recognition is a crucial component in table recognition systems, converting non-editable table images into editable table formats (e.g., HTML). The goal of table structure recognition is to identify the rows, columns, and cell positions of tables. The performance of this module directly impacts the accuracy and efficiency of the entire table recognition system. The module typically outputs HTML or LaTeX code for the table area, which is then passed to the table content recognition module for further processing.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description SLANetInference Model/Trained Model 59.52 522.536 1845.37 6.9 M SLANet is a table structure recognition model developed by Baidu PaddlePaddle Vision Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information. SLANet_plusInference Model/Trained Model 63.69 522.536 1845.37 6.9 M  SLANet_plus is an enhanced version of SLANet, a table structure recognition model developed by Baidu PaddlePaddle's Vision Team. Compared to SLANet, SLANet_plus significantly improves its recognition capabilities for wireless and complex tables, while reducing the model's sensitivity to the accuracy of table localization. Even when there are offsets in table localization, it can still perform relatively accurate recognition.  <p>Note: The above accuracy metrics are evaluated on a self-built English table recognition dataset by PaddleX. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Guide</p> <p>After installing the wheel package, a few lines of code can complete the inference of the table structure recognition module. You can easily switch models within this module and integrate the model inference into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"SLANet\")\noutput = model.predict(\"table_recognition.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better table structure recognition models. Before developing table structure recognition models with PaddleX, ensure you have installed the PaddleOCR plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Guide</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the corresponding dataset for the task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use a private dataset for model training, refer to PaddleX Table Structure Recognition Task Module Data Annotation Tutorial</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following command:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/table_rec_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/table_rec_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Run a single command to complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/table_rec_dataset_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Details of Validation Results (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 2000,\n    \"train_sample_paths\": [\n      \"../dataset/table_rec_dataset_examples/images/border_right_7384_X9UFEPKVMLALY7DDB11A.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_top_13708_VE2DGBD4DCQU2ITLBTEA.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_top_6490_14Z6ZN6G52GG4XA0K4XU.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_top_14236_DG96EX0EDKIIDK8P6ENG.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_19648_SV8B7X34RTYRAT2T5CPI.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_bottom_7186_HODBC25HISMCSVKY0HJ9.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/head_border_bottom_5773_4K4H9OVK9X9YVHE4Y1BQ.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_7760_8C62CCH5T57QUGE0NTHZ.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_bottom_15707_B1YVOU3X4NHHB6TL269O.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/no_border_5223_HLG406UK35UD5EUYC2AV.jpg\"\n    ],\n    \"val_samples\": 100,\n    \"val_sample_paths\": [\n      \"../dataset/table_rec_dataset_examples/images/border_2945_L7MSRHBZRW6Y347G39O6.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/head_border_bottom_4825_LH9WI6X104CP3VFXPSON.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/head_border_bottom_16837_79KHWU9WDM9ZQHNBGQAL.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_bottom_10107_9ENLLC29SQ6XI8WZY53E.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_top_16668_JIS0YFDZKTKETZIEKCKX.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_18653_J9SSKHLFTRJD4J8W17OW.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_bottom_8396_VJ3QJ3I0DP63P4JR77FE.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_9017_K2V7QBWSU2BA4R3AJSO7.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/border_top_19494_SDFMWP92NOB2OT7109FI.jpg\",\n      \"../dataset/table_rec_dataset_examples/images/no_border_288_6LK683JUCMOQ38V5BV29.jpg\"\n    ]\n  },\n  \"analysis\": {},\n  \"dataset_path\": \"./dataset/table_rec_dataset_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"PubTabTableRecDataset\"\n}\n</code></pre> <p>In the above validation results, <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 2000;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 100;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of samples in the validation set of this dataset.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Table structure recognition does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>The dataset splitting parameters can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. An example of part of the configuration file is shown below:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, ensuring the sum with <code>val_percent</code> equals 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/table_rec_dataset_examples\n</code></pre> <p>After the data splitting is executed, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/table_rec_dataset_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>A single command can complete the model training. Taking the training of the table structure recognition model SLANet as an example:</p> <p><pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/table_rec_dataset_examples\n</code></pre> the following steps are required:</p> <ul> <li>Specify the path of the model's <code>.yaml</code> configuration file (here it is <code>SLANet.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path of the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Train</code> in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to specify training on the first 2 GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the configuration file parameter instructions for the corresponding task module of the model PaddleX Common Model Configuration File Parameters.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command: <pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/table_rec_dataset_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>SLANet.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter to set it, such as <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including acc ;</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#441-model-inference","title":"4.4.1 Model Inference","text":"<ul> <li> <p>Inference predictions can be performed through the command line with just one command. Before running the following code, please download the demo image to your local machine. <pre><code>python main.py -c paddlex/configs/table_recognition/SLANet.yaml  \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_accuracy/inference\" \\\n    -o Predict.input=\"table_recognition.jpg\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> </li> <li> <p>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>SLANet.yaml</code>)</p> </li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_accuracy/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> <li>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/table_structure_recognition.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <p>1.Pipeline Integration</p> <p>The table structure recognition module can be integrated into PaddleX pipelines such as the General Table Recognition Pipeline and the Document Scene Information Extraction Pipeline v3 (PP-ChatOCRv3). Simply replace the model path to update the table structure recognition module in the relevant pipelines. For pipeline integration, you can deploy your obtained model using high-performance inference and service-oriented deployment.</p> <p>2.Module Integration</p> <p>The model weights you produce can be directly integrated into the table structure recognition module. Refer to the Python example code in Quick Integration , and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html","title":"Text Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#i-overview","title":"I. Overview","text":"<p>The text detection module is a crucial component in OCR (Optical Character Recognition) systems, responsible for locating and marking regions containing text within images. The performance of this module directly impacts the accuracy and efficiency of the entire OCR system. The text detection module typically outputs bounding boxes for text regions, which are then passed on to the text recognition module for further processing.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#ii-supported-models","title":"II. Supported Models","text":"ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_detInference Model/Trained Model 82.69 83.3501 2434.01 109 The server-side text detection model of PP-OCRv4, featuring higher accuracy and suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Trained Model 77.79 10.6923 120.177 4.7 The mobile text detection model of PP-OCRv4, optimized for efficiency and suitable for deployment on edge devices"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide.</p> <p>Just a few lines of code can complete the inference of the text detection module, allowing you to easily switch between models under this module. You can also integrate the model inference of the text detection module into your project. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-OCRv4_mobile_det\")\noutput = model.predict(\"general_ocr_001.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek even higher accuracy from existing models, you can leverage PaddleX's custom development capabilities to develop better text detection models. Before developing text detection models with PaddleX, ensure you have installed the PaddleOCR plugin for PaddleX. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare a dataset for the specific task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to the PaddleX Text Detection/Text Recognition Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_det_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/ocr_det_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_det_dataset_examples\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and gather basic information about it. Once the command runs successfully, <code>Check dataset passed !</code> will be printed in the log. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs will be stored in the <code>./output/check_dataset</code> directory in the current directory. The output directory includes sample images and histograms of sample distribution.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 200,\n    \"train_sample_paths\": [\n      \"../dataset/ocr_det_dataset_examples/images/train_img_61.jpg\",\n      \"../dataset/ocr_det_dataset_examples/images/train_img_289.jpg\"\n    ],\n    \"val_samples\": 50,\n    \"val_sample_paths\": [\n      \"../dataset/ocr_det_dataset_examples/images/val_img_61.jpg\",\n      \"../dataset/ocr_det_dataset_examples/images/val_img_137.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/ocr_det_dataset_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"TextDetDataset\"\n}\n</code></pre> <p>In the above validation result, <code>check_pass</code> being <code>true</code> indicates that the dataset format meets the requirements. The explanation of other metrics is as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in the dataset is 200;</li> <li><code>attributes.val_samples</code>: The number of validation samples in the dataset is 50;</li> <li><code>attributes.train_sample_paths</code>: List of relative paths for visualizing training sample images in the dataset;</li> <li><code>attributes.val_sample_paths</code>: List of relative paths for visualizing validation sample images in the dataset;</li> </ul> <p>Additionally, the dataset validation also analyzed the distribution of the length and width of all images in the dataset and plotted a distribution histogram (histogram.png):</p> <p></p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Text detection does not support data format conversion.</p> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> section in the configuration file. Below are some example explanations for the parameters in the configuration file:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, and the sum with <code>val_percent</code> must be 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_det_dataset_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command-line arguments:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_det_dataset_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command. Here's an example of training the PP-OCRv4 mobile text detection model (<code>PP-OCRv4_mobile_det</code>):</p> <p><pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ocr_det_dataset_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>PP-OCRv4_mobile_det.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file or adjusted by appending parameters in the command line. For example, to specify training on the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration Parameters Documentation.</li> </ul> \ud83d\udc49 More Information (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ocr_det_dataset_examples\n</code></pre> <p>Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (in this case, <code>PP-OCRv4_mobile_det.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Evaluate</code> in the <code>.yaml</code> configuration file. For details, please refer to PaddleX General Model Configuration File Parameter Instructions.</p> \ud83d\udc49 More Instructions (Click to Expand) <p>During model evaluation, you need to specify the path to the model weight file. Each configuration file has a built-in default weight save path. If you need to change it, you can set it by adding a command line argument, such as <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> will be generated, which records the evaluation results. Specifically, it records whether the evaluation task was completed successfully and the model's evaluation metrics, including <code>precision</code>, <code>recall</code>, and <code>hmean</code>.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions via the command line, simply use the following command. Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_mobile_det.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_accuracy/inference\" \\\n    -o Predict.input=\"general_ocr_001.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>PP-OCRv4_mobile_det.yaml</code>)</li> <li>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_accuracy/inference\"</code></li> <li> <p>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</p> </li> <li> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</p> </li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/text_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>Models can be directly integrated into PaddleX pipelines or into your own projects.</p> <p>1.Pipeline Integration</p> <p>The text detection module can be integrated into PaddleX pipelines such as the General OCR Pipeline, Table Recognition Pipeline, and PP-ChatOCRv3-doc. Simply replace the model path to update the text detection module of the relevant pipeline.</p> <p>2.Module Integration</p> <p>The model weights you produce can be directly integrated into the text detection module. Refer to the Python example code in Quick Integration, and simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_image_unwarping.html","title":"Text Image Unwarping Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/text_image_unwarping.html#i-overview","title":"I. Overview","text":"<p>The primary purpose of Text Image Unwarping is to perform geometric transformations on images in order to correct issues such as document distortion, tilt, perspective deformation, etc., enabling more accurate recognition by subsequent text recognition modules.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_image_unwarping.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link MS-SSIM \uff08%\uff09 Model Size (M) information UVDocInference Model/Trained Model 54.40 30.3 M High-precision Text Image Unwarping Model <p>The accuracy metrics of the above models are measured on the DocUNet benchmark dataset.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_image_unwarping.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>Just a few lines of code can complete the inference of the Text Image Unwarping module, allowing you to easily switch between models under this module. You can also integrate the model inference of the the Text Image Unwarping module into your project.</p> <p>Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"UVDoc\")\noutput = model.predict(\"doc_test.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_image_unwarping.html#iv-custom-development","title":"IV. Custom Development","text":"<p>The current module temporarily does not support fine-tuning training and only supports inference integration. Fine-tuning training for this module is planned to be supported in the future.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html","title":"Text Recognition Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#i-overview","title":"I. Overview","text":"<p>The text recognition module is the core component of an OCR (Optical Character Recognition) system, responsible for extracting text information from text regions within images. The performance of this module directly impacts the accuracy and efficiency of the entire OCR system. The text recognition module typically receives bounding boxes of text regions output by the text detection module as input. Through complex image processing and deep learning algorithms, it converts the text in images into editable and searchable electronic text. The accuracy of text recognition results is crucial for subsequent applications such as information extraction and data mining.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#ii-supported-model-list","title":"II. Supported Model List","text":"ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4, developed by Baidu's PaddlePaddle Vision Team, is the next version of the PP-OCRv3 text recognition model. By introducing data augmentation schemes, GTC-NRTR guidance branches, and other strategies, it further improves text recognition accuracy without compromising model inference speed. The model offers both server and mobile versions to meet industrial needs in different scenarios. PP-OCRv4_server_rec Inference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more, with 1.1w images for text recognition. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>\u2757 The above list features the 2 core models that the image classification module primarily supports. In total, this module supports 4 models. The complete list of models is as follows:</p>  \ud83d\udc49Model List Details ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4, developed by Baidu's PaddlePaddle Vision Team, is the next version of the PP-OCRv3 text recognition model. By introducing data augmentation schemes, GTC-NRTR guidance branches, and other strategies, it further improves text recognition accuracy without compromising model inference speed. The model offers both server and mobile versions to meet industrial needs in different scenarios. PP-OCRv4_server_rec Inference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more, with 1.1w images for text recognition. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time Model Size (M) Description ch_SVTRv2_recInference Model/Trained Model 68.81 8.36801 165.706 73.9 M SVTRv2, a server-side text recognition model developed by the OpenOCR team at the Vision and Learning Lab (FVL) of Fudan University, also won first place in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge. Its A-rank end-to-end recognition accuracy is 6% higher than PP-OCRv4.  <p>Note: The evaluation set for the above accuracy metrics is the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge - Track 1 A-rank. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time Model Size (M) Description ch_RepSVTR_recInference Model/Trained Model 65.07 10.5047 51.5647 22.1 M   RepSVTR, a mobile text recognition model based on SVTRv2, won first place in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge. Its B-rank end-to-end recognition accuracy is 2.5% higher than PP-OCRv4, with comparable inference speed. <p>Note: The evaluation set for the above accuracy metrics is the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge - Track 1 B-rank. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>Before quick integration, you need to install the PaddleX wheel package. For the installation method, please refer to the PaddleX Local Installation Tutorial. After installing the wheel package, a few lines of code can complete the inference of the text recognition module. You can switch models under this module freely, and you can also integrate the model inference of the text recognition module into your project.</p> <p>Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"PP-OCRv4_mobile_rec\")\noutput = model.predict(\"general_ocr_rec_001.png\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, please refer to the PaddleX Single-Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you are seeking higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better  text recognition models. Before using PaddleX to develop text recognition models, please ensure that you have installed the relevant model training plugins for OCR in PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, it is necessary to prepare the corresponding dataset for each task module. PaddleX provides a data validation function for each module, and only data that passes the validation can be used for model training. Additionally, PaddleX offers Demo datasets for each module, allowing you to complete subsequent development based on the officially provided Demo data. If you wish to use a private dataset for subsequent model training, you can refer to the PaddleX Text Detection/Text Recognition Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#411-download-demo-data","title":"4.1.1 Download Demo Data","text":"<p>You can use the following commands to download the Demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ocr_rec_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/ocr_rec_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>A single command can complete data validation:</p> <p><pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_dataset_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset and summarize its basic information. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log. The validation results file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the <code>./output/check_dataset</code> directory in the current directory, including visual examples of sample images and sample distribution histograms.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 4468,\n    \"train_sample_paths\": [\n      \"../dataset/ocr_rec_dataset_examples/images/train_word_1.png\",\n      \"../dataset/ocr_rec_dataset_examples/images/train_word_10.png\"\n    ],\n    \"val_samples\": 2077,\n    \"val_sample_paths\": [\n      \"../dataset/ocr_rec_dataset_examples/images/val_word_1.png\",\n      \"../dataset/ocr_rec_dataset_examples/images/val_word_10.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/ocr_rec_dataset_examples\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"MSTextRecDataset\"\n}\n</code></pre> <p>In the above validation result, <code>check_pass</code> being <code>true</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 4468;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 2077;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized training set samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized validation set samples in this dataset; Additionally, the dataset validation also analyzes the distribution of character length ratios in the dataset and generates a distribution histogram (histogram.png):</li> </ul> <p></p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format or re-split the training/validation ratio of the dataset by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Dataset Format Conversion/Dataset Splitting Details (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Text recognition does not currently support data conversion.</p> <p>(2) Dataset Splitting</p> <p>The parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> section in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set. The type is any integer between 0-100, and it must sum up to 100 with <code>val_percent</code>; For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</li> </ul> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_dataset_examples\n</code></pre> <p>After data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ocr_rec_dataset_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with a single command. Here's an example of training the PP-OCRv4 mobile text recognition model (PP-OCRv4_mobile_rec):</p> <p><pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ocr_rec_dataset_examples\n</code></pre> The steps required are:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>PP-OCRv4_mobile_rec.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU))</li> <li>Specify the mode as model training: <code>-o Global.mode=train</code></li> <li>Specify the path to the training dataset: <code>-o Global.dataset_dir</code>. Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file or adjusted by appending parameters in the command line. For example, to specify training on the first 2 GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX Common Configuration File Parameters.</li> </ul> \ud83d\udc49 More Information (Click to Expand) <ul> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can set it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</li> <li> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, related file paths, etc.;</p> </li> <li><code>train.log</code>: Training log file, recording changes in model metrics and loss during training;</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li><code>.pdparams</code>, <code>.pdema</code>, <code>.pdopt.pdstate</code>, <code>.pdiparams</code>, <code>.pdmodel</code>: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>```bash\npython main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ocr_rec_dataset_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>PP-OCRv4_mobile_rec.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Information (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter to set it, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After completing the model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully and the model's evaluation metrics, including  acc\u3001norm_edit_dis\uff1b</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction via the command line, simply use the following command:</p> <p>Before running the following code, please download the demo image to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/best_accuracy/inference\" \\\n    -o Predict.input=\"general_ocr_rec_001.png\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it is <code>PP-OCRv4_mobile_rec.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_accuracy/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/ocr_modules/text_recognition.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>Models can be directly integrated into the PaddleX pipelines or into your own projects.</p> <p>1.Pipeline Integration</p> <p>The text recognition module can be integrated into PaddleX pipelines such as the General OCR Pipeline, General Table Recognition Pipeline, and Document Scene Information Extraction Pipeline v3 (PP-ChatOCRv3). Simply replace the model path to update the text recognition module of the relevant pipeline.</p> <p>2.Module Integration</p> <p>The weights you produce can be directly integrated into the text recognition module. Refer to the Quick Integration Python example code. Simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html","title":"Time Series Anomaly Detection Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#i-overview","title":"I. Overview","text":"<p>Time series anomaly detection focuses on identifying abnormal points or periods in time series data that do not conform to expected patterns, trends, or periodic regularities. These anomalies can be caused by system failures, external shocks, data entry errors, or rare events, and are of great significance for timely response, risk assessment, and business decision-making.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link Precision Recall F1-Score Model Size (M) Description AutoEncoder_ad_adInference Model/Trained Model 0.9898 0.9396 0.9641 72.8K AutoEncoder_ad_ad is a simple, efficient, and easy-to-use time series anomaly detection model Nonstationary_adInference Model/Trained Model 0.9855 0.8895 0.9351 1.5MB Based on the transformer structure, optimized for anomaly detection in non-stationary time series AutoEncoder_adInference Model/Trained Model 0.9936 0.8436 0.9125 32K AutoEncoder_ad is a classic autoencoder-based, efficient, and easy-to-use time series anomaly detection model PatchTST_adInference Model/Trained Model 0.9878 0.9070 0.9457 164K PatchTST is a high-precision time series anomaly detection model that balances local patterns and global dependencies TimesNet_adInference Model/Trained Model 0.9837 0.9480 0.9656 732K Through multi-period analysis, TimesNet is an adaptive and high-precision time series anomaly detection model <p>Note: The above accuracy metrics are measured on the PSM dataset with a time series length of 100.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For details, refer to the PaddleX Local Installation Guide</p> <p>After installing the wheel package, a few lines of code can complete the inference of the time series anomaly detection module. You can switch models under this module freely, and you can also integrate the model inference of the time series anomaly detection module into your project. Before running the following code, please download the demo csv to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"AutoEncoder_ad\")\noutput = model.predict(\"ts_ad.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre> For more information on using PaddleX's single model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy from existing models, you can use PaddleX's custom development capabilities to develop better time series anomaly detection models. Before developing time series anomaly models with PaddleX, please ensure that the PaddleTS plugin is installed. The installation process can be found in the PaddleX Local Installation Guide.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the dataset for the corresponding task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for subsequent model training, refer to the PaddleX Time Series Anomaly Detection Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following command to download the demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ts_anomaly_examples.tar -P ./dataset\ntar -xf ./dataset/ts_anomaly_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>You can complete data validation with a single command: <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset, summarize its basic information, and print <code>Check dataset passed !</code> in the log if the command runs successfully. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the current directory's <code>./output/check_dataset</code> directory, including example time series data.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 22032,\n    \"train_table\": [\n      [\n        \"timestamp\",\n        \"feature_0\",\n        \"...\",\n        \"feature_24\",\n        \"label\"\n      ],\n      [\n        0.0,\n        0.7326893750079723,\n        \"...\",\n        0.1382488479262673,\n        0.0\n      ]\n    ],\n    \"val_samples\": 198290,\n    \"val_table\": [\n      [\n        \"timestamp\",\n        \"feature_0\",\n        \"...\",\n        \"feature_24\",\n        \"label\"\n      ],\n      [\n        22032.0,\n        0.8604795809835284,\n        \"...\",\n        0.1428571428571428,\n        0.0\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"\"\n  },\n  \"dataset_path\": \"./dataset/ts_anomaly_examples\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSADDataset\"\n}\n</code></pre> <p>The verification results above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 22032;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 198290;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the top 10 rows of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the top 10 rows of validation samples in this dataset. Note: Only data that has passed validation can be used for training and evaluation.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#413-dataset-format-conversion-dataset-splitting-optional","title":"4.1.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>After completing the data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details of Format Conversion / Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Time series anomaly detection supports converting <code>xlsx</code> and <code>xls</code> format datasets to <code>csv</code> format.</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to convert the dataset format, supporting <code>xlsx</code> and <code>xls</code> formats to <code>CSV</code> format, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format does not need to be set, default is <code>null</code>;</li> </ul> <p>To enable format conversion, modify the configuration as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: null\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples\n</code></pre> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples \\\n    -o CheckDataset.convert.enable=True\n</code></pre> <p>(2) Dataset Splitting</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to convert the dataset format, <code>True</code> to enable dataset format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, time series anomaly detection only supports converting xlsx annotation files to csv, the source dataset format does not need to be set, default is <code>null</code>;</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset, <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set, an integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If re-splitting the dataset, set the percentage of the validation set, an integer between 0-100, ensuring the sum with <code>train_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with 90% training set and 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the Time Series Forecasting model (AutoEncoder_ad) as an example:</p> <pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>AutoEncoder_ad.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to train using the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX TS Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, with the default path being <code>output</code>. To specify a different save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced, and static graph weights are used by default for model inference.</li> <li> <p>After model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, including whether the training task completed successfully, produced weight metrics, and related file paths.</p> </li> <li><code>train.log</code>: Training log file, recording model metric changes, loss changes, etc.</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameters used for this training session.</li> <li><code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference</code>: Model weight-related files, including Model weight-related files, including network parameters, optimizers, and network architecture.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ts_anomaly_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>AutoEncoder_ad.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, the following outputs are typically generated:</p> <p>Upon completion of model evaluation, an <code>evaluate_result.json</code> file will be produced, which records the evaluation results, specifically indicating whether the evaluation task was completed successfully and the model's evaluation metrics, including <code>f1</code>, <code>recall</code>, and <code>precision</code>.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions through the command line, simply use the following command:</p> <p>Before running the following code, please download the demo csv to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/AutoEncoder_ad.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=\"ts_ad.csv\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>AutoEncoder_ad.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or directly into your own project.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The time series prediction module can be integrated into PaddleX pipelines such as Time Series Anomaly Detection. Simply replace the model path to update the time series prediction model. In pipeline integration, you can use service deployment to deploy your obtained model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the time series anomaly detection module. Refer to the Python example code in Quick Integration, simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html","title":"Time Series Classification Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#i-overview","title":"I. Overview","text":"<p>Time series classification involves identifying and categorizing different patterns in time series data by analyzing trends, periodicity, seasonality, and other factors that vary over time. This technique is widely used in medical diagnosis and other fields, effectively classifying key information in time series data to provide robust support for decision-making.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link Acc(%) Model Size (M) Description TimesNet_clsInference Model/Trained Model 87.5 792K TimesNet is an adaptive and high-accuracy time series classification model through multi-period analysis <p>Note: The evaluation set for the above accuracy metrics is UWaveGestureLibrary.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to PaddleX Local Installation Guide</p> <p>After installing the wheel package, you can perform inference for the time series classification module with just a few lines of code. You can switch models under this module freely, and you can also integrate the model inference of the time series classification module into your project. Before running the following code, please download the demo csv to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"TimesNet_cls\")\noutput = model.predict(\"ts_cls.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre> For more information on using PaddleX's single-model inference APIs, refer to PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you aim for higher accuracy with existing models, you can leverage PaddleX's custom development capabilities to develop better time series classification models. Before using PaddleX to develop time series classification models, ensure you have installed the PaddleTS plugin. Refer to the PaddleX Local Installation Guide for the installation process.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>Before model training, you need to prepare the dataset for the corresponding task module. PaddleX provides data validation functionality for each module, and only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for subsequent model training, refer to PaddleX Time Series Classification Task Module Data Annotation Tutorial.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ts_classify_examples.tar -P ./dataset\ntar -xf ./dataset/ts_classify_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>You can complete data validation with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset, summarize its basic information, and print <code>Check dataset passed !</code> in the log if the command runs successfully. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the current directory's <code>./output/check_dataset</code> directory, including example time series data and class distribution histograms.</p> \ud83d\udc49 Validation Result Details (Click to Expand) <p>The specific content of the validation result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 82620,\n    \"train_table\": [\n      [\n        \"Unnamed: 0\",\n        \"group_id\",\n        \"dim_0\",\n        ...,\n        \"dim_60\",\n        \"label\",\n        \"time\"\n      ],\n      [\n        0.0,\n        0.0,\n        0.000949,\n        ...,\n        0.12107,\n        1.0,\n        0.0\n      ]\n    ],\n    \"val_samples\": 83025,\n    \"val_table\": [\n      [\n        \"Unnamed: 0\",\n        \"group_id\",\n        \"dim_0\",\n        ...,\n        \"dim_60\",\n        \"label\",\n        \"time\"\n      ],\n      [\n        0.0,\n        0.0,\n        0.004578,\n        ...,\n        0.15728,\n        1.0,\n        0.0\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/ts_classify_examples\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSCLSDataset\"\n}\n</code></pre> <p>The verification results above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 12194;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 3484;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the top 10 rows of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the top 10 rows of validation samples in this dataset;</li> </ul> <p>Furthermore, the dataset validation also involved an analysis of the distribution of sample numbers across all categories within the dataset, and a distribution histogram (histogram.png) was generated accordingly.</p> <p></p> <p>Note: Only data that has passed validation can be used for training and evaluation.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#413-dataset-format-conversiondataset-splitting-optional","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>After completing data validation, you can convert the dataset format and re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> \ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>(1) Dataset Format Conversion</p> <p>Time-series classification supports converting <code>xlsx</code> and <code>xls</code> format datasets to <code>csv</code> format.</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion, supporting conversion from <code>xlsx</code> and <code>xls</code> formats to <code>CSV</code> format, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format does not need to be set, default is <code>null</code>;</li> </ul> <p>To enable format conversion, modify the configuration as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: null\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> <p>The above parameters can also be set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples \\\n    -o CheckDataset.convert.enable=True\n</code></pre> <p>(2) Dataset Splitting</p> <p>Parameters related to dataset validation can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to perform dataset format conversion, <code>True</code> to enable, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, time-series classification only supports converting xlsx annotation files to csv, the source dataset format does not need to be set, default is <code>null</code>;</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to re-split the dataset, <code>True</code> to enable, default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is re-split, the percentage of the training set needs to be set, an integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If the dataset is re-split, the percentage of the validation set needs to be set, an integer between 0-100, ensuring the sum with <code>train_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters can also be set by appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the Time Series Forecasting model (TimesNet_cls) as an example:</p> <pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>TimesNet_cls.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to train using the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX TS Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, with the default path being <code>output</code>. To specify a different save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced, and static graph weights are used by default for model inference.</li> <li> <p>After model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, including whether the training task completed successfully, produced weight metrics, and related file paths.</p> </li> <li><code>train.log</code>: Training log file, recording model metric changes, loss changes, etc.</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameters used for this training session.</li> <li><code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference</code>: Model weight-related files, including Model weight-related files, including network parameters, optimizers, and network architecture.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation can be done with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>TimesNet_cls.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other relevant parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, typically, the following outputs are generated:</p> <p>Upon completion of model evaluation, an <code>evaluate_result.json</code> file is produced, which records the evaluation results, specifically whether the evaluation task was completed successfully and the model's evaluation metrics, including Top-1 Accuracy and F1 score.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#44-model-inference-and-model-integration","title":"4.4 Model Inference and Model Integration","text":"<p>After completing model training and evaluation, you can use the trained model weights for inference prediction or Python integration.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference prediction via the command line, simply use the following command:</p> <p>Before running the following code, please download the demo csv to your local machine.</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=\"ts_cls.csv\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>TimesNet_cls.yaml</code> - Note: This should likely be <code>TimesNet_cls.yaml</code> for consistency)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the model weights path: <code>-o Predict.model_dir=\"./output/inference\"</code></li> <li>Specify the input data path: <code>-o Predict.input=\"...\"</code> Other relevant parameters can be set by modifying the <code>Global</code> and <code>Predict</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_classification.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>Models can be directly integrated into the PaddleX pipeline or directly into your own projects.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The time series prediction module can be integrated into PaddleX pipelines such as Time Series Classification. Simply replace the model path to update the time series prediction model. In pipeline integration, you can use service deployment to deploy your trained model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the time series classification module. Refer to the Python example code in Quick Integration (Note: This section header is in Chinese and should be translated or removed for consistency), simply replace the model with the path to your trained model.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html","title":"Time Series Forecasting Module Development Tutorial","text":""},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#i-overview","title":"I. Overview","text":"<p>Time series forecasting aims to predict the possible values or states at a future point in time or within a future time period by analyzing patterns, trends, periodicity, and other characteristics in historical data. This helps enterprises and organizations make more accurate decisions, optimize resource allocation, reduce risks, and seize potential market opportunities. These time series data typically originate from various sensors, economic activities, social behaviors, and other real-world application scenarios. For example, stock prices, temperature changes, website traffic, sales data, and the like are all typical examples of time series data.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#ii-supported-model-list","title":"II. Supported Model List","text":"Model NameModel Download Link mse mae Model Size (M) Introduce DLinearInference Model/Trained Model 0.382 0.394 76k Simple structure, high efficiency and easy-to-use time series prediction model NonstationaryInference Model/Trained Model 0.600 0.515 60.3M Based on the transformer structure, targeted optimization of long-term time series prediction models for non-stationary time series PatchTSTInference Model/Trained Model 0.385 0.397 2.2M High-precision long-term time series prediction model that takes into account both local patterns and global dependencies TiDEInference Model/Trained Model 0.405 0.412 34.9M High-precision model suitable for handling multivariate, long-term time series prediction problems TimesNetInference Model/Trained Model 0.417 0.431 5.2M Through multi-period analysis, TimesNet is a highly adaptable high-precision time series analysis model <p>Note: The above accuracy metrics are measured on the ETTH1 test dataset, with an input sequence length of 96, and a prediction sequence length of 96 for all models except TiDE, which has a prediction sequence length of 720.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#iii-quick-integration","title":"III. Quick Integration","text":"<p>\u2757 Before quick integration, please install the PaddleX wheel package. For detailed instructions, refer to the PaddleX Local Installation Guide</p> <p>Just a few lines of code can complete the inference of the Time Series Forecasting module, allowing you to easily switch between models under this module. You can also integrate the model inference of the the Time Series Forecasting module into your project. Before running the following code, please download the demo csv to your local machine.</p> <p><pre><code>from paddlex import create_model\nmodel = create_model(\"DLinear\")\noutput = model.predict(\"ts_fc.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre> For more information on using PaddleX's single-model inference API, refer to the PaddleX Single Model Python Script Usage Instructions.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#iv-custom-development","title":"IV. Custom Development","text":"<p>If you seek higher accuracy, you can leverage PaddleX's custom development capabilities to develop better Time Series Forecasting models. Before developing a Time Series Forecasting model with PaddleX, ensure you have installed PaddleClas plugin for PaddleX. The installation process can be found in the custom development section of the PaddleX Local Installation Tutorial.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#41-dataset-preparation","title":"4.1 Dataset Preparation","text":"<p>Before model training, you need to prepare a dataset for the task. PaddleX provides data validation functionality for each module. Only data that passes validation can be used for model training. Additionally, PaddleX provides demo datasets for each module, which you can use to complete subsequent development. If you wish to use private datasets for model training, refer to PaddleX Time Series Forecasting Task Module Data Preparation Tutorial.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#411-demo-data-download","title":"4.1.1 Demo Data Download","text":"<p>You can download the demo dataset to a specified folder using the following commands:</p> <pre><code>wget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ts_dataset_examples.tar -P ./dataset\ntar -xf ./dataset/ts_dataset_examples.tar -C ./dataset/\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#412-data-validation","title":"4.1.2 Data Validation","text":"<p>Data validation can be completed with a single command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_dataset_examples\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Once the command runs successfully, a message saying <code>Check dataset passed !</code> will be printed in the log. The verification results will be saved in <code>./output/check_dataset_result.json</code>, and related outputs will be stored in the <code>./output/check_dataset</code> directory, including visual examples of sample images and a histogram of sample distribution.</p> \ud83d\udc49 Verification Result Details (click to expand) <p>The specific content of the verification result file is:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 12194,\n    \"train_table\": [\n      [\n        \"date\",\n        \"HUFL\",\n        \"HULL\",\n        \"MUFL\",\n        \"MULL\",\n        \"LUFL\",\n        \"LULL\",\n        \"OT\"\n      ],\n      [\n        \"2016-07-01 00:00:00\",\n        5.827000141143799,\n        2.009000062942505,\n        1.5989999771118164,\n        0.4620000123977661,\n        4.203000068664552,\n        1.3400000333786009,\n        30.5310001373291\n      ],\n      [\n        \"2016-07-01 01:00:00\",\n        5.692999839782715,\n        2.075999975204468,\n        1.4919999837875366,\n        0.4259999990463257,\n        4.142000198364259,\n        1.371000051498413,\n        27.78700065612793\n      ]\n    ],\n    \"val_samples\": 3484,\n    \"val_table\": [\n      [\n        \"date\",\n        \"HUFL\",\n        \"HULL\",\n        \"MUFL\",\n        \"MULL\",\n        \"LUFL\",\n        \"LULL\",\n        \"OT\"\n      ],\n      [\n        \"2017-11-21 02:00:00\",\n        12.994000434875488,\n        4.889999866485597,\n        10.055999755859377,\n        2.878000020980835,\n        2.559000015258789,\n        1.2489999532699585,\n        4.7129998207092285\n      ],\n      [\n        \"2017-11-21 03:00:00\",\n        11.92199993133545,\n        4.554999828338623,\n        9.097000122070312,\n        3.0920000076293945,\n        2.559000015258789,\n        1.2790000438690186,\n        4.8540000915527335\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"\"\n  },\n  \"dataset_path\": \"./dataset/ts_dataset_examples\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSDataset\"\n}\n</code></pre> <p>The verification results above indicate that <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 12194;</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 3484;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the top 10 rows of training samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the top 10 rows of validation samples in this dataset;</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#413-dataset-format-conversiondataset-splitting-optional-click-to-expand","title":"4.1.3 Dataset Format Conversion/Dataset Splitting (Optional) (Click to Expand)","text":"\ud83d\udc49 Details on Format Conversion/Dataset Splitting (Click to Expand) <p>After completing dataset verification, you can convert the dataset format or re-split the training/validation ratio by modifying the configuration file or appending hyperparameters.</p> <p>(1) Dataset Format Conversion</p> <p>Time Series Forecasting supports converting <code>xlsx</code> and <code>xls</code> format datasets to the required format.</p> <p>Parameters related to dataset verification can be set by modifying the <code>CheckDataset</code> fields in the configuration file. Example explanations for some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>convert</code>:</li> <li><code>enable</code>: Whether to enable dataset format conversion, supporting <code>xlsx</code> and <code>xls</code> format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, the source dataset format needs to be set, default is <code>null</code>.</li> </ul> <p>Modify the <code>paddlex/configs/ts_forecast/DLinear.yaml</code> configuration as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  convert:\n    enable: True\n    src_dataset_type: null\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_forecast_to_convert\n</code></pre> <p>Of course, the above parameters also support being set by appending command-line arguments. For a <code>LabelMe</code> format dataset, the command is:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_forecast_to_convert \\\n    -o CheckDataset.convert.enable=True \\\n</code></pre> <p>(2) Dataset Splitting</p> <p>Parameters for dataset splitting can be set by modifying the <code>CheckDataset</code> fields in the configuration file. Example explanations for some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:</li> <li><code>split</code>:</li> <li><code>enable</code>: Whether to enable re-splitting the dataset, set to <code>True</code> to perform dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, set the percentage of the training set, which should be an integer between 0 and 100, ensuring the sum with <code>val_percent</code> is 100;</li> </ul> <p>For example, if you want to re-split the dataset with a 90% training set and a 10% validation set, modify the configuration file as follows:</p> <pre><code>......\nCheckDataset:\n  ......\n  split:\n    enable: True\n    train_percent: 90\n    val_percent: 10\n  ......\n</code></pre> <p>Then execute the command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_dataset_examples\n</code></pre> <p>After dataset splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path.</p> <p>The above parameters also support setting through appending command line arguments:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml  \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_dataset_examples \\\n    -o CheckDataset.split.enable=True \\\n    -o CheckDataset.split.train_percent=90 \\\n    -o CheckDataset.split.val_percent=10\n</code></pre>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#42-model-training","title":"4.2 Model Training","text":"<p>Model training can be completed with just one command. Here, we use the Time Series Forecasting model (DLinear) as an example:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ts_dataset_examples\n</code></pre> <p>You need to follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>DLinear.yaml</code>,When training other models, you need to specify the corresponding configuration files. The relationship between the model and configuration files can be found in the PaddleX Model List (CPU/GPU)).</li> <li>Set the mode to model training: <code>-o Global.mode=train</code></li> <li>Specify the training dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Train</code> fields in the <code>.yaml</code> configuration file, or adjusted by appending parameters in the command line. For example, to train using the first two GPUs: <code>-o Global.device=gpu:0,1</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. For more modifiable parameters and their detailed explanations, refer to the PaddleX TS Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <ul> <li>During model training, PaddleX automatically saves model weight files, with the default path being <code>output</code>. To specify a different save path, use the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced, and static graph weights are used by default for model inference.</li> <li> <p>After model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> </li> <li> <p><code>train_result.json</code>: Training result record file, including whether the training task completed successfully, produced weight metrics, and related file paths.</p> </li> <li><code>train.log</code>: Training log file, recording model metric changes, loss changes, etc.</li> <li><code>config.yaml</code>: Training configuration file, recording the hyperparameters used for this training session.</li> <li><code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference</code>: Model weight-related files, including Model weight-related files, including network parameters, optimizers, and network architecture.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#43-model-evaluation","title":"4.3 Model Evaluation","text":"<p>After model training, you can evaluate the specified model weights on the validation set to verify model accuracy. Using PaddleX for model evaluation requires just one command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ts_dataset_examples\n</code></pre> <p>Similar to model training, follow these steps:</p> <ul> <li>Specify the <code>.yaml</code> configuration file path for the model (here it's <code>DLinear.yaml</code>).</li> <li>Set the mode to model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the validation dataset path: <code>-o Global.dataset_dir</code></li> </ul> <p>Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For more details, refer to the PaddleX TS Configuration Parameters Documentation.</p> \ud83d\udc49 More Details (Click to Expand) <p>When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply append the command line parameter, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p> <p>After model evaluation, the following outputs are typically produced:</p> <ul> <li><code>evaluate_result.json</code>: Records the evaluation results, specifically whether the evaluation task completed successfully and the model's evaluation metrics, including <code>mse</code> and <code>mae</code>.</li> </ul>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#44-model-inference-and-integration","title":"4.4 Model Inference and Integration","text":"<p>After model training and evaluation, you can use the trained model weights for inference predictions or Python integration.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#441-model-inference","title":"4.4.1 Model Inference","text":"<p>To perform inference predictions via the command line, use the following command:</p> <p>Before running the following code, please download the demo csv to your local machine.</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=\"ts_fc.csv\"\n</code></pre> <p>Similar to model training and evaluation, the following steps are required:</p> <ul> <li> <p>Specify the <code>.yaml</code> configuration file path of the model (here it's <code>DLinear.yaml</code>)</p> </li> <li> <p>Set the mode to model inference prediction: <code>-o Global.mode=predict</code></p> </li> <li> <p>Specify the model weights path: <code>-o Predict.model_dir=\"./output/best_accuracy/inference\"</code></p> </li> </ul> <p>Specify the input data path: <code>-o Predict.inputh=\"...\"</code> Other related parameters can be set by modifying the fields under Global and Predict in the <code>.yaml</code> configuration file. For details, refer to PaddleX Common Model Configuration File Parameter Description.</p> <p>Alternatively, you can use the PaddleX wheel package for inference, easily integrating the model into your own projects.</p>"},{"location":"en/module_usage/tutorials/time_series_modules/time_series_forecasting.html#442-model-integration","title":"4.4.2 Model Integration","text":"<p>The model can be directly integrated into the PaddleX pipeline or into your own projects.</p> <ol> <li>Pipeline Integration</li> </ol> <p>The Time Series Forecasting module can be integrated into PaddleX pipelines such as the Time Series Forecasting Pipeline (ts_fc). Simply replace the model path to update the Time Series Forecasting module's model.</p> <ol> <li>Module Integration</li> </ol> <p>The weights you produce can be directly integrated into the Time Series Forecasting module. You can refer to the Python sample code in Quick Integration and just replace the model with the path to the model you trained.</p>"},{"location":"en/other_devices_support/multi_devices_use_guide.html","title":"PaddleX Multi-Hardware Usage Guide","text":"<p>This document focuses on the usage guide of PaddleX for Huawei Ascend NPU, Cambricon MLU, Kunlun XPU, and Hygon DCU hardware platforms.</p>"},{"location":"en/other_devices_support/multi_devices_use_guide.html#1-installation","title":"1. Installation","text":""},{"location":"en/other_devices_support/multi_devices_use_guide.html#11-paddlepaddle-installation","title":"1.1 PaddlePaddle Installation","text":"<p>First, please complete the installation of PaddlePaddle according to your hardware platform. The installation tutorials for each hardware are as follows:</p> <p>Ascend NPU: Ascend NPU PaddlePaddle Installation Guide</p> <p>Cambricon MLU: Cambricon MLU PaddlePaddle Installation Guide</p> <p>Kunlun XPU: Kunlun XPU PaddlePaddle Installation Guide</p> <p>Hygon DCU: Hygon DCU PaddlePaddle Installation Guide</p>"},{"location":"en/other_devices_support/multi_devices_use_guide.html#12-paddlex-installation","title":"1.2 PaddleX Installation","text":"<p>Welcome to use PaddlePaddle's low-code development tool, PaddleX. Before we officially start the local installation, please clarify your development needs and choose the appropriate installation mode based on your requirements.</p> <p>PaddleX offers two installation modes: Wheel Package Installation and Plugin Installation. The following details the application scenarios and installation methods for these two modes.</p>"},{"location":"en/other_devices_support/multi_devices_use_guide.html#121-wheel-package-installation-mode","title":"1.2.1 Wheel Package Installation Mode","text":"<p>If your application scenario for PaddleX is model inference and integration, we recommend using the more convenient and lightweight Wheel Package Installation Mode.</p> <p>After installing PaddlePaddle, you can directly execute the following commands to quickly install the PaddleX Wheel package:</p> <pre><code>pip install https://paddle-model-ecology.bj.bcebos.com/paddlex/whl/paddlex-3.0.0b1-py3-none-any.whl\n</code></pre>"},{"location":"en/other_devices_support/multi_devices_use_guide.html#122-plugin-installation-mode","title":"1.2.2 Plugin Installation Mode","text":"<p>If your application scenario for PaddleX is secondary development, we recommend using the more powerful Plugin Installation Mode.</p> <p>After installing the PaddleX plugins you need, you can not only perform inference and integration on the models supported by the plugins but also conduct more advanced operations such as model training for secondary development.</p> <p>The plugins supported by PaddleX are as follows. Please determine the name(s) of the plugin(s) you need based on your development requirements:</p> \ud83d\udc49 Plugin and Pipeline Correspondence (Click to Expand) Pipeline Module Corresponding Plugin General Image Classification Image Classification <code>PaddleClas</code> General Object Detection Object Detection <code>PaddleDetection</code> General Semantic Segmentation Semantic Segmentation <code>PaddleSeg</code> General Instance Segmentation Instance Segmentation <code>PaddleDetection</code> General OCR Text DetectionText Recognition <code>PaddleOCR</code> General Table Recognition Layout Region DetectionTable Structure RecognitionText DetectionText Recognition <code>PaddleOCR</code><code>PaddleDetection</code> Document Scene Information Extraction v3 Table Structure RecognitionLayout Region DetectionText DetectionText RecognitionSeal Text DetectionDocument Image CorrectionDocument Image Orientation Classification <code>PaddleOCR</code><code>PaddleDetection</code><code>PaddleClas</code> Time Series Prediction Time Series Prediction Module <code>PaddleTS</code> Time Series Anomaly Detection Time Series Anomaly Detection Module <code>PaddleTS</code> Time Series Classification Time Series Classification Module <code>PaddleTS</code> General Multi-label Classification Image Multi-label Classification <code>PaddleClas</code> Small Object Detection Small Object Detection <code>PaddleDetection</code> Image Anomaly Detection Unsupervised Anomaly Detection <code>PaddleSeg</code> <p>If the plugin(s) you need to install is/are PaddleXXX (can be multiple), after installing PaddlePaddle, you can directly execute the following commands to quickly install the corresponding PaddleX plugin(s):</p> <p><pre><code># obtain PaddleX source code\ngit clone https://github.com/PaddlePaddle/PaddleX.git\ncd PaddleX\n\n# Install PaddleX whl\n# -e: Install in editable mode, so changes to the current project's code will directly affect the installed PaddleX Wheel\npip install -e .\n\n# Install PaddleX Plugins\npaddlex --install PaddleXXX\n</code></pre> For example, if you need to install the PaddleOCR and PaddleClas plugins, you can execute the following command:</p> <pre><code># Install PaddleOCR and PaddleClas Plugins\npaddlex --install PaddleOCR PaddleClas\n</code></pre> <p>If you wish to install all plugins, you do not need to specify the plugin names. Simply execute the following command:</p> <pre><code># Install All PaddleX Plugins\npaddlex --install\n</code></pre> <p>The default clone source for plugins is github.com, but it also supports gitee.com. You can specify the clone source using <code>--platform</code>.</p> <p>For instance, if you want to install all PaddleX plugins using the gitee.com clone source, execute the following command:</p> <pre><code># Install PaddleX Plugins using gitee.com\npaddlex --install --platform gitee.com\n</code></pre> <p>Upon successful installation, you will see the following prompt:</p> <pre><code>All packages are installed.\n</code></pre>"},{"location":"en/other_devices_support/multi_devices_use_guide.html#2-usage","title":"2. Usage","text":"<p>The usage of PaddleX model pipeline development tool on hardware platforms such as Ascend NPU, Cambricon MLU, Kunlun XPU, and Hygon DCU is identical to that on GPU. You only need to modify the device configuration parameters according to your hardware platform. For detailed usage tutorials, please refer to PaddleX Pipeline Development Tool Local Usage Guide.</p>"},{"location":"en/other_devices_support/paddlepaddle_install_DCU.html","title":"Hygon DCU PaddlePaddle Installation Tutorial","text":"<p>Currently, PaddleX supports Haiguang Z100 series chips. Considering environmental differences, we recommend using the officially released Haiguang DCU development image by PaddlePaddle, which is pre-installed with the Haiguang DCU basic runtime library (DTK).</p>"},{"location":"en/other_devices_support/paddlepaddle_install_DCU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<p>Pull the image. Note that this image is only for development environments and does not include pre-compiled PaddlePaddle installation packages.</p> <pre><code>docker pull registry.baidubce.com/device/paddle-dcu:dtk23.10.1-kylinv10-gcc73-py310\n</code></pre> <p>Start the container with the following command as a reference:</p> <pre><code>docker run -it --name paddle-dcu-dev -v `pwd`:/work \\\n  -w=/work --shm-size=128G --network=host --privileged  \\\n  --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n  registry.baidubce.com/device/paddle-dcu:dtk23.10.1-kylinv10-gcc73-py310 /bin/bash\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_DCU.html#2-install-paddlepaddle-package","title":"2. Install PaddlePaddle Package","text":"<p>Within the started docker container, download and install the wheel package released by PaddlePaddle's official website. Note: The DCU version of PaddlePaddle framework only supports Hygon C86 architecture.</p> <pre><code># Download and install the wheel package\npip install paddlepaddle-rocm -i https://www.paddlepaddle.org.cn/packages/nightly/dcu\n</code></pre> <p>After the installation package is installed, run the following command to verify it:</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>The expected output is as follows:</p> <pre><code>PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_MLU.html","title":"Cambricon MLU Installation Tutorial for PaddlePaddle","text":"<p>Currently, PaddleX supports the Cambricon MLU370X8 chip. Considering environmental differences, we recommend using the Cambricon MLU development image provided by PaddlePaddle to prepare your environment.</p>"},{"location":"en/other_devices_support/paddlepaddle_install_MLU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<p>Pull the image. This image is for development only and does not include a pre-compiled PaddlePaddle installation package.</p> <pre><code># Applicable to X86 architecture, Arch64 architecture image is not provided for now\ndocker pull registry.baidubce.com/device/paddle-mlu:ctr2.15.0-ubuntu20-gcc84-py310\n</code></pre> <p>Start the container with the following command as a reference:</p> <pre><code>docker run -it --name paddle-mlu-dev -v $(pwd):/work \\\n  -w=/work --shm-size=128G --network=host --privileged  \\\n  --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \\\n  -v /usr/bin/cnmon:/usr/bin/cnmon \\\n  registry.baidubce.com/device/paddle-mlu:ctr2.15.0-ubuntu20-gcc84-py310 /bin/bash\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_MLU.html#2-install-paddle-package","title":"2. Install Paddle Package","text":"<p>Within the started docker container, download and install the wheel package released by PaddlePaddle. Currently, Python 3.10 wheel packages are provided. If you require other Python versions, refer to the PaddlePaddle official documentation for compilation and installation instructions.</p> <pre><code># Download and install the wheel package\n# Note: You need to install the CPU version of PaddlePaddle first\npython -m pip install paddlepaddle==3.0.0.dev20240624 -i https://www.paddlepaddle.org.cn/packages/nightly/cpu/\npython -m pip install paddle-custom-mlu==3.0.0.dev20240806 -i https://www.paddlepaddle.org.cn/packages/nightly/mlu/\n</code></pre> <p>Verify the installation. After installation, run the following command:</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>The expected output is:</p> <pre><code>Running verify PaddlePaddle program ...\nPaddlePaddle works well on 1 mlu.\nPaddlePaddle works well on 16 mlus.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_NPU.html","title":"Ascend NPU PaddlePaddle Installation Tutorial","text":"<p>Currently, PaddleX supports the Ascend 910B chip (more models are under support. If you have a related need for other models, please submit an issue to inform us). The Ascend driver version is 23.0.3. Considering the differences in environments, we recommend using the Ascend development image provided by PaddlePaddle to complete the environment preparation.</p>"},{"location":"en/other_devices_support/paddlepaddle_install_NPU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<ul> <li>Pull the image. This image is only for the development environment and does not contain a pre-compiled PaddlePaddle installation package. The image has CANN-8.0.T13, the Ascend operator library, installed by default. <pre><code># For X86 architecture\ndocker pull registry.baidubce.com/device/paddle-npu:cann80T13-ubuntu20-x86_64-gcc84-py39\n# For Aarch64 architecture\ndocker pull registry.baidubce.com/device/paddle-npu:cann80T13-ubuntu20-aarch64-gcc84-py39\n</code></pre></li> <li>Start the container with the following command. ASCEND_RT_VISIBLE_DEVICES specifies the visible NPU card numbers. <pre><code>docker run -it --name paddle-npu-dev -v $(pwd):/work \\\n    --privileged --network=host --shm-size=128G -w=/work \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -e ASCEND_RT_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\" \\\n    registry.baidubce.com/device/paddle-npu:cann80T13-ubuntu20-$(uname -m)-gcc84-py39 /bin/bash\n</code></pre></li> </ul>"},{"location":"en/other_devices_support/paddlepaddle_install_NPU.html#2-install-paddle-package","title":"2. Install Paddle Package","text":"<p>Currently, Python 3.9 wheel installation packages are provided. If you have a need for other Python versions, you can refer to the PaddlePaddle official documentation to compile and install them yourself.</p> <ul> <li>Download and install the Python 3.9 wheel installation package <pre><code># Note: You need to install the CPU version of PaddlePaddle first\npython3.9 -m pip install paddlepaddle==3.0.0.dev20240520 -i https://www.paddlepaddle.org.cn/packages/nightly/cpu/\npython3.9 -m pip install paddle_custom_npu==3.0.0.dev20240719 -i https://www.paddlepaddle.org.cn/packages/nightly/npu/\n</code></pre></li> <li>After verifying that the installation package is installed, run the following command <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> The expected output is as follows</li> </ul> <pre><code>Running verify PaddlePaddle program ...\nPaddlePaddle works well on 1 npu.\nPaddlePaddle works well on 8 npus.\nPaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_XPU.html","title":"Kunlun XPU PaddlePaddle Installation Tutorial","text":"<p>Currently, PaddleX supports Kunlun R200/R300 and other chips. Considering environmental differences, we recommend using the Kunlun XPU development image officially released by PaddlePaddle, which is pre-installed with the Kunlun basic runtime environment library (XRE).</p>"},{"location":"en/other_devices_support/paddlepaddle_install_XPU.html#1-docker-environment-preparation","title":"1. Docker Environment Preparation","text":"<p>Pull the image. This image is only for the development environment and does not include a pre-compiled PaddlePaddle installation package.</p> <p><pre><code>docker pull registry.baidubce.com/device/paddle-xpu:ubuntu20-x86_64-gcc84-py310 # For X86 architecture\ndocker pull registry.baidubce.com/device/paddle-xpu:kylinv10-aarch64-gcc82-py310 # For ARM architecture\n</code></pre> Refer to the following command to start the container:</p> <pre><code>docker run -it --name=xxx -m 81920M --memory-swap=81920M \\\n    --shm-size=128G --privileged --net=host \\\n    -v $(pwd):/workspace -w /workspace \\\n    registry.baidubce.com/device/paddle-xpu:$(uname -m)-py310 bash\n</code></pre>"},{"location":"en/other_devices_support/paddlepaddle_install_XPU.html#2-install-paddle-package","title":"2. Install Paddle Package","text":"<p>Currently, Python3.10 wheel installation packages are provided. If you have a need for other Python versions, you can refer to the PaddlePaddle official documentation to compile and install them yourself.</p> <p>Install the Python3.10 wheel installation package:</p> <pre><code>pip install https://paddle-whl.bj.bcebos.com/paddlex/xpu/paddlepaddle_xpu-2.6.1-cp310-cp310-linux_x86_64.whl # For X86 architecture\npip install https://paddle-whl.bj.bcebos.com/paddlex/xpu/paddlepaddle_xpu-2.6.1-cp310-cp310-linux_aarch64.whl # For ARM architecture\n</code></pre> <p>Verify the installation package. After installation, run the following command:</p> <pre><code>python -c \"import paddle; paddle.utils.run_check()\"\n</code></pre> <p>The expected output is:</p> <pre><code>PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.\n</code></pre>"},{"location":"en/pipeline_deploy/edge_deploy.html","title":"PaddleX Edge Deployment Demo Usage Guide","text":"<ul> <li>PaddleX Edge Deployment Demo Usage Guide</li> <li>Installation Process and Usage<ul> <li>Environment Preparation</li> <li>Material Preparation</li> <li>Deployment Steps</li> </ul> </li> <li>Reference Materials</li> <li>Feedback Section</li> </ul> <p>This guide mainly introduces the operation method of the PaddleX edge deployment demo on the Android shell. This guide applies to 8 models across 6 modules:</p> Module Specific Model CPU GPU Object Detection PicoDet-S \u2705 \u2705 PicoDet-L \u2705 \u2705 Layout Area Detection PicoDet_layout_1x \u2705 \u2705 Semantic Segmentation PP-LiteSeg-T \u2705 \u2705 Image Classification PP-LCNet_x1_0 \u2705 \u2705 MobileNetV3_small_x1_0 \u2705 \u2705 Text Detection PP-OCRv4_mobile_det \u2705 Text Recognition PP-OCRv4_mobile_rec \u2705 <p>Note - <code>GPU</code> refers to mapping computations to GPU execution using OpenCL to fully utilize GPU hardware computing power and improve inference performance.</p>"},{"location":"en/pipeline_deploy/edge_deploy.html#installation-process-and-usage","title":"Installation Process and Usage","text":""},{"location":"en/pipeline_deploy/edge_deploy.html#environment-preparation","title":"Environment Preparation","text":"<ol> <li> <p>Install CMake build tool locally and download the required version of NDK software package from the Android NDK official website. For example, if developing on a Mac, download the NDK software package for the Mac platform from the Android NDK official website.</p> <p>Environment Requirements - <code>CMake &gt;= 3.10</code> (Minimum version not verified, recommend 3.20 and above) - <code>Android NDK &gt;= r17c</code> (Minimum version not verified, recommend r20b and above)</p> <p>Tested Environment Used in This Guide: - <code>cmake == 3.20.0</code> - <code>android-ndk == r20b</code></p> </li> <li> <p>Prepare an Android phone and enable USB debugging mode. Enable method: <code>Phone Settings -&gt; Locate Developer Options -&gt; Turn on Developer Options and USB Debugging Mode</code>.</p> </li> <li> <p>Install ADB tool on your computer for debugging. ADB installation methods:</p> <p>3.1. For Mac:</p> <pre><code>brew cask install android-platform-tools\n</code></pre> <p>3.2. For Linux:</p> <pre><code># Debian-based Linux distributions\nsudo apt update\nsudo apt install -y wget adb\n\n# Red Hat-based Linux distributions\nsudo yum install adb\n</code></pre> <p>3.3. For Windows:</p> <p>Install ADB by downloading the ADB software package from Google's Android platform: Link</p> <p>Open a terminal, connect your phone to the computer, and enter in the terminal:</p> <pre><code> adb devices\n</code></pre> <p>If there is an output from the device, it indicates that the installation was successful.</p> <pre><code> List of devices attached\n 744be294    device\n</code></pre> </li> </ol>"},{"location":"en/pipeline_deploy/edge_deploy.html#material-preparation","title":"Material Preparation","text":"<ol> <li> <p>Clone the <code>feature/paddle-x</code> branch of the <code>Paddle-Lite-Demo</code> repository into the <code>PaddleX-Lite-Deploy</code> directory.</p> <pre><code>git clone -b feature/paddle-x https://github.com/PaddlePaddle/Paddle-Lite-Demo.git PaddleX-Lite-Deploy\n</code></pre> </li> <li> <p>Fill out the survey to download the compressed package, place the compressed package in the specified unzip directory, switch to the specified unzip directory, and execute the unzip command.</p> <ul> <li>Object Detection Survey</li> <li>Semantic Segmentation Survey</li> <li>Image Classification Survey</li> <li>OCR Survey</li> </ul> <p>Below is an example of the unzip operation for object_detection. Refer to the table below for other pipelines.</p> <pre><code># 1. Switch to the specified unzip directory\ncd PaddleX-Lite-Deploy/object_detection/android/shell/cxx/picodet_detection\n\n# 2. Execute the unzip command\nunzip object_detection.zip\n</code></pre> <p> Pipeline Name Unzip Directory Unzip Command Object Detection PaddleX-Lite-Deploy/object_detection/android/shell/cxx/picodet_detection unzip object_detection.zip Semantic Segmentation PaddleX-Lite-Deploy/semantic_segmentation/android/shell/cxx/semantic_segmentation unzip semantic_segmentation.zip Image Classification PaddleX-Lite-Deploy/image_classification/android/shell/cxx/image_classification unzip image_classification.zip OCR PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo unzip ocr.zip </p> </li> </ol>"},{"location":"en/pipeline_deploy/edge_deploy.html#deployment-steps","title":"Deployment Steps","text":"<ol> <li> <p>Switch the working directory to <code>PaddleX_Lite_Deploy/libs</code> and run the <code>download.sh</code> script to download the necessary Paddle Lite prediction library. This step only needs to be executed once to support each demo.</p> </li> <li> <p>Switch the working directory to <code>PaddleX_Lite_Deploy/{Task_Name}/assets</code>, run the <code>download.sh</code> script to download the paddle_lite_opt tool optimized model, test images, label files, etc.</p> </li> <li> <p>Switch the working directory to <code>PaddleX_Lite_Deploy/{Task_Name}/android/shell/cxx/{Demo_Name}</code>, run the <code>build.sh</code> script to complete the compilation and execution of the executable file.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/{Task_Name}/android/shell/cxx/{Demo_Name}</code>, run the <code>run.sh</code> script to complete the prediction on the edge.</p> <p>Note: - <code>{Pipeline_Name}</code> and <code>{Demo_Name}</code> are placeholders. Refer to the table at the end of this section for specific values. - <code>download.sh</code> and <code>run.sh</code> support passing in model names to specify models. If not specified, the default model will be used. Refer to the <code>Model_Name</code> column in the table at the end of this section for currently supported models. - To use your own trained model, refer to the Model Conversion Method to obtain the <code>.nb</code> model, place it in the <code>PaddleX_Lite_Deploy/{Pipeline_Name}/assets/{Model_Name}</code> directory, where <code>{Model_Name}</code> is the model name, e.g., <code>PaddleX_Lite_Deploy/object_detection/assets/PicoDet-L</code>. - Before running the <code>build.sh</code> script, change the path specified by <code>NDK_ROOT</code> to the actual installed NDK path. - Keep ADB connected when running the <code>build.sh</code> script. - On Windows systems, you can use Git Bash to execute the deployment steps. - If compiling on a Windows system, set <code>CMAKE_SYSTEM_NAME</code> to <code>windows</code> in <code>CMakeLists.txt</code>. - If compiling on a Mac system, set <code>CMAKE_SYSTEM_NAME</code> to <code>darwin</code> in <code>CMakeLists.txt</code>.</p> </li> </ol> <p>Below is an example for object_detection. For other demos, change the directories switched in steps 2 and 3 according to the table at the end of this section.</p> <pre><code># 1. Download the necessary Paddle Lite prediction library\ncd PaddleX_Lite_Deploy/libs\nsh download.sh\n\n# 2. Download the paddle_lite_opt tool optimized model, test images, and label files\ncd ../object_detection/assets\nsh download.sh\n# Supports passing in model names to specify the downloaded model. Refer to the Model_Name column in the table at the end of this section for supported models.\n# sh download.sh PicoDet-L\n\n# 3. Complete the compilation of the executable file\ncd ../android/app/shell/cxx/picodet_detection\nsh build.sh\n\n# 4. Prediction\nsh run.sh\n# Supports passing in model names to specify the prediction model. Refer to the Model_Name column in the table at the end of this section for supported models.\n# sh run.sh PicoDet-L\n</code></pre> <p>The run results are shown below, and a result image named <code>dog_picodet_detection_result.jpg</code> is generated:</p> <pre><code>======= benchmark summary =======\ninput_shape(s) (NCHW): {1, 3, 320, 320}\nmodel_dir:./models/PicoDet-S/model.nb\nwarmup:1\nrepeats:10\npower_mode:1\nthread_num:0\n&lt;b&gt;* time info(ms) &lt;/b&gt;*\n1st_duration:320.086\nmax_duration:277.331\nmin_duration:272.67\navg_duration:274.91\n\n====== output summary ======\ndetection, image size: 768, 576, detect object: bicycle, score: 0.905929, location: x=125, y=1\n</code></pre> <p></p> <p>This section describes the deployment steps applicable to the demos listed in the following table:</p> Pipeline Pipeline_Name Module Demo_Name Specific Model Model_Name General Object Detection object_detection Object Detection picodet_detection PicoDet-S PicoDet-S\uff08default\uff09PicoDet-S_gpu PicoDet-L PicoDet-LPicoDet-L_gpu PicoDet_layout_1x PicoDet_layout_1xPicoDet_layout_1x_gpu General Semantic Segmentation semantic_segmentation Semantic Segmentation semantic_segmentation PP-LiteSeg-T PP-LiteSeg-T\uff08default\uff09PP-LiteSeg-T_gpu General Image Classification image_classification Image Classification image_classification PP-LCNet_x1_0 PP-LCNet_x1_0\uff08default\uff09PP-LCNet_x1_0_gpu MobileNetV3_small_x1_0 MobileNetV3_small_x1_0MobileNetV3_small_x1_0_gpu General OCR ocr Text Detection ppocr_demo PP-OCRv4_mobile_det PP-OCRv4_mobile_det Text Recognition PP-OCRv4_mobile_rec PP-OCRv4_mobile_rec <p>Note - Currently, there is no demo for deploying the Layout Area Detection module on the edge, so the <code>picodet_detection</code> demo is reused to deploy the <code>PicoDet_layout_1x</code> model.</p>"},{"location":"en/pipeline_deploy/edge_deploy.html#reference-materials","title":"Reference Materials","text":"<p>This guide only introduces the basic installation and usage process of the edge deployment demo. If you want to learn more detailed information, such as code introduction, code explanation, updating models, updating input and output preprocessing, updating prediction libraries, etc., please refer to the following documents:</p> <ul> <li>Object Detection</li> <li>Semantic Segmentation</li> <li>Image Classification</li> <li>OCR</li> </ul>"},{"location":"en/pipeline_deploy/edge_deploy.html#feedback-section","title":"Feedback Section","text":"<p>The edge deployment capabilities are continuously optimized. Welcome to submit issue to report problems and needs, and we will follow up promptly.</p>"},{"location":"en/pipeline_deploy/high_performance_inference.html","title":"PaddleX High-Performance Inference Guide","text":"<p>In real-world production environments, many applications have stringent standards for deployment strategy performance metrics, particularly response speed, to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins designed to deeply optimize model inference and pre/post-processing, achieving significant speedups in the end-to-end process. This document will first introduce the installation and usage of the high-performance inference plugins, followed by a list of pipelines and models currently supporting the use of these plugins.</p>"},{"location":"en/pipeline_deploy/high_performance_inference.html#1-installation-and-usage-of-high-performance-inference-plugins","title":"1. Installation and Usage of High-Performance Inference Plugins","text":"<p>Before using the high-performance inference plugins, ensure you have completed the installation of PaddleX according to the PaddleX Local Installation Tutorial, and have successfully run the quick inference of the pipeline using either the PaddleX pipeline command line instructions or the Python script instructions.</p>"},{"location":"en/pipeline_deploy/high_performance_inference.html#11-installing-high-performance-inference-plugins","title":"1.1 Installing High-Performance Inference Plugins","text":"<p>Find the corresponding installation command based on your processor architecture, operating system, device type, and Python version in the table below and execute it in your deployment environment. Please replace <code>{paddlex version number}</code> with the actual paddlex version number, such as the current latest stable version <code>3.0.0b2</code>. If you need to use the version corresponding to the development branch, replace <code>{paddlex version number}</code> with <code>0.0.0.dev0</code>.</p> Processor Architecture Operating System Device Type Python Version Installation Command x86-64 Linux CPU 3.8 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.8 - --arch x86_64 --os linux --device cpu --py 38 3.9 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.9 - --arch x86_64 --os linux --device cpu --py 39 3.10 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.10 - --arch x86_64 --os linux --device cpu --py 310 GPU\u00a0(CUDA\u00a011.8\u00a0+\u00a0cuDNN\u00a08.6) 3.8 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.8 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 38 3.9 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.9 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 39 3.10 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/{paddlex version number}/install_paddlex_hpi.py | python3.10 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 310 <ul> <li>For Linux systems, execute the installation instructions using Bash.</li> <li>When using NVIDIA GPUs, please use the installation instructions corresponding to the CUDA and cuDNN versions that match your environment. Otherwise, you will not be able to use the high-performance inference plugin properly.</li> <li>When the device type is CPU, the installed high-performance inference plugin only supports inference using the CPU; for other device types, the installed high-performance inference plugin supports inference using the CPU or other devices.</li> </ul>"},{"location":"en/pipeline_deploy/high_performance_inference.html#12-obtaining-serial-numbers-and-activation","title":"1.2 Obtaining Serial Numbers and Activation","text":"<p>On the Baidu AIStudio Community - AI Learning and Training Platform page, under the \"Open-source Pipeline Deployment Serial Number Inquiry and Acquisition\" section, select \"Acquire Now\" as shown in the following image:</p> <p></p> <p>Select the pipeline you wish to deploy and click \"Acquire\". Afterwards, you can find the acquired serial number in the \"Open-source Pipeline Deployment SDK Serial Number Management\" section at the bottom of the page:</p> <p></p> <p>After using the serial number to complete activation, you can utilize high-performance inference plugins. PaddleX provides both online and offline activation methods (both only support Linux systems):</p> <ul> <li>Online Activation: When using the inference API or CLI, specify the serial number and enable online activation to automatically complete the process.</li> <li>Offline Activation: Follow the instructions in the serial number management interface (click \"Offline Activation\" under \"Operations\") to obtain the device fingerprint of your machine. Bind the serial number with the device fingerprint to obtain a certificate and complete the activation. For this activation method, you need to manually store the certificate in the <code>${HOME}/.baidu/paddlex/licenses</code> directory on the machine (create the directory if it does not exist) and specify the serial number when using the inference API or CLI.</li> </ul> <p>Please note: Each serial number can only be bound to a unique device fingerprint and can only be bound once. This means that if users deploy models on different machines, they must prepare separate serial numbers for each machine.</p>"},{"location":"en/pipeline_deploy/high_performance_inference.html#13-enabling-high-performance-inference-plugins","title":"1.3 Enabling High-Performance Inference Plugins","text":"<p>For Linux systems, if using the high-performance inference plugin in a Docker container, please mount the host machine's <code>/dev/disk/by-uuid</code> and <code>${HOME}/.baidu/paddlex/licenses</code> directories to the container.</p> <p>For PaddleX CLI, specify <code>--use_hpip</code> and set the serial number to enable the high-performance inference plugin. If you wish to activate the license online, specify <code>--update_license</code> when using the serial number for the first time. Taking the general image classification pipeline as an example:</p> <pre><code>paddlex \\\n    --pipeline image_classification \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg \\\n    --device gpu:0 \\\n    --use_hpip \\\n    --serial_number {serial_number}\n\n# If you wish to perform online activation\npaddlex \\\n    --pipeline image_classification \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg \\\n    --device gpu:0 \\\n    --use_hpip \\\n    --serial_number {serial_number} \\\n    --update_license\n</code></pre> <p>For PaddleX Python API, enabling the high-performance inference plugin is similar. Still taking the general image classification pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"image_classification\",\n    use_hpip=True,\n    hpi_params={\"serial_number\": \"{serial_number}\"},\n)\n\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\")\n</code></pre> <p>The inference results obtained with the high-performance inference plugin enabled are consistent with those without the plugin enabled. For some models, enabling the high-performance inference plugin for the first time may take a longer time to complete the construction of the inference engine. PaddleX will cache the relevant information in the model directory after the first construction of the inference engine and reuse the cached content in subsequent runs to improve initialization speed.</p>"},{"location":"en/pipeline_deploy/high_performance_inference.html#14-modifying-high-performance-inference-configurations","title":"1.4 Modifying High-Performance Inference Configurations","text":"<p>PaddleX combines model information and runtime environment information to provide default high-performance inference configurations for each model. These default configurations are carefully prepared to be applicable in several common scenarios and achieve relatively optimal performance. Therefore, users typically may not need to be concerned with the specific details of these configurations. However, due to the diversity of actual deployment environments and requirements, the default configuration may not yield ideal performance in certain scenarios and could even result in inference failures. In cases where the default configuration does not meet the requirements, users can manually adjust the configuration by modifying the Hpi field in the inference.yml file within the model directory (if this field does not exist, it needs to be added). The following are two common situations:</p> <ul> <li> <p>Switching inference backends:</p> <p>When the default inference backend is not available, the inference backend needs to be switched manually. Users should modify the <code>selected_backends</code> field (if it does not exist, it needs to be added).</p> <pre><code>Hpi:\n  ...\n  selected_backends:\n    cpu: paddle_infer\n    gpu: onnx_runtime\n  ...\n</code></pre> <p>Each entry should follow the format <code>{device type}: {inference backend name}</code>.</p> <p>The currently available inference backends are:</p> <ul> <li><code>paddle_infer</code>: The Paddle Inference engine. Supports CPU and GPU. Compared to the PaddleX quick inference, TensorRT subgraphs can be integrated to enhance inference performance on GPUs.</li> <li><code>openvino</code>: OpenVINO, a deep learning inference tool provided by Intel, optimized for model inference performance on various Intel hardware. Supports CPU only. The high-performance inference plugin automatically converts the model to the ONNX format and uses this engine for inference.</li> <li><code>onnx_runtime</code>: ONNX Runtime, a cross-platform, high-performance inference engine. Supports CPU and GPU. The high-performance inference plugin automatically converts the model to the ONNX format and uses this engine for inference.</li> <li><code>tensorrt</code>: TensorRT, a high-performance deep learning inference library provided by NVIDIA, optimized for NVIDIA GPUs to improve speed. Supports GPU only. The high-performance inference plugin automatically converts the model to the ONNX format and uses this engine for inference.</li> </ul> </li> <li> <p>Modifying dynamic shape configurations for Paddle Inference or TensorRT:</p> <p>Dynamic shape is the ability of TensorRT to defer specifying parts or all of a tensor\u2019s dimensions until runtime. If the default dynamic shape configuration does not meet requirements (e.g., the model may require input shapes beyond the default range), users need to modify the <code>trt_dynamic_shapes</code> or <code>dynamic_shapes</code> field in the inference backend configuration:</p> <pre><code>Hpi:\n  ...\n  backend_configs:\n    # Configuration for the Paddle Inference backend\n    paddle_infer:\n      ...\n      trt_dynamic_shapes:\n        x:\n          - [1, 3, 300, 300]\n          - [4, 3, 300, 300]\n          - [32, 3, 1200, 1200]\n      ...\n    # Configuration for the TensorRT backend\n    tensorrt:\n      ...\n      dynamic_shapes:\n        x:\n          - [1, 3, 300, 300]\n          - [4, 3, 300, 300]\n          - [32, 3, 1200, 1200]\n      ...\n</code></pre> <p>In <code>trt_dynamic_shapes</code> or <code>dynamic_shapes</code>, each input tensor requires a specified dynamic shape in the format: <code>{input tensor name}: [{minimum shape}, [{optimal shape}], [{maximum shape}]]</code>. For details on minimum, optimal, and maximum shapes and further information, please refer to the official TensorRT documentation.</p> <p>After completing the modifications, please delete the cache files in the model directory (<code>shape_range_info.pbtxt</code> and files starting with <code>trt_serialized</code>).</p> </li> </ul>"},{"location":"en/pipeline_deploy/high_performance_inference.html#2-pipelines-and-models-supporting-high-performance-inference-plugins","title":"2. Pipelines and Models Supporting High-Performance Inference Plugins","text":"Pipeline Module Model Support List OCR Text Detection \u2705 Text Recognition \u2705 PP-ChatOCRv3 Table Recognition \u2705 Layout Detection \u2705 Text Detection \u2705 Text Recognition \u2705 Seal Text Detection \u2705 Text Image Unwarping \u2705 Document Image Orientation Classification \u2705 Table Recognition Layout Detection \u2705 Table Recognition \u2705 Text Detection \u2705 Text Recognition \u2705 Object Detection Object Detection FasterRCNN-Swin-Tiny-FPN \u274cCenterNet-DLA-34 \u274c CenterNet-ResNet50 \u274c Instance Segmentation Instance Segmentation Mask-RT-DETR-S \u274c Image Classification Image Classification \u2705 Semantic Segmentation Semantic Segmentation \u2705 Time Series Forecasting Time Series Forecasting \u274c Time Series Anomaly Detection Time Series Anomaly Forecasting \u274c Time Series Classification Time Series Classification \u274c Small Object Detection Small Object Detection \u2705 Multi-Label Image Classification Multi-Label Image  Classification \u2705 Image Anomaly Detection Unsupervised Anomaly Detection \u2705 Layout Parsing Table Structure Recognition \u2705 Layout Region Analysis \u2705 Text Detection \u2705 Text Recognition \u2705 Formula Recognition \u2705 Seal Text Detection \u2705 Text Image Unwarping \u2705 Document Image Orientation Classification \u2705 Formula Recognition Layout Detection \u2705 Formula Recognition \u2705 Seal Recognition Layout Region Analysis \u2705 Seal Text Detection \u2705 Text Recognition \u2705 Image Recognition Subject Detection \u2705 Image Feature \u2705 Pedestrian Attribute Recognition Pedestrian Detection \u274c Pedestrian Attribute Recognition \u274c Vehicle Attribute Recognition Vehicle Detection \u274c Vehicle Attribute Recognition \u274c Face Recognition Face Detection \u2705 Face Feature \u2705"},{"location":"en/pipeline_deploy/service_deploy.html","title":"PaddleX Serving Deployment Guide","text":"<p>Serving deployment is a common form of deployment in real-world production environments. By encapsulating inference capabilities as services, clients can access these services through network requests to obtain inference results. PaddleX enables users to achieve low-cost serving deployment for production lines. This document will first introduce the basic process of serving deployment using PaddleX, followed by considerations and potential operations when using the service in a production environment.</p> <p>Note - Serving deployment provides services for model pipelines, not specific to individual pipeline modules.</p> <p>Serving Deployment Example Diagram:</p> <p></p>"},{"location":"en/pipeline_deploy/service_deploy.html#1-basic-process","title":"1. Basic Process","text":""},{"location":"en/pipeline_deploy/service_deploy.html#11-install-the-serving-deployment-plugin","title":"1.1 Install the Serving Deployment Plugin","text":"<p>Execute the following command to install the serving deployment plugin:</p> <pre><code>paddlex --install serving\n</code></pre>"},{"location":"en/pipeline_deploy/service_deploy.html#12-start-the-service","title":"1.2 Start the Service","text":"<p>Start the service through the PaddleX CLI with the following command format:</p> <pre><code>paddlex --serve --pipeline {pipeline_name_or_path} [{other_command_line_options}]\n</code></pre> <p>Taking the General Image Classification Pipeline as an example:</p> <pre><code>paddlex --serve --pipeline image_classification\n</code></pre> <p>After the service starts successfully, you will see information similar to the following:</p> <pre><code>INFO:     Started server process [63108]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n</code></pre> <p><code>--pipeline</code> can be specified as an official pipeline name or the path to a local pipeline configuration file. PaddleX uses this to build the pipeline and deploy it as a service. To adjust configurations (such as model path, batch_size, deployment device), please refer to the \"Model Application\" section in the General Image Classification Pipeline Tutorial.</p> <p>Command-line options related to serving deployment are as follows:</p> Name Description <code>--pipeline</code> Pipeline name or pipeline configuration file path. <code>--device</code> Deployment device for the pipeline. Defaults to <code>cpu</code> (If GPU is unavailable) or <code>gpu</code> (If GPU is available). <code>--host</code> Hostname or IP address bound to the server. Defaults to 0.0.0.0. <code>--port</code> Port number listened to by the server. Defaults to 8080. <code>--use_hpip</code> Enables the high-performance inference plugin if specified. <code>--serial_number</code> Serial number used by the high-performance inference plugin. Only valid when the high-performance inference plugin is enabled. Note that not all pipelines and models support the use of the high-performance inference plugin. For detailed support, please refer to the PaddleX High-Performance Inference Guide. <code>--update_license</code> Activates the license online if specified. Only valid when the high-performance inference plugin is enabled."},{"location":"en/pipeline_deploy/service_deploy.html#13-call-the-service","title":"1.3 Call the Service","text":"<p>Please refer to the \"Development Integration/Deployment\" section in the usage tutorials for each pipeline. You can find the pipeline tutorials here.</p>"},{"location":"en/pipeline_deploy/service_deploy.html#2-deploy-services-for-production","title":"2. Deploy Services for Production","text":"<p>When deploying services into production environments, the stability, efficiency, and security of the services are of paramount importance. Below are some recommendations for deploying services into production.</p>"},{"location":"en/pipeline_deploy/service_deploy.html#21-utilize-paddlex-high-performance-inference-plugin","title":"2.1 Utilize PaddleX high-performance inference Plugin","text":"<p>In scenarios where strict response time requirements are imposed on applications, the PaddleX high-performance inference Plugin can be used to accelerate model inference and pre/post-processing, thereby reducing response time and increasing throughput.</p> <p>To use the PaddleX high-performance inference Plugin, please refer to the PaddleX High-Performance Inference Guide for installing the high-performance inference plugin, obtaining serial numbers, and activating the plugin. Additionally, not all pipelines, models, and environments support the use of the high-performance inference plugin. For detailed support information, please refer to the section on pipelines and models that support the high-performance inference plugin.</p> <p>When starting the PaddleX pipeline service, you can specify <code>--use_hpip</code> along with the serial number to use the high-performance inference plugin. If you wish to perform online activation, you should also specify <code>--update_license</code>. Example usage:</p> <pre><code>paddlex --serve --pipeline image_classification --use_hpip --serial_number {serial_number}\n\n# If you wish to perform online activation\npaddlex --serve --pipeline image_classification --use_hpip --serial_number {serial_number} --update_license\n</code></pre>"},{"location":"en/pipeline_deploy/service_deploy.html#22-consider-security","title":"2.2 Consider Security","text":"<p>A typical scenario involves an application accepting inputs from the network, with the PaddleX pipeline service acting as a module within the application, interacting with other modules through APIs. In this case, the position of the PaddleX pipeline service within the application is crucial. The service-oriented deployment solution provided by PaddleX focuses on efficiency and ease of use but does not perform sufficient security checks on request bodies. Malicious requests from the network, such as excessively large images or carefully crafted data, can lead to severe consequences like service crashes. Therefore, it is recommended to place the PaddleX pipeline service within the application's internal network, avoiding direct processing of external inputs, and ensuring it only processes trustworthy requests. Appropriate protective measures, such as input validation and authentication, should be added at the application's outer layer.</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html","title":"Overview of PaddleX Model Pipeline Usage","text":"<p>If you have already experienced the pre-trained model pipeline effects in PaddleX and wish to proceed directly with model fine-tuning, you can jump to Model Selection.</p> <p>The complete PaddleX model pipeline development process is illustrated in the following diagram:</p> <p>PaddleX Model Pipeline Development Flowchart</p> <pre><code>graph LR\n    select_pipeline(Select Pipeline) --&gt; online_experience[Quick Experience]\n    online_experience --&gt; online_ok{Satisfied with Results?}\n    online_ok --No--&gt; select_model[Select Model]\n    select_model --&gt; model_finetune[Model Fine-tuning]\n    online_ok --Yes--&gt; development_integration(Development Integration/Deployment)\n    model_finetune --&gt; pipeline_test[Pipeline Testing]\n    pipeline_test --&gt; test_ok{Satisfied with Results?}\n    test_ok --No--&gt; select_model\n    test_ok --Yes--&gt; development_integration</code></pre> <p>The pre-trained model pipelines provided by PaddleX allow for quick experience of effects. If the pipeline effects meet your requirements, you can directly proceed with development integration/deployment of the pre-trained model pipeline. If the effects are not as expected, you can use your private data to fine-tune the models within the pipeline until satisfactory results are achieved.</p> <p>Below, let's take the task of boarding pass recognition as an example to introduce the local usage process of the PaddleX model pipeline tool. Before use, please ensure you have completed the installation of PaddleX according to the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#1-select-pipeline","title":"1. Select Pipeline","text":"<p>Each pipeline in PaddleX can solve specific task scenarios such as object detection, time series prediction, semantic segmentation, etc. You need to select the pipeline for subsequent development based on the specific task. For example, for the boarding pass recognition task, the corresponding PaddleX pipeline is the General OCR Pipeline. More task-pipeline correspondences can be found in the PaddleX Models List (CPU/GPU).</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#2-quick-start","title":"2. Quick Start","text":"<p>Each pipeline in PaddleX integrates numerous pre-trained models. You can first experience the effects of the PaddleX pre-trained model pipeline. If the effects of the pre-trained model pipeline meet your expectations, you can proceed directly with Development Integration/Deployment. If not, optimize the pipeline effects according to the subsequent steps.</p> <p>PaddleX provides three ways to quickly experience pipeline effects. You can choose the appropriate method based on your needs:</p> <ul> <li>Online Quick Experience URL: PaddleX Pipeline List (CPU/GPU)</li> <li>Command Line Quick Experience: PaddleX Pipeline Command Line Usage Instructions</li> <li>Python Script Quick Experience: PaddleX Pipeline Python API Usage Instructions</li> </ul> <p>To demonstrate the OCR pipeline for the boarding pass recognition task, you can quickly experience the pipeline's effect in three ways:</p> <p>\ud83c\udf10 Online Experience</p> <p>You can experience the effects of the universal OCR pipeline in AI Studio online. Use the official demo image provided for recognition, for example:</p> <p></p> <p>\ud83d\udcbb Command Line Experience</p> <p>A single command can quickly experience the pipeline effects. Use the test file, and replace <code>--input</code> with a local path for prediction: <pre><code>paddlex --pipeline OCR --input general_ocr_002.png --device gpu:0\n</code></pre> Parameter description:</p> <pre><code>--pipeline: Pipeline name, which is the OCR pipeline in this case.\n--input: Local path or URL of the input image to be processed.\n--device: GPU serial number used (for example, gpu:0 means using the 0th GPU, gpu:1,2 means using the 1st and 2nd GPUs), or you can choose to use CPU (--device cpu).\n</code></pre> \ud83d\udc49Click to view the running results <p>After running, the result is:</p> <pre><code>{'input_path': 'general_ocr_002.png', 'dt_polys': [array([[ 6, 13],\n       [64, 13],\n       [64, 31],\n       [ 6, 31]], dtype=int16), array([[210,  14],\n       [238,  14],\n       ...\n       [830, 445],\n       [830, 464],\n       [338, 473]], dtype=int16)], 'dt_scores': [0.7629529090100092, 0.7717284653547034, 0.7139251666762622, 0.8057611181556994, 0.8840947658872964, 0.793295938183885, 0.8342027855884783, 0.8081378522874861, 0.8436969344212185, 0.8500845646497226, 0.7932189714842249, 0.8875924621248228, 0.8827884273639948, 0.8322404317386042, 0.8614796803023563, 0.8804252994596097, 0.9069978945305474, 0.8383917914190059, 0.8495824076580516, 0.8825556800041383, 0.852788927706737, 0.8379584696974435, 0.8633519228646618, 0.763234473595298, 0.8602154244410916, 0.9206341882426813, 0.6341425973804049, 0.8490156149797171, 0.758314821564747, 0.8757849788793592, 0.772485060565334, 0.8404023012596349, 0.8190037953773427, 0.851908529295617, 0.6126112758079643, 0.7324388418218587], 'rec_text': ['www.9', '5', 'boarding pass', 'BOARDING', 'PASS', 'cabin class', '', 'CLASS', 'SERIAL NO', 'seat number', 'SEAT NO', 'flight FLIGHT', 'date DATE', '03DEC', 'W', '035', 'MU 2379', 'departure city', 'FROM', 'boarding gate', 'GATE', 'boarding time BDT', 'destination TO', 'Fuzhou', 'Taiyuan', 'G11', 'FUZHOU', 'ID NO', 'NAME', 'ZHANGQIWEI', 'ticket number TKTNO', 'Zhang Qiwei', 'fare FARE', 'ETKT7813699238489/1', 'The boarding gate closes 10 minutes before departure', 'GATES CLOSE 10 MINUTES BEFORE DEPARTURE TIME'], 'rec_score': [0.683099627494812, 0.23417049646377563, 0.9969978928565979, 0.9945957660675049, 0.9787729382514954, 0.9983421564102173, 0.0, 0.9896272420883179, 0.9927973747253418, 0.9976049065589905, 0.9330753684043884, 0.9562691450119019, 0.9312669038772583, 0.9749765396118164, 0.9749416708946228, 0.9988260865211487, 0.9319792985916138, 0.9979889988899231, 0.9956836700439453, 0.9991750717163086, 0.9938803315162659, 0.9982991218566895, 0.9701204299926758, 0.9986245632171631, 0.9888408780097961, 0.9793729782104492, 0.9952947497367859, 0.9945247173309326, 0.9919753670692444, 0.991995632648468, 0.9937331080436707, 0.9963390827178955, 0.9954304695129395, 0.9934715628623962, 0.9974429607391357, 0.9529641270637512]}\n</code></pre> <p>The visualization result is as follows:</p> <p></p> <p>When executing the above command, the default OCR pipeline configuration file is loaded. If you need a custom configuration file, you can follow the steps below:</p> \ud83d\udc49Click to expand <p>Get the OCR pipeline configuration file:</p> <pre><code>paddlex --get_pipeline_config OCR\n</code></pre> <p>After execution, the OCR pipeline configuration file will be saved in the current path. If you want to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config OCR --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./ocr.yaml</code>, just execute:</p> <pre><code>paddlex --pipeline ./ocr.yaml --input general_ocr_002.png\n</code></pre> <p>Parameters such as <code>--model</code>, <code>--device</code> do not need to be specified, and the parameters in the configuration file will be used. If parameters are still specified, the specified parameters will prevail.</p> <p>\ud83d\udcbb Python Script Experience</p> <p>A few lines of code can quickly experience the pipeline effects:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"OCR\")\n\noutput = pipeline.predict(\"general_ocr_002.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> <p>The following steps are executed:</p> <ul> <li><code>create_pipeline()</code> instantiates the pipeline object</li> <li>Passes the image and calls the <code>predict</code> method of the pipeline object for inference prediction</li> <li>Processes the prediction results</li> </ul> <p>\u2757 The results obtained from running the Python script are the same as those from the command line method.</p> <p>If the pre-trained model pipeline meets your expectations, you can proceed directly to development integration/deployment. If not, optimize the pipeline effects according to the following steps.</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#3-model-selection-optional","title":"3. Model Selection (Optional)","text":"<p>Since a pipeline may contain one or more models, when fine-tuning models, you need to determine which model to fine-tune based on testing results. Taking the OCR pipeline for boarding pass recognition as an example, this pipeline includes a text detection model (e.g., <code>PP-OCRv4_mobile_det</code>) and a text recognition model (e.g., <code>PP-OCRv4_mobile_rec</code>). If the text positioning is inaccurate, you need to fine-tune the text detection model. If the text recognition is inaccurate, you need to fine-tune the text recognition model. If you are unsure which models are included in the pipeline, you can refer to the PaddleX Models List (CPU/GPU)</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#4-model-fine-tuning-optional","title":"4. Model Fine-tuning (Optional)","text":"<p>After determining the model to fine-tune, you need to train the model with your private dataset. PaddleX provides a single-model development tool that can complete model training with a single command:</p> <p><pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_mobile_rec.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=your/dataset_dir\n</code></pre> In addition, PaddleX provides detailed tutorials for preparing private datasets for model fine-tuning, single-model inference, and more. For details, please refer to the PaddleX Modules Tutorials</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#5-pipeline-testing-optional","title":"5. Pipeline Testing (Optional)","text":"<p>After fine-tuning your model with a private dataset, you will obtain local model weight files.</p> <p>To use the fine-tuned model weights, simply modify the production line configuration file by replacing the local paths of the fine-tuned model weights with the corresponding paths in the configuration file:</p> <p><pre><code>......\nPipeline:\n  det_model: PP-OCRv4_server_det  # Can be modified to the local path of the fine-tuned text detection model\n  det_device: \"gpu\"\n  rec_model: PP-OCRv4_server_rec  # Can be modified to the local path of the fine-tuned text recognition model\n  rec_batch_size: 1\n  rec_device: \"gpu\"\n......\n</code></pre> Then, refer to the command line method or Python script method to load the modified pipeline configuration file.</p> <p>If the results are satisfactory, proceed with Development Integration/Deployment. If not, return to Model Selection to continue fine-tuning other task modules until you achieve satisfactory results.</p>"},{"location":"en/pipeline_usage/pipeline_develop_guide.html#6-development-integration-and-deployment","title":"6. Development Integration and Deployment","text":"<p>If the pre-trained pipeline meets your requirements for inference speed and accuracy, you can proceed directly to development integration/deployment.</p> <p>If you need to apply the pipeline directly in your Python project, you can refer to the PaddleX Pipeline Python Script Usage Guide and the Python example code in the Quick Start section.</p> <p>In addition, PaddleX also provides three other deployment methods, with detailed instructions as follows:</p> <p>\ud83d\ude80 high-performance inference: In actual production environments, many applications have stringent standards for the performance metrics (especially response speed) of deployment strategies to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that aim to deeply optimize model inference and pre/post-processing for significant speedups in the end-to-end process. Refer to the PaddleX High-Performance Inference Guide for detailed high-performance inference procedures.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. Refer to the PaddleX Service-Oriented Deployment Guide for detailed service-oriented deployment procedures.</p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. Refer to the PaddleX Edge Deployment Guide for detailed edge deployment procedures.</p> <p>Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p> <p>\u2757 PaddleX provides detailed usage instructions for each pipeline. You can choose according to your needs. Here are all the pipelines and their corresponding detailed instructions:</p> Pipeline Name Detailed Description PP-ChatOCR-doc v3 PP-ChatOCR-doc v3 Pipeline Usage Tutorial Image Classification Image Classification Pipeline Usage Tutorial Object Detection Object Detection Pipeline Usage Tutorial Instance Segmentation Instance Segmentation Pipeline Usage Tutorial Semantic Segmentation Semantic Segmentation Pipeline Usage Tutorial Image Multi-label Classification Image Multi-label Classification Pipeline Usage Tutorial Image Recognition Image Recognition Pipeline Usage Tutorial Pedestrian Attribute Recognition Pedestrian Attribute Recognition Pipeline Usage Tutorial Vehicle Attribute Recognition Vehicle Attribute Recognition Pipeline Usage Tutorial Face Recognition Face Recognition Pipeline Usage Tutorial Small Object Detection Small Object Detection Pipeline Usage Tutorial Image Anomaly Detection Image Anomaly Detection Pipeline Usage Tutorial OCR OCR Pipeline Usage Tutorial Table Recognition Table Recognition Pipeline Usage Tutorial Layout Parsing Layout Parsing Pipeline Usage Tutorial Formula Recognition Formula Recognition Pipeline Usage Tutorial Seal Recognition Seal Recognition Pipeline Usage Tutorial Time Series Forecasting Time Series Forecasting Pipeline Usage Tutorial Time Series Anomaly Detection Time Series Anomaly Detection Pipeline Usage Tutorial Time Series Classification Time Series Classification Pipeline Usage Tutorial"},{"location":"en/pipeline_usage/instructions/pipeline_CLI_usage.html","title":"PaddleX Pipeline CLI Usage Instructions","text":"<p>Before using the CLI command line for rapid inference of the pipeline, please ensure that you have completed the installation of PaddleX according to the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/instructions/pipeline_CLI_usage.html#i-usage-example","title":"I. Usage Example","text":""},{"location":"en/pipeline_usage/instructions/pipeline_CLI_usage.html#1-quick-experience","title":"1. Quick Experience","text":"<p>Taking the image classification pipeline as an example, the usage is as follows:</p> <p><pre><code>paddlex --pipeline image_classification \\\n        --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg \\\n        --device gpu:0 \\\n        --save_path ./output/\n</code></pre> This single step completes the inference prediction and saves the prediction results. Explanations for the relevant parameters are as follows:</p> <ul> <li><code>pipeline</code>: The name of the pipeline or the local path to the pipeline configuration file, such as the pipeline name \"image_classification\", or the path to the pipeline configuration file \"path/to/image_classification.yaml\";</li> <li><code>input</code>: The path to the data file to be predicted, supporting local file paths, local directories containing data files to be predicted, and file URL links;</li> <li><code>device</code>: Used to set the model inference device. If set for GPU, you can specify the card number, such as \"cpu\", \"gpu:2\". When not specified, if GPU is available, it will be used; otherwise, CPU will be used;</li> <li><code>save_path</code>: The save path for prediction results. When not specified, the prediction results will not be saved;</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_CLI_usage.html#2-custom-pipeline-configuration","title":"2. Custom Pipeline Configuration","text":"<p>If you need to modify the pipeline configuration, you can retrieve the configuration file and modify it. Still taking the image classification pipeline as an example, the way to retrieve the configuration file is as follows:</p> <pre><code>paddlex --get_pipeline_config image_classification\n\n# Please enter the path that you want to save the pipeline config file: (default `./`)\n./configs/\n\n# The pipeline config has been saved to: configs/image_classification.yaml\n</code></pre> <p>After modifying the production line configuration file <code>configs/image_classification.yaml</code>, such as the content for the image classification configuration file:</p> <pre><code>Global:\n  pipeline_name: image_classification\n  input: https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\n\nPipeline:\n  model: PP-LCNet_x0_5\n  batch_size: 1\n  device: \"gpu:0\"\n</code></pre> <p>Once the modification is completed, you can use this configuration file to perform model pipeline inference prediction as follows:</p> <pre><code>paddlex --pipeline configs/image_classification.yaml \\\n        --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg \\\n        --save_path ./output/\n\n# {'input_path': '/root/.paddlex/predict_input/general_image_classification_001.jpg', 'class_ids': [296, 170, 356, 258, 248], 'scores': array([0.62817, 0.03729, 0.03262, 0.03247, 0.03196]), 'label_names': ['ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 'Irish wolfhound', 'weasel', 'Samoyed, Samoyede', 'Eskimo dog, husky']}\n</code></pre>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html","title":"PaddleX Model Pipeline Python Usage Instructions","text":"<p>Before using Python scripts for rapid inference on model pipelines, please ensure you have installed PaddleX following the PaddleX Local Installation Guide.</p>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#i-usage-example","title":"I. Usage Example","text":"<p>Taking the image classification pipeline as an example, the usage is as follows:</p> <p><pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(\"image_classification\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/res.json\")\n</code></pre> In short, there are only three steps:</p> <ul> <li>Call the <code>create_pipeline()</code> method to instantiate the prediction model pipeline object;</li> <li>Call the <code>predict()</code> method of the prediction model pipeline object for inference;</li> <li>Call <code>print()</code>, <code>save_to_xxx()</code> and other related methods to visualize or save the prediction results.</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#ii-api-description","title":"II. API Description","text":""},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#1-instantiate-the-prediction-model-pipeline-object-by-calling-create_pipeline","title":"1. Instantiate the Prediction Model Pipeline Object by Calling <code>create_pipeline()</code>","text":"<ul> <li><code>create_pipeline</code>: Instantiates the prediction model pipeline object;</li> <li>Parameters:<ul> <li><code>pipeline_name</code>: <code>str</code> type, the pipeline name or the local pipeline configuration file path, such as \"image_classification\", \"/path/to/image_classification.yaml\";</li> <li><code>device</code>: <code>str</code> type, used to set the model inference device, such as \"cpu\" or \"gpu:2\" for GPU settings;</li> <li><code>pp_option</code>: <code>PaddlePredictorOption</code> type, used to set the model inference backend;</li> </ul> </li> <li>Return Value: <code>BasePredictor</code> type.</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#2-perform-inference-by-calling-the-predict-method-of-the-prediction-model-pipeline-object","title":"2. Perform Inference by Calling the <code>predict()</code> Method of the Prediction Model Pipeline Object","text":"<ul> <li><code>predict</code>: Uses the defined prediction model pipeline to predict input data;</li> <li>Parameters:<ul> <li><code>input</code>: Any type, supporting str representing the path of the file to be predicted, or a directory containing files to be predicted, or a network URL; for CV tasks, supports numpy.ndarray representing image data; for TS tasks, supports pandas.DataFrame type data; also supports lists of the above types;</li> </ul> </li> <li>Return Value: <code>generator</code>, returns the prediction result of one sample per call;</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#3-visualize-the-prediction-results","title":"3. Visualize the Prediction Results","text":"<p>The prediction results of the model pipeline support access, visualization, and saving, which can be achieved through corresponding attributes or methods, specifically as follows:</p>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#attributes","title":"Attributes:","text":"<ul> <li><code>str</code>: <code>str</code> type representation of the prediction result;</li> <li>Return Value: <code>str</code> type, string representation of the prediction result;</li> <li><code>json</code>: Prediction result in JSON format;</li> <li>Return Value: <code>dict</code> type;</li> <li><code>img</code>: Visualization image of the prediction result;</li> <li>Return Value: <code>PIL.Image</code> type;</li> <li><code>html</code>: HTML representation of the prediction result;</li> <li>Return Value: <code>str</code> type;</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#3-visualize-the-prediction-results_1","title":"3. Visualize the Prediction Results","text":"<p>The prediction results support to be accessed, visualized, and saved, which can be achieved through corresponding attributes or methods, specifically as follows:</p>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#attributes_1","title":"Attributes:","text":"<ul> <li><code>str</code>: Representation of the prediction result in <code>str</code> type;</li> <li>Returns: A <code>str</code> type, the string representation of the prediction result.</li> <li><code>json</code>: The prediction result in JSON format;</li> <li>Returns: A <code>dict</code> type.</li> <li><code>img</code>: The visualization image of the prediction result;</li> <li>Returns: A <code>PIL.Image</code> type.</li> <li><code>html</code>: The HTML representation of the prediction result;</li> <li>Returns: A <code>str</code> type.</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#methods","title":"Methods:","text":"<ul> <li><code>print()</code>: Outputs the prediction result. Note that when the prediction result is not convenient for direct output, relevant content will be omitted;</li> <li>Parameters:<ul> <li><code>json_format</code>: <code>bool</code> type, default is <code>False</code>, indicating that json formatting is not used;</li> <li><code>indent</code>: <code>int</code> type, default is <code>4</code>, valid when <code>json_format</code> is <code>True</code>, indicating the indentation level for json formatting;</li> <li><code>ensure_ascii</code>: <code>bool</code> type, default is <code>False</code>, valid when <code>json_format</code> is <code>True</code>;</li> </ul> </li> <li>Return Value: None;</li> <li><code>save_to_json()</code>: Saves the prediction result as a JSON file. Note that when the prediction result contains data that cannot be serialized in JSON, automatic format conversion will be performed to achieve serialization and saving;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result;</li> <li><code>indent</code>: <code>int</code> type, default is <code>4</code>, valid when <code>json_format</code> is <code>True</code>, indicating the indentation level for json formatting;</li> <li><code>ensure_ascii</code>: <code>bool</code> type, default is <code>False</code>, valid when <code>json_format</code> is <code>True</code>;</li> </ul> </li> <li>Return Value: None;</li> <li><code>save_to_img()</code>: Visualizes the prediction result and saves it as an image;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_csv()</code>: Saves the prediction result as a CSV file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_html()</code>: Saves the prediction result as an HTML file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> <li><code>save_to_xlsx()</code>: Saves the prediction result as an XLSX file;</li> <li>Parameters:<ul> <li><code>save_path</code>: <code>str</code> type, the path to save the result.</li> </ul> </li> <li>Returns: None.</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#4-inference-backend-configuration","title":"4. Inference Backend Configuration","text":"<p>PaddleX supports configuring the inference backend through <code>PaddlePredictorOption</code>. Relevant APIs are as follows:</p>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#attributes_2","title":"Attributes:","text":"<ul> <li><code>device</code>: Inference device;</li> <li>Supports setting the device type and card number represented by <code>str</code>. Device types include 'gpu', 'cpu', 'npu', 'xpu', 'mlu'. When using an accelerator card, you can specify the card number, e.g., 'gpu:0' for GPU 0. The default is 'gpu:0';</li> <li>Return value: <code>str</code> type, the currently set inference device.</li> <li><code>run_mode</code>: Inference backend;</li> <li>Supports setting the inference backend as a <code>str</code> type, options include 'paddle', 'trt_fp32', 'trt_fp16', 'trt_int8', 'mkldnn', 'mkldnn_bf16'. 'mkldnn' is only selectable when the inference device is 'cpu'. The default is 'paddle';</li> <li>Return value: <code>str</code> type, the currently set inference backend.</li> <li><code>cpu_threads</code>: Number of CPU threads for the acceleration library, only valid when the inference device is 'cpu';</li> <li>Supports setting an <code>int</code> type for the number of CPU threads for the acceleration library during CPU inference;</li> <li>Return value: <code>int</code> type, the currently set number of threads for the acceleration library.</li> </ul>"},{"location":"en/pipeline_usage/instructions/pipeline_python_API.html#methods_1","title":"Methods:","text":"<ul> <li><code>get_support_run_mode</code>: Get supported inference backend configurations;</li> <li>Parameters: None;</li> <li>Return value: List type, the available inference backend configurations.</li> <li><code>get_support_device</code>: Get supported device types for running;</li> <li>Parameters: None;</li> <li>Return value: List type, the available device types.</li> <li><code>get_device</code>: Get the currently set device;</li> <li>Parameters: None;</li> <li>Return value: <code>str</code> type. ```</li> </ul>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html","title":"Face Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#1-introduction-to-the-face-recognition-pipeline","title":"1. Introduction to the Face Recognition Pipeline","text":"<p>Face recognition is a crucial component in the field of computer vision, aiming to automatically identify individuals by analyzing and comparing facial features. This task involves not only detecting faces in images but also extracting and matching facial features to find corresponding identity information in a database. Face recognition is widely used in security authentication, surveillance systems, social media, smart devices, and other scenarios.</p> <p>The face recognition pipeline is an end-to-end system dedicated to solving face detection and recognition tasks. It can quickly and accurately locate face regions in images, extract facial features, and retrieve and compare them with pre-established features in a feature database to confirm identity information.</p> <p></p> <p>The face recognition pipeline includes a face detection module and a face feature module, with several models in each module. Which models to use can be selected based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy; if you prioritize inference speed, choose models with faster inference; if you prioritize model size, choose models with smaller storage requirements.</p> <p>Face Detection Module:</p> ModelModel Download Link AP (%)Easy/Medium/Hard GPU Inference Time (ms) CPU Inference Time Model Size (M) Description BlazeFaceInference Model/Trained Model 77.7/73.4/49.5 0.447 A lightweight and efficient face detection model BlazeFace-FPN-SSHInference Model/Trained Model 83.2/80.5/60.5 52.4 73.2 0.606 Improved BlazeFace with FPN and SSH structures PicoDet_LCNet_x2_5_faceInference Model/Trained Model 93.7/90.7/68.1 33.7 185.1 28.9 Face detection model based on PicoDet_LCNet_x2_5 PP-YOLOE_plus-S_faceInference Model/Trained Model 93.9/91.8/79.8 25.8 159.9 26.5 Face detection model based on PP-YOLOE_plus-S <p>Note: The above accuracy metrics are evaluated on the WIDER-FACE validation set with an input size of 640x640. All GPU inference times are based on an NVIDIA V100 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 6271C CPU @ 2.60GHz and FP32 precision.</p> <p>Face Recognition Module:</p> ModelModel Download Link Output Feature Dimension Acc (%)AgeDB-30/CFP-FP/LFW GPU Inference Time (ms) CPU Inference Time Model Size (M) Description MobileFaceNetInference Model/Trained Model 128 96.28/96.71/99.58 5.7 101.6 4.1 Face recognition model trained on MS1Mv3 based on MobileFaceNet ResNet50_faceInference Model/Trained Model 512 98.12/98.56/99.77 8.7 200.7 87.2 Face recognition model trained on MS1Mv3 based on ResNet50 <p>Note: The above accuracy metrics are Accuracy scores measured on the AgeDB-30, CFP-FP, and LFW datasets, respectively. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX can be quickly experienced. You can experience the effects of the face recognition pipeline online or locally using command-line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#21-online-experience","title":"2.1 Online Experience","text":"<p>Oneline Experience is not supported at the moment.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#22-local-experience","title":"2.2 Local Experience","text":"<p>\u2757 Before using the facial recognition pipeline locally, please ensure that you have completed the installation of the PaddleX wheel package according to the PaddleX Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>Command line experience is not supported at the moment.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#222-integration-via-python-script","title":"2.2.2 Integration via Python Script","text":"<p>Please download the test image for testing. In the example of running this pipeline, you need to pre-build a facial feature library. You can refer to the following instructions to download the official demo data to be used for subsequent construction of the facial feature library. You can use the following command to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/face_demo_gallery.tar\ntar -xf ./face_demo_gallery.tar\n</code></pre> <p>If you wish to build a facial feature library using a private dataset, please refer to Section 2.3: Data Organization for Building a Feature Library. Afterward, you can complete the establishment of the facial feature library and quickly perform inference with the facial recognition pipeline using just a few lines of code.</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"face_recognition\")\n\nindex_data = pipeline.build_index(gallery_imgs=\"face_demo_gallery\", gallery_label=\"face_demo_gallery/gallery.txt\")\nindex_data.save(\"face_index\")\n\noutput = pipeline.predict(\"friends1.jpg\", index=index_data)\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a face recognition pipeline object. The specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the pipeline name, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available when the pipeline supports high-performance inference. <code>bool</code> <code>False</code> <p>(2) Call the <code>build_index</code> method of the face recognition pipeline object to build the facial feature library. The specific parameters are described as follows:</p> Parameter Description Type Default <code>gallery_imgs</code> Base library images to be added, supported formats: 1. <code>str</code> type representing the root directory of images, with data organization consistent with the method used when constructing the index library, refer to Section 2.3: Data Organization for Building a Feature Library; 2. <code>[numpy.ndarray, numpy.ndarray, ..]</code> type base library image data. <code>str</code>|<code>list</code> None <code>gallery_label</code> Annotation information for base library images, supported formats: 1. <code>str</code> type representing the path to the annotation file, with data organization consistent with the method used when constructing the feature library, refer to Section 2.3: Data Organization for Building a Feature Library; 2. <code>[str, str, ..]</code> type representing the annotations of base library images. <code>str</code> None <p>The feature library object <code>index</code> supports the <code>save</code> method, which is used to save the feature library to disk:</p> Parameter Description Type Default Value <code>save_path</code> The directory to save the feature library file, e.g., <code>drink_index</code>. <code>str</code> None <p>(3) Call the <code>predict</code> method of the face recognition pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing in Python variables, such as image data represented by <code>numpy.ndarray</code>. <code>str</code> Supports passing in the file path of the data to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing in the URL of the data file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing in a local directory containing the data files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing in a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for image classification tasks, and the value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing in a list, where the list elements need to be the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(4) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>(5) Process the prediction results: The prediction result for each sample is of type <code>dict</code>, and it supports printing or saving to a file. The supported file types depend on the specific pipeline, such as:</p> Method Description Method Parameters print Print results to the terminal <code>- format_json</code>: Boolean, whether to format the output with JSON indentation, default is True; <code>- indent</code>: Integer, JSON formatting setting, effective only when format_json is True, default is 4; <code>- ensure_ascii</code>: Boolean, JSON formatting setting, effective only when format_json is True, default is False; save_to_json Save results as a JSON file <code>- save_path</code>: String, file path for saving; if it's a directory, the saved file name matches the input file name; <code>- indent</code>: Integer, JSON formatting setting, default is 4; <code>- ensure_ascii</code>: Boolean, JSON formatting setting, default is False; save_to_img Save results as an image file <code>- save_path</code>: String, file path for saving; if it's a directory, the saved file name matches the input file name; <p>If you have obtained the configuration file, you can customize various settings of the facial recognition pipeline by simply modifying the <code>pipeline</code> parameter value in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/face_recognition.yaml</code>, you just need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/face_recognition.yaml\", index=\"face_index\")\n\noutput = pipeline.predict(\"friends1.jpg\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#223-adding-and-deleting-operations-in-the-face-feature-library","title":"2.2.3 Adding and Deleting Operations in the Face Feature Library","text":"<p>If you wish to add more face images to the feature library, you can call the <code>append_index</code> method; to delete face image features, you can call the <code>remove_index</code> method.</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"face_recognition\")\n\nindex_data = pipeline.build_index(gallery_imgs=\"face_demo_gallery\", gallery_label=\"face_demo_gallery/gallery.txt\", index_type=\"IVF\", metric_type=\"IP\")\nindex_data = pipeline.append_index(gallery_imgs=\"face_demo_gallery\", gallery_label=\"face_demo_gallery/gallery.txt\", index=index_data)\nindex_data = pipeline.remove_index(remove_ids=\"face_demo_gallery/remove_ids.txt\", index=index_data)\nindex_data.save(\"face_index\")\n</code></pre> <p>The <code>add_index</code> method parameters are described as follows:</p> Parameter Description Type Default <code>gallery_imgs</code> Base library images to be added, supported formats: 1. <code>str</code> type representing the root directory of images, with data organization consistent with the method used when constructing the index library, refer to Section 2.3: Data Organization for Building a Feature Library; 2. <code>[numpy.ndarray, numpy.ndarray, ..]</code> type base library image data. <code>str</code>|<code>list</code> None <code>gallery_label</code> Annotation information for base library images, supported formats: 1. <code>str</code> type representing the path to the annotation file, with data organization consistent with the method used when constructing the feature library, refer to Section 2.3: Data Organization for Building a Feature Library; 2. <code>[str, str, ..]</code> type representing the annotations of base library images. <code>str</code>|<code>list</code> None <code>remove_ids</code> Index numbers to be removed, supported formats: 1. <code>str</code> type representing the path of a txt file, with content being the IDs of indexes to be removed, one \"id\" per line; 2. <code>[int, int, ..]</code> type representing the index numbers to be removed. Only effective in <code>remove_index</code>. <code>str</code>|<code>list</code> None <code>index</code> Feature library, supported formats: 1. The path to the directory containing the feature library files (<code>vector.index</code> and <code>index_info.yaml</code>); 2. <code>IndexData</code> type feature library object, only effective in <code>append_index</code> and <code>remove_index</code>, representing the feature library to be modified. <code>str</code>|<code>IndexData</code> None <code>index_type</code> Supports <code>HNSW32</code>, <code>IVF</code>, <code>Flat</code>. Among them, <code>HNSW32</code> offers fast retrieval speed and high accuracy, but does not support the <code>remove_index()</code> operation; <code>IVF</code> offers fast retrieval speed but relatively lower accuracy, supporting both <code>append_index()</code> and <code>remove_index()</code> operations; <code>Flat</code> offers lower retrieval speed but higher accuracy, supporting both <code>append_index()</code> and <code>remove_index()</code> operations. <code>str</code> <code>HNSW32</code> <code>metric_type</code> Supports: <code>IP</code>, Inner Product; <code>L2</code>, Euclidean Distance. <code>str</code> <code>IP</code>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#23-data-organization-for-feature-library-construction","title":"2.3 Data Organization for Feature Library Construction","text":"<p>The face recognition pipeline example in PaddleX requires a pre-constructed feature library for face feature retrieval. If you wish to build a face feature library with private data, you need to organize the data as follows:</p> <pre><code>data_root             # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images            # Directory for saving images, the directory name can be changed\n\u2502   \u251c\u2500\u2500 ID0           # Identity ID name, preferably meaningful, such as a person's name\n\u2502   \u2502   \u251c\u2500\u2500 xxx.jpg   # Image, nested directories are supported\n\u2502   \u2502   \u251c\u2500\u2500 xxx.jpg   # Image, nested directories are supported\n\u2502   \u2502       ...\n\u2502   \u251c\u2500\u2500 ID1           # Identity ID name, preferably meaningful, such as a person's name\n\u2502   \u2502   \u251c\u2500\u2500 xxx.jpg   # Image, nested directories are supported\n\u2502   \u2502   \u251c\u2500\u2500 xxx.jpg   # Image, nested directories are supported\n\u2502   \u2502       ...\n\u2502       ...\n\u2514\u2500\u2500 gallery.txt       # Annotation file for the feature library dataset, the file name cannot be changed. Each line gives the path of the face image to be retrieved and the image feature label, separated by a space. Example content: images/Chandler/Chandler00037.jpg Chandler\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the face recognition pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to directly apply the face recognition pipeline in your Python project, you can refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API reference and multi-language service invocation examples:</p> API Reference <p>The main operations provided by the service are as follows:</p> <ul> <li><code>buildIndex</code></li> </ul> <p>Build feature vector index.</p> <p><code>POST /face-recognition-index-build</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Description Required <code>imageLabelPairs</code> <code>array</code> Image-label pairs used to build the index. Yes <p>Each element in <code>imageLabelPairs</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>image</code> <code>string</code> The URL of the image file accessible to the service or the Base64 encoded content of the image file. <code>label</code> <code>string</code> Label. <ul> <li>When the request is successfully processed, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>indexKey</code> <code>string</code> The key corresponding to the index, used to identify the established index. Can be used as input for other operations. <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>addImagesToIndex</code></li> </ul> <p>Add images (corresponding feature vectors) to the index.</p> <p><code>POST /face-recognition-index-add</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Description Required <code>imageLabelPairs</code> <code>array</code> Image-label pairs used to build the index. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. Yes <p>Each element in <code>imageLabelPairs</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>image</code> <code>string</code> The URL of the image file accessible to the service or the Base64 encoded content of the image file. <code>label</code> <code>string</code> Label. <ul> <li>When the request is successfully processed, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>removeImagesFromIndex</code></li> </ul> <p>Remove images (corresponding feature vectors) from the index.</p> <p><code>POST /face-recognition-index-remove</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Description Required <code>ids</code> <code>array</code> IDs of vectors to be removed from the index. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. Yes <ul> <li>When the request is successfully processed, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>infer</code></li> </ul> <p>Perform image recognition.</p> <p><code>POST /face-recognition-infer</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of the image file accessible to the service or the Base64 encoded content of the image file. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. No <ul> <li>When the request is successfully processed, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>faces</code> <code>array</code> Information about detected faces. <code>image</code> <code>string</code> Recognition result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>faces</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> Face target position. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner, in order. <code>recResults</code> <code>array</code> Recognition results. <code>score</code> <code>number</code> Detection score. <p>Each element in <code>recResults</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>label</code> <code>string</code> Label. <code>score</code> <code>number</code> Recognition score. Multilingual Service Invocation Examples <pre><code>import base64\nimport pprint\nimport sys\n\nimport requests\n\nAPI_BASE_URL = \"http://0.0.0.0:8080\"\n\nbase_image_label_pairs = [\n    {\"image\": \"./demo0.jpg\", \"label\": \"ID0\"},\n    {\"image\": \"./demo1.jpg\", \"label\": \"ID1\"},\n    {\"image\": \"./demo2.jpg\", \"label\": \"ID2\"},\n]\nimage_label_pairs_to_add = [\n    {\"image\": \"./demo3.jpg\", \"label\": \"ID2\"},\n]\nids_to_remove = [1]\ninfer_image_path = \"./demo4.jpg\"\noutput_image_path = \"./out.jpg\"\n\nfor pair in base_image_label_pairs:\n    with open(pair[\"image\"], \"rb\") as file:\n        image_bytes = file.read()\n        image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n    pair[\"image\"] = image_data\n\npayload = {\"imageLabelPairs\": base_image_label_pairs}\nresp_index_build = requests.post(f\"{API_BASE_URL}/face-recognition-index-build\", json=payload)\nif resp_index_build.status_code != 200:\n    print(f\"Request to face-recognition-index-build failed with status code {resp_index_build}.\")\n    pprint.pp(resp_index_build.json())\n    sys.exit(1)\nresult_index_build = resp_index_build.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_build['idMap'])}\")\n\nfor pair in image_label_pairs_to_add:\n    with open(pair[\"image\"], \"rb\") as file:\n        image_bytes = file.read()\n        image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n    pair[\"image\"] = image_data\n\npayload = {\"imageLabelPairs\": image_label_pairs_to_add, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_index_add = requests.post(f\"{API_BASE_URL}/face-recognition-index-add\", json=payload)\nif resp_index_add.status_code != 200:\n    print(f\"Request to face-recognition-index-add failed with status code {resp_index_add}.\")\n    pprint.pp(resp_index_add.json())\n    sys.exit(1)\nresult_index_add = resp_index_add.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_add['idMap'])}\")\n\npayload = {\"ids\": ids_to_remove, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_index_remove = requests.post(f\"{API_BASE_URL}/face-recognition-index-remove\", json=payload)\nif resp_index_remove.status_code != 200:\n    print(f\"Request to face-recognition-index-remove failed with status code {resp_index_remove}.\")\n    pprint.pp(resp_index_remove.json())\n    sys.exit(1)\nresult_index_remove = resp_index_remove.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_remove['idMap'])}\")\n\nwith open(infer_image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_infer = requests.post(f\"{API_BASE_URL}/face-recognition-infer\", json=payload)\nif resp_infer.status_code != 200:\n    print(f\"Request to face-recogntion-infer failed with status code {resp_infer}.\")\n    pprint.pp(resp_infer.json())\n    sys.exit(1)\nresult_infer = resp_infer.json()[\"result\"]\n\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result_infer[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected faces:\")\npprint.pp(result_infer[\"faces\"])\n</code></pre> <p> </p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method where computing and data processing functions are placed on the user's device itself, allowing the device to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide. You can choose an appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the Face Recognition Pipeline do not meet your expectations in terms of accuracy or speed for your specific scenario, you can try to further fine-tune the existing models using your own domain-specific or application-specific data to enhance the recognition performance of the pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the Face Recognition Pipeline consists of two modules (face detection and face recognition), the suboptimal performance of the pipeline may stem from either module.</p> <p>You can analyze images with poor recognition results. If you find that many faces are not detected during the analysis, it may indicate deficiencies in the face detection model. In this case, you need to refer to the Custom Development section in the Face Detection Module Development Tutorial and use your private dataset to fine-tune the face detection model. If matching errors occur in detected faces, it suggests that the face feature model needs further improvement. You should refer to the Custom Development section in the Face Feature Module Development Tutorial to fine-tune the face feature model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After completing fine-tuning training with your private dataset, you will obtain local model weight files.</p> <p>To use the fine-tuned model weights, you only need to modify the pipeline configuration file by replacing the local paths of the fine-tuned model weights with the corresponding paths in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  device: \"gpu:0\"\n  det_model: \"BlazeFace\"        # Can be modified to the local path of the fine-tuned face detection model\n  rec_model: \"MobileFaceNet\"    # Can be modified to the local path of the fine-tuned face recognition model\n  det_batch_size: 1\n  rec_batch_size: 1\n  device: gpu\n......\n</code></pre> Subsequently, refer to the command-line method or Python script method in 2.2 Local Experience to load the modified pipeline configuration file. Note: Currently, setting separate <code>batch_size</code> for face detection and face recognition models is not supported.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/face_recognition.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modifying the <code>--device</code> parameter allows seamless switching between different hardware.</p> <p>For example, when running the face recognition pipeline using Python and changing the running device from an NVIDIA GPU to an Ascend NPU, you only need to modify the <code>device</code> in the script to <code>npu</code>:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"face_recognition\",\n    device=\"npu:0\" # gpu:0 --&gt; npu:0\n)\n</code></pre> If you want to use the face recognition pipeline on more types of hardware, please refer to the PaddleX Multi-device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html","title":"General Image Recognition Pipeline Usage Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#1-introduction-to-the-general-image-recognition-pipeline","title":"1. Introduction to the General Image Recognition Pipeline","text":"<p>The General Image Recognition Pipeline aims to solve the problem of open-domain object localization and recognition. Currently, PaddleX's General Image Recognition Pipeline supports PP-ShiTuV2.</p> <p>PP-ShiTuV2 is a practical general image recognition system mainly composed of three modules: mainbody detection module, image feature module, and vector retrieval module. The system integrates and improves various strategies in multiple aspects, including backbone network, loss function, data augmentation, learning rate scheduling, regularization, pre-trained model, and model pruning and quantization. It optimizes each module and ultimately achieves better performance in multiple application scenarios.</p> <p></p> <p>The General Image Recognition Pipeline includes the mainbody detection module and the image feature module, with several models to choose. You can select the model to use based on the benchmark data below. If you prioritize model precision, choose a model with higher precision. If you prioritize inference speed, choose a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p> <p>Object Detection Module:</p> Model mAP(0.5:0.95) mAP(0.5) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-ShiTuV2_det 41.5 62.0 33.7 537.0 27.54 An mainbody detection model based on PicoDet_LCNet_x2_5, which may detect multiple common objects simultaneously. <p>Note: The above accuracy metrics are based on the private mainbody detection dataset.</p> <p>Image Feature Module:</p> Model Recall@1 (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-ShiTuV2_rec 84.2 5.23428 19.6005 16.3 M PP-ShiTuV2 is a general image feature system consisting of three modules: mainbody detection, feature extraction, and vector retrieval. These models are part of the feature extraction module, and different models can be selected based on system requirements. PP-ShiTuV2_rec_CLIP_vit_base 88.69 13.1957 285.493 306.6 M PP-ShiTuV2_rec_CLIP_vit_large 91.03 51.1284 1131.28 1.05 G <p>Note: The above accuracy metrics are based on AliProducts Recall@1. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX can be quickly experienced. You can use Python to experience locally.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#21-online-experience","title":"2.1 Online Experience","text":"<p>Not supported yet.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#22-local-experience","title":"2.2 Local Experience","text":"<p>\u2757 Before using the General Image Recognition Pipeline locally, please ensure you have installed the PaddleX wheel package according to the PaddleX Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>The pipeline does not support command line experience at this time.</p> <p>By default, the built-in General Image Recognition Pipeline configuration file is used. If you want to change it, you can run the following command to obtain:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config PP-ShiTuV2\n</code></pre> <p>After execution, the General Image Recognition Pipeline configuration file will be saved in the current directory. If you want to customize the save location, you can run the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config PP-ShiTuV2 --save_path ./my_path\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<ul> <li>In the example of using this pipeline, a feature vector library needs to be built beforehand. You can download the officially provided drink recognition test dataset drink_dataset_v2.0 to build the feature vector library. If you want to use a private dataset, you can refer to Section 2.3 Data Organization for Building the Feature Library. After that, you can quickly build the feature vector library and predict using the General Image Recognition Pipeline with just a few lines of code.</li> </ul> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"PP-ShiTuV2\")\n\npipeline.build_index(data_root=\"drink_dataset_v2.0/\", index_dir=\"index_dir\")\n\noutput = pipeline.predict(\"./drink_dataset_v2.0/test_images/\", index_dir=\"index_dir\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n\n````\n\nIn the above Python script, the following steps are executed:\n\n(1) Call the `create_pipeline` function to create a general image recognition pipeline object. The specific parameter descriptions are as follows:\n\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Parameter&lt;/th&gt;\n&lt;th&gt;Parameter Description&lt;/th&gt;\n&lt;th&gt;Parameter Type&lt;/th&gt;\n&lt;th&gt;Default Value&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;pipeline&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX.&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;None&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;index_dir&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The directory where the retrieval database files used for pipeline inference are located. If this parameter is not passed, &lt;code&gt;index_dir&lt;/code&gt; needs to be specified in &lt;code&gt;predict()&lt;/code&gt;.&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;None&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;device&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The inference device for the pipeline model. Supports: \"gpu\", \"cpu\".&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;gpu&lt;/code&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;use_hpip&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;Whether to enable high-performance inference, which is only available when the pipeline supports it.&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n(2) Call the `build_index` function of the general image recognition pipeline object to build the feature vector library. The specific parameters are described as follows:\n\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Parameter&lt;/th&gt;\n&lt;th&gt;Parameter Description&lt;/th&gt;\n&lt;th&gt;Parameter Type&lt;/th&gt;\n&lt;th&gt;Default Value&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;data_root&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The root directory of the dataset. The data organization method refers to &lt;a href=\"#2.3-Data-Organization-for-Building-the-Feature-Library\"&gt;Section 2.3 Data Organization for Building the Feature Library&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;None&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;index_dir&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The save path for the feature library. After successfully calling the &lt;code&gt;build_index&lt;/code&gt; function, two files will be generated in this path: &lt;code&gt;\"id_map.pkl\"&lt;/code&gt; saves the mapping relationship between image IDs and image feature labels; &lt;code&gt;\"vector.index\"&lt;/code&gt; stores the feature vectors of each image.&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;None&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n(3) Call the `predict` function of the general image recognition pipeline object for inference prediction: The `predict` function parameter is `input`, which is used to input the data to be predicted, supporting multiple input methods. Specific examples are as follows:\n\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Parameter Type&lt;/th&gt;\n&lt;th&gt;Parameter Description&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Python Var&lt;/td&gt;\n&lt;td&gt;Supports directly passing in Python variables, such as &lt;code&gt;numpy.ndarray&lt;/code&gt; representing image data.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;str&lt;/td&gt;\n&lt;td&gt;Supports passing in the file path of the data to be predicted, such as the local path of an image file: &lt;code&gt;/root/data/img.jpg&lt;/code&gt;.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;str&lt;/td&gt;\n&lt;td&gt;Supports passing in the URL of the data file to be predicted, such as the network URL of an image file: &lt;a href=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/yuanqisenlin.jpeg\"&gt;Example&lt;/a&gt;.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;str&lt;/td&gt;\n&lt;td&gt;Supports passing in a local directory that contains the data files to be predicted, such as the local path: &lt;code&gt;/root/data/&lt;/code&gt;.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;dict&lt;/td&gt;\n&lt;td&gt;Supports passing in a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: &lt;code&gt;{\"img\": \"/root/data1\"}&lt;/code&gt;.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;list&lt;/td&gt;\n&lt;td&gt;Supports passing in a list, where the elements of the list need to be the above types of data, such as &lt;code&gt;[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]&lt;/code&gt;, &lt;code&gt;[\"/root/data1\", \"/root/data2\"]&lt;/code&gt;, &lt;code&gt;[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]&lt;/code&gt;.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\nAdditionally, the `predict` method supports the `index_dir` parameter for setting the retrieval database:\n\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Parameter Type&lt;/th&gt;\n&lt;th&gt;Parameter Description&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;code&gt;index_dir&lt;/code&gt;&lt;/td&gt;\n&lt;td&gt;The directory where the retrieval database files used for pipeline inference are located. If this parameter is not passed, the default retrieval database specified through the &lt;code&gt;index_dir&lt;/code&gt; parameter in &lt;code&gt;create_pipeline()&lt;/code&gt; will be used.&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n(4) Obtain the prediction results by calling the `predict` method: The `predict` method is a `generator`, so prediction results need to be obtained by iteration. The `predict` method predicts data in batches.\n\n(5) Process the prediction results: The prediction result for each sample is of `dict` type and supports printing or saving to a file. The supported save types are related to the specific pipeline, such as:\n\n&lt;table&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Method&lt;/th&gt;\n&lt;th&gt;Description&lt;/th&gt;\n&lt;th&gt;Method Parameters&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;print&lt;/td&gt;\n&lt;td&gt;Print the results to the terminal&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;- format_json&lt;/code&gt;: bool type, whether to use json indentation formatting for the output content, default is True;&lt;br&gt;&lt;code&gt;- indent&lt;/code&gt;: int type, json formatting setting, only effective when format_json is True, default is 4;&lt;br&gt;&lt;code&gt;- ensure_ascii&lt;/code&gt;: bool type, json formatting setting, only effective when format_json is True, default is False;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;save_to_json&lt;/td&gt;\n&lt;td&gt;Save the results as a json-formatted file&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;- save_path&lt;/code&gt;: str type, the save file path. When it is a directory, the saved file naming is consistent with the input file type naming;&lt;br&gt;&lt;code&gt;- indent&lt;/code&gt;: int type, json formatting setting, default is 4;&lt;br&gt;&lt;code&gt;- ensure_ascii&lt;/code&gt;: bool type, json formatting setting, default is False;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;save_to_img&lt;/td&gt;\n&lt;td&gt;Save the results as an image-formatted file&lt;/td&gt;\n&lt;td&gt;&lt;code&gt;- save_path&lt;/code&gt;: str type, the save file path. When it is a directory, the saved file naming is consistent with the input file type naming;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\nIf you have a configuration file, you can customize the configurations for the general image recognition pipeline by modifying the `pipeline` parameter value in the `create_pipeline` method to the path of the pipeline configuration file.\n\nFor example, if your configuration file is saved at `./my_path/PP-ShiTuV2.yaml`, you only need to execute:\n\n```python\nfrom paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/PP-ShiTuV2.yaml\", index_dir=\"index_dir\")\n\noutput = pipeline.predict(\"./drink_dataset_v2.0/test_images/\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#223-add-or-remove-features-from-the-feature-library","title":"2.2.3 Add or Remove Features from the Feature Library","text":"<p>If you want to add more images to the feature library, you can call the <code>append_index</code> function; to remove image features, you can call the <code>remove_index</code> function.</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\"PP-ShiTuV2\")\npipeline.build_index(data_root=\"drink_dataset_v2.0/\", index_dir=\"index_dir\", index_type=\"IVF\")\npipeline.append_index(data_root=\"drink_dataset_v2.0/\", index_dir=\"index_dir\", index_type=\"IVF\")\npipeline.remove_index(data_root=\"drink_dataset_v2.0/\", index_dir=\"index_dir\", index_type=\"IVF\")\n</code></pre> <p>The parameter descriptions for the above methods are as follows:</p> Parameter Description Type Default Value <code>data_root</code> The root directory of the dataset to be added. The data organization should be the same as when building the feature library, refer to Section 2.3 Data Organization for Building the Feature Library <code>str</code> None <code>index_dir</code> The storage directory for the feature library. In <code>append_index</code> and <code>remove_index</code>, it is also the path of the feature library to be modified (or deleted). <code>str</code> None <code>index_type</code> Supports <code>HNSW32</code>, <code>IVF</code>, <code>Flat</code>. Among them, <code>HNSW32</code> has faster retrieval speed and higher accuracy but does not support the <code>remove_index()</code> operation; <code>IVF</code> has faster retrieval speed but relatively lower accuracy, and supports <code>append_index()</code> and <code>remove_index()</code> operations; <code>Flat</code> has lower retrieval speed but higher accuracy, and supports <code>append_index()</code> and <code>remove_index()</code> operations. <code>str</code> <code>HNSW32</code> <code>metric_type</code> Supports: <code>IP</code>, Inner Product; <code>L2</code>, Euclidean Distance. <code>str</code> <code>IP</code>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#23-data-organization-for-building-the-feature-library","title":"2.3 Data Organization for Building the Feature Library","text":"<p>The PaddleX general image recognition pipeline requires a pre-built feature library for feature retrieval. If you want to build a feature vector library with private data, you need to organize the data as follows:</p> <pre><code>data_root             # Root directory of the dataset, the directory name can be changed\n\u251c\u2500\u2500 images            # Directory for saving images, the directory name can be changed\n\u2502   \u2502   ...\n\u2514\u2500\u2500 gallery.txt       # Annotation file for the feature library dataset, the file name cannot be changed. Each line gives the path of the image to be retrieved and the image label, separated by a space, for example: \u201c0/0.jpg label\u201d\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the pipeline directly in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Meaning <code>errorCode</code> <code>integer</code> Error code. Fixed to <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed to <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property, which is an <code>object</code> type that stores operation result information.</p> <ul> <li>When the request is not processed successfully, the properties of the response body are as follows:</li> </ul> Name Type Meaning <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>The main operations provided by the service are as follows:</p> <ul> <li><code>buildIndex</code></li> </ul> <p>Build feature vector index.</p> <p><code>POST /shitu-index-build</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Meaning Required <code>imageLabelPairs</code> <code>array</code> Image-label pairs for building the index. Yes <p>Each element in <code>imageLabelPairs</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>image</code> <code>string</code> The URL of an image file accessible by the service, or the Base64 encoding result of the image file content. <code>label</code> <code>string</code> Label. <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>indexKey</code> <code>string</code> The key corresponding to the index, used to identify the established index. Can be used as input for other operations. <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>addImagesToIndex</code></li> </ul> <p>Add images (corresponding feature vectors) to the index.</p> <p><code>POST /shitu-index-add</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Meaning Required <code>imageLabelPairs</code> <code>array</code> Image-label pairs for building the index. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. Yes <p>Each element in <code>imageLabelPairs</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>image</code> <code>string</code> The URL of an image file accessible by the service, or the Base64 encoding result of the image file content. <code>label</code> <code>string</code> Label. <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>removeImagesFromIndex</code></li> </ul> <p>Remove images (corresponding feature vectors) from the index.</p> <p><code>POST /shitu-index-remove</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Meaning Required <code>ids</code> <code>array</code> IDs of the vectors to be removed from the index. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>idMap</code> <code>object</code> Mapping from vector ID to label. <ul> <li><code>infer</code></li> </ul> <p>Perform image recognition.</p> <p><code>POST /shitu-infer</code></p> <ul> <li>The properties of the request body are as follows:</li> </ul> Name Type Meaning Required <code>image</code> <code>string</code> The URL of an image file accessible by the service, or the Base64 encoding result of the image file content. Yes <code>indexKey</code> <code>string</code> The key corresponding to the index. Provided by the <code>buildIndex</code> operation. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Meaning <code>detectedObjects</code> <code>array</code> Information of the detected targets. <code>image</code> <code>string</code> Recognition result image. The image is in JPEG format, encoded with Base64. <p>Each element in <code>detectedObjects</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>bbox</code> <code>array</code> Target location. The elements in the array are the x-coordinate of the upper-left corner, the y-coordinate of the upper-left corner, the x-coordinate of the lower-right corner, and the y-coordinate of the lower-right corner, respectively. <code>recResults</code> <code>array</code> Recognition results. <code>score</code> <code>number</code> Detection score. <p>Each element in <code>recResults</code> is an <code>object</code> with the following properties:</p> Name Type Meaning <code>label</code> <code>string</code> Label. <code>score</code> <code>number</code> Recognition score. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport pprint\nimport sys\n\nimport requests\n\nAPI_BASE_URL = \"http://0.0.0.0:8080\"\n\nbase_image_label_pairs = [\n    {\"image\": \"./demo0.jpg\", \"label\": \"\u200b\u5154\u5b50\u200b\"},\n    {\"image\": \"./demo1.jpg\", \"label\": \"\u200b\u5154\u5b50\u200b\"},\n    {\"image\": \"./demo2.jpg\", \"label\": \"\u200b\u5c0f\u72d7\u200b\"},\n]\nimage_label_pairs_to_add = [\n    {\"image\": \"./demo3.jpg\", \"label\": \"\u200b\u5c0f\u72d7\u200b\"},\n]\nids_to_remove = [1]\ninfer_image_path = \"./demo4.jpg\"\noutput_image_path = \"./out.jpg\"\n\nfor pair in base_image_label_pairs:\n    with open(pair[\"image\"], \"rb\") as file:\n        image_bytes = file.read()\n        image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n    pair[\"image\"] = image_data\n\npayload = {\"imageLabelPairs\": base_image_label_pairs}\nresp_index_build = requests.post(f\"{API_BASE_URL}/shitu-index-build\", json=payload)\nif resp_index_build.status_code != 200:\n    print(f\"Request to shitu-index-build failed with status code {resp_index_build}.\")\n    pprint.pp(resp_index_build.json())\n    sys.exit(1)\nresult_index_build = resp_index_build.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_build['idMap'])}\")\n\nfor pair in image_label_pairs_to_add:\n    with open(pair[\"image\"], \"rb\") as file:\n        image_bytes = file.read()\n        image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n    pair[\"image\"] = image_data\n\npayload = {\"imageLabelPairs\": image_label_pairs_to_add, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_index_add = requests.post(f\"{API_BASE_URL}/shitu-index-add\", json=payload)\nif resp_index_add.status_code != 200:\n    print(f\"Request to shitu-index-add failed with status code {resp_index_add}.\")\n    pprint.pp(resp_index_add.json())\n    sys.exit(1)\nresult_index_add = resp_index_add.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_add['idMap'])}\")\n\npayload = {\"ids\": ids_to_remove, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_index_remove = requests.post(f\"{API_BASE_URL}/shitu-index-remove\", json=payload)\nif resp_index_remove.status_code != 200:\n    print(f\"Request to shitu-index-remove failed with status code {resp_index_remove}.\")\n    pprint.pp(resp_index_remove.json())\n    sys.exit(1)\nresult_index_remove = resp_index_remove.json()[\"result\"]\nprint(f\"Number of images indexed: {len(result_index_remove['idMap'])}\")\n\nwith open(infer_image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data, \"indexKey\": result_index_build[\"indexKey\"]}\nresp_infer = requests.post(f\"{API_BASE_URL}/shitu-infer\", json=payload)\nif resp_infer.status_code != 200:\n    print(f\"Request to shitu-infer failed with status code {resp_infer}.\")\n    pprint.pp(resp_infer.json())\n    sys.exit(1)\nresult_infer = resp_infer.json()[\"result\"]\n\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result_infer[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected objects:\")\npprint.pp(result_infer[\"detectedObjects\"])\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method for your model pipeline based on your needs and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Image Recognition Pipeline do not meet your expectations in terms of precision or speed. You can further fine-tune the existing models using your own data from specific domains or application scenarios to enhance the recognition performance of the pipeline in your context.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-Tuning","text":"<p>Since the General Image Recognition Pipeline consists of two modules (the mainbody detection module and the image feature module), the suboptimal performance of the pipeline may stem from either module.</p> <p>You can analyze images with poor recognition results. After analysising, if you find that many mainbody objects are not detected, it may indicate deficiencies in the mainbody detection model. You need to refer to the Custom Development section in the Object Detection Module Development Tutorial and use your private dataset to fine-tune the mainbody detection model. If there are mismatches in the detected mainbody objects, it suggests that the image feature model requires further improvement. You should refer to the Custom Development section in the Image Feature Module Development Tutorial and fine-tune the image feature model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After you complete the fine-tuning training with your private dataset, you will obtain local model files.</p> <p>To use the fine-tuned model, you only need to modify the pipeline configuration file by replacing with the paths to your fine-tuned model:</p> <p><pre><code>Pipeline:\n  device: \"gpu:0\"\n  det_model: \"./PP-ShiTuV2_det_infer/\"        # Can be modified to the local path of the fine-tuned mainbody detection model\n  rec_model: \"./PP-ShiTuV2_rec_infer/\"        # Can be modified to the local path of the fine-tuned image feature model\n  det_batch_size: 1\n  rec_batch_size: 1\n  device: gpu\n</code></pre> Subsequently, refer to the command-line method or Python script method in 2.2 Local Experience to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply by modifying the <code>--device</code> parameter, seamless switching between different hardware can be achieved.</p> <p>For example, when running the General Image Recognition Pipeline using Python and changing the running device from an NVIDIA GPU to an Ascend NPU, you only need to modify the <code>device</code> in the script to <code>npu</code>:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"PP-ShiTuV2\",\n    device=\"npu:0\" # gpu:0 --&gt; npu:0\n)\n</code></pre> <p>If you want to use the General Image Recognition Pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html","title":"Image Anomaly Detection Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#1-introduction-to-image-anomaly-detection-pipeline","title":"1. Introduction to Image Anomaly Detection Pipeline","text":"<p>Image anomaly detection is an image processing technique that identifies unusual or non-conforming patterns within images through analysis. It is widely applied in industrial quality inspection, medical image analysis, and security monitoring. By leveraging machine learning and deep learning algorithms, image anomaly detection can automatically recognize potential defects, anomalies, or abnormal behaviors in images, enabling us to promptly identify issues and take corresponding actions. The image anomaly detection system is designed to automatically detect and mark anomalies in images, enhancing work efficiency and accuracy.</p> <p></p> <p>The image anomaly detection pipeline includes an unsupervised anomaly detection module, with the following model benchmarks:</p> Model NameModel Download Link Avg (%) Model Size (M) STFPMInference Model/Trained Model 96.2 21.5 M <p>Note: The above accuracy metrics are the average anomaly scores on the MVTec AD validation set. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX provides pre-trained models for the anomaly detection pipeline, allowing for quick experience of its effects. You can use the command line or Python to experience the image anomaly detection pipeline locally.</p> <p>Before using the image anomaly detection pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline anomaly_detection --input uad_grid.png --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it's the image anomaly detection pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). CPU can also be selected (--device cpu).\n</code></pre> <p>When executing the above command, the default image anomaly detection pipeline configuration file is loaded. If you need to customize the configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config anomaly_detection\n</code></pre> <p>After execution, the image anomaly detection pipeline configuration file will be saved in the current directory. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config anomaly_detection --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./anomaly_detection.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./anomaly_detection.yaml --input uad_grid.png --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <p><pre><code>{'input_path': 'uad_grid.png', 'pred': '...'}\n</code></pre> </p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>A few lines of code are sufficient for quick inference using the pipeline. Taking the image anomaly detection pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"anomaly_detection\")\n\noutput = pipeline.predict(\"uad_grid.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n    res.save_to_json(\"./output/\")\n</code></pre> <p>The results obtained are the same as those from the command line approach.</p> <p>In the above Python script, the following steps are executed:</p> <p>\uff081\uff09Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it's a pipeline name, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the pipeline supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the pipeline object for inference prediction: The <code>predict</code> method takes <code>x</code> as its parameter, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. str Supports passing the path to the data file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing the URL of the data file to be predicted, such as the network URL of an image file: Example. str Supports passing a local directory, which should contain the data files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing a dictionary type, where the key needs to correspond to the specific task, e.g., \"img\" for image classification tasks, and the value of the dictionary supports the above data types, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing a list, where the list elements need to be of the above data types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/anomaly_detection.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/anomaly_detection.yaml\")\noutput = pipeline.predict(\"uad_grid.png\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualized image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the pipeline directly in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs anomaly detection on images.</p> <p><code>POST /image-anomaly-detection</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of the image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>labelMap</code> <code>array</code> Records the class label of each pixel in the image (arranged in row-major order), where <code>255</code> represents an anomaly point, and <code>0</code> represents a non-anomaly point. <code>size</code> <code>array</code> Image shape. The elements in the array are the height and width of the image in order. <code>image</code> <code>string</code> Anomaly detection result image. The image is in JPEG format and encoded in Base64. <p>Example of <code>result</code>:</p> <pre><code>{\n\"labelMap\": [\n0,\n0,\n255,\n0\n],\n\"size\": [\n2,\n2\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/image-anomaly-detection\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/image-anomaly-detection\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/image-anomaly-detection\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode labelMap = result.get(\"labelMap\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/image-anomaly-detection\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Labelmap []map[string]interface{} `json:\"labelMap\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/image-anomaly-detection\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/image-anomaly-detection'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/image-anomaly-detection\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, enabling devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method for your model pipeline based on your needs and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the image anomaly detection pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the image anomaly detection pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the image anomaly detection pipeline includes an unsupervised image anomaly detection module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Unsupervised Anomaly Detection Module Tutorial and use your private dataset to fine-tune the image anomaly detection model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weights files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding position in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: STFPM   # Can be modified to the local path of the fine-tuned model\n  batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Then, refer to the command line or Python script methods in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference with the image anomaly detection pipeline, the Python command is:</p> <p><pre><code>paddlex --pipeline anomaly_detection --input uad_grid.png --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline anomaly_detection --input uad_grid.png --device npu:0\n</code></pre> If you want to use the image anomaly detection pipeline on more types of hardware, please refer to the PaddleX Multi-device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html","title":"General Image Classification Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#1-introduction-to-the-general-image-classification-pipeline","title":"1. Introduction to the General Image Classification Pipeline","text":"<p>Image classification is a technique that assigns images to predefined categories. It is widely applied in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification.</p> <p></p> <p>The General Image Classification Pipeline includes an image classification module. If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p> ModelModel Download Link Top1 Acc(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Storage Size (M) CLIP_vit_base_patch16_224Inference Model/Trained Model 85.36 13.1957 285.493 306.5 M MobileNetV3_small_x1_0Inference Model/Trained Model 68.2 6.00993 12.9598 10.5 M PP-HGNet_smallInference Model/Trained Model 81.51 5.50661 119.041 86.5 M PP-HGNetV2-B0Inference Model/Trained Model 77.77 6.53694 23.352 21.4 M PP-HGNetV2-B4Inference Model/Trained Model 83.57 9.66407 54.2462 70.4 M PP-HGNetV2-B6Inference Model/Trained Model 86.30 21.226 255.279 268.4 M PP-LCNet_x1_0Inference Model/Trained Model 71.32 3.84845 9.23735 10.5 M ResNet50Inference Model/Trained Model 76.5 9.62383 64.8135 90.8 M SwinTransformer_tiny_patch4_window7_224Inference Model/Trained Model 81.10 8.54846 156.306 100.1 M <p>\u2757 The above list features the 9 core models that the image classification module primarily supports. In total, this module supports 80 models. The complete list of models is as follows:</p>  \ud83d\udc49Details of Model List ModelModel Download Link Top-1 Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description CLIP_vit_base_patch16_224Inference Model/Trained Model 85.36 13.1957 285.493 306.5 M CLIP is an image classification model based on the correlation between vision and language. It adopts contrastive learning and pre-training methods to achieve unsupervised or weakly supervised image classification, especially suitable for large-scale datasets. By mapping images and texts into the same representation space, the model learns general features, exhibiting good generalization ability and interpretability. With relatively good training errors, it performs well in many downstream tasks. CLIP_vit_large_patch14_224Inference Model/Trained Model 88.1 51.1284 1131.28 1.04 G ConvNeXt_base_224Inference Model/Trained Model 83.84 12.8473 1513.87 313.9 M The ConvNeXt series of models were proposed by Meta in 2022, based on the CNN architecture. This series of models builds upon ResNet, incorporating the advantages of SwinTransformer, including training strategies and network structure optimization ideas, to improve the pure CNN architecture network. It explores the performance limits of convolutional neural networks. The ConvNeXt series of models possesses many advantages of convolutional neural networks, including high inference efficiency and ease of migration to downstream tasks. ConvNeXt_base_384Inference Model/Trained Model 84.90 31.7607 3967.05 313.9 M ConvNeXt_large_224Inference Model/Trained Model 84.26 26.8103 2463.56 700.7 M ConvNeXt_large_384Inference Model/Trained Model 85.27 66.4058 6598.92 700.7 M ConvNeXt_smallInference Model/Trained Model 83.13 9.74075 1127.6 178.0 M ConvNeXt_tinyInference Model/Trained Model 82.03 5.48923 672.559 104.1 M FasterNet-LInference Model/Trained Model 83.5 23.4415 - 357.1 M FasterNet is a neural network designed to improve runtime speed. Its key improvements are as follows: 1. Re-examined popular operators and found that low FLOPS mainly stem from frequent memory accesses, especially in depthwise convolutions; 2. Proposed Partial Convolution (PConv) to extract image features more efficiently by reducing redundant computations and memory accesses; 3. Launched the FasterNet series of models based on PConv, a new design scheme that achieves significantly higher runtime speeds on various devices without compromising model task performance. FasterNet-MInference Model/Trained Model 83.0 21.8936 - 204.6 M FasterNet-SInference Model/Trained Model 81.3 13.0409 - 119.3 M FasterNet-T0Inference Model/Trained Model 71.9 12.2432 - 15.1 M FasterNet-T1Inference Model/Trained Model 75.9 11.3562 - 29.2 M FasterNet-T2Inference Model/Trained Model 79.1 10.703 - 57.4 M MobileNetV1_x0_5Inference Model/Trained Model 63.5 1.86754 7.48297 4.8 M MobileNetV1 is a network released by Google in 2017 for mobile devices or embedded devices. This network decomposes traditional convolution operations into depthwise separable convolutions, which are a combination of Depthwise convolution and Pointwise convolution. Compared to traditional convolutional networks, this combination can significantly reduce the number of parameters and computations. Additionally, this network can be used for image classification and other vision tasks. MobileNetV1_x0_25Inference Model/Trained Model 51.4 1.83478 4.83674 1.8 M MobileNetV1_x0_75Inference Model/Trained Model 68.8 2.57903 10.6343 9.3 M MobileNetV1_x1_0Inference Model/Trained Model 71.0 2.78781 13.98 15.2 M MobileNetV2_x0_5Inference Model/Trained Model 65.0 4.94234 11.1629 7.1 M MobileNetV2 is a lightweight network proposed by Google following MobileNetV1. Compared to MobileNetV1, MobileNetV2 introduces Linear bottlenecks and Inverted residual blocks as the basic structure of the network. By stacking these basic modules extensively, the network structure of MobileNetV2 is formed. Finally, it achieves higher classification accuracy with only half the FLOPs of MobileNetV1. MobileNetV2_x0_25Inference Model/Trained Model 53.2 4.50856 9.40991 5.5 M MobileNetV2_x1_0Inference Model/Trained Model 72.2 6.12159 16.0442 12.6 M MobileNetV2_x1_5Inference Model/Trained Model 74.1 6.28385 22.5129 25.0 M MobileNetV2_x2_0Inference Model/Trained Model 75.2 6.12888 30.8612 41.2 M MobileNetV3_large_x0_5Inference Model/Trained Model 69.2 6.31302 14.5588 9.6 M MobileNetV3 is a NAS-based lightweight network proposed by Google in 2019. To further enhance performance, relu and sigmoid activation functions are replaced with hard_swish and hard_sigmoid activation functions, respectively. Additionally, some improvement strategies specifically designed to reduce network computations are introduced. MobileNetV3_large_x0_35Inference Model/Trained Model 64.3 5.76207 13.9041 7.5 M MobileNetV3_large_x0_75Inference Model/Trained Model 73.1 8.41737 16.9506 14.0 M MobileNetV3_large_x1_0Inference Model/Trained Model 75.3 8.64112 19.1614 19.5 M MobileNetV3_large_x1_25Inference Model/Trained Model 76.4 8.73358 22.1296 26.5 M MobileNetV3_small_x0_5Inference Model/Trained Model 59.2 5.16721 11.2688 6.8 M MobileNetV3_small_x0_35Inference Model/Trained Model 53.0 5.22053 11.0055 6.0 M MobileNetV3_small_x0_75Inference Model/Trained Model 66.0 5.39831 12.8313 8.5 M MobileNetV3_small_x1_0Inference Model/Trained Model 68.2 6.00993 12.9598 10.5 M MobileNetV3_small_x1_25Inference Model/Trained Model 70.7 6.9589 14.3995 13.0 M MobileNetV4_conv_largeInference Model/Trained Model 83.4 12.5485 51.6453 125.2 M MobileNetV4 is an efficient architecture specifically designed for mobile devices. Its core lies in the introduction of the UIB (Universal Inverted Bottleneck) module, a unified and flexible structure that integrates IB (Inverted Bottleneck), ConvNeXt, FFN (Feed Forward Network), and the latest ExtraDW (Extra Depthwise) module. Alongside UIB, Mobile MQA, a customized attention block for mobile accelerators, was also introduced, achieving up to 39% significant acceleration. Furthermore, MobileNetV4 introduces a novel Neural Architecture Search (NAS) scheme to enhance the effectiveness of the search process. MobileNetV4_conv_mediumInference Model/Trained Model 79.9 9.65509 26.6157 37.6 M MobileNetV4_conv_smallInference Model/Trained Model 74.6 5.24172 11.0893 14.7 M MobileNetV4_hybrid_largeInference Model/Trained Model 83.8 20.0726 213.769 145.1 M MobileNetV4_hybrid_mediumInference Model/Trained Model 80.5 19.7543 62.2624 42.9 M PP-HGNet_baseInference Model/Trained Model 85.0 14.2969 327.114 249.4 M PP-HGNet (High Performance GPU Net) is a high-performance backbone network developed by Baidu PaddlePaddle's vision team, tailored for GPU platforms. This network combines the fundamentals of VOVNet with learnable downsampling layers (LDS Layer), incorporating the advantages of models such as ResNet_vd and PPHGNet. On GPU platforms, this model achieves higher accuracy compared to other SOTA models at the same speed. Specifically, it outperforms ResNet34-0 by 3.8 percentage points and ResNet50-0 by 2.4 percentage points. Under the same SLSD conditions, it ultimately surpasses ResNet50-D by 4.7 percentage points. Additionally, at the same level of accuracy, its inference speed significantly exceeds that of mainstream Vision Transformers. PP-HGNet_smallInference Model/Trained Model 81.51 5.50661 119.041 86.5 M PP-HGNet_tinyInference Model/Trained Model 79.83 5.22006 69.396 52.4 M PP-HGNetV2-B0Inference Model/Trained Model 77.77 6.53694 23.352 21.4 M PP-HGNetV2 (High Performance GPU Network V2) is the next-generation version of Baidu PaddlePaddle's PP-HGNet, featuring further optimizations and improvements upon its predecessor. It pushes the limits of NVIDIA's \"Accuracy-Latency Balance,\" significantly outperforming other models with similar inference speeds in terms of accuracy. It demonstrates strong performance across various label classification and evaluation scenarios. PP-HGNetV2-B1Inference Model/Trained Model 79.18 6.56034 27.3099 22.6 M PP-HGNetV2-B2Inference Model/Trained Model 81.74 9.60494 43.1219 39.9 M PP-HGNetV2-B3Inference Model/Trained Model 82.98 11.0042 55.1367 57.9 M PP-HGNetV2-B4Inference Model/Trained Model 83.57 9.66407 54.2462 70.4 M PP-HGNetV2-B5Inference Model/Trained Model 84.75 15.7091 115.926 140.8 M PP-HGNetV2-B6Inference Model/Trained Model 86.30 21.226 255.279 268.4 M PP-LCNet_x0_5Inference Model/Trained Model 63.14 3.67722 6.66857 6.7 M PP-LCNet is a lightweight backbone network developed by Baidu PaddlePaddle's vision team. It enhances model performance without increasing inference time, significantly surpassing other lightweight SOTA models. PP-LCNet_x0_25Inference Model/Trained Model 51.86 2.65341 5.81357 5.5 M PP-LCNet_x0_35Inference Model/Trained Model 58.09 2.7212 6.28944 5.9 M PP-LCNet_x0_75Inference Model/Trained Model 68.18 3.91032 8.06953 8.4 M PP-LCNet_x1_0Inference Model/Trained Model 71.32 3.84845 9.23735 10.5 M PP-LCNet_x1_5Inference Model/Trained Model 73.71 3.97666 12.3457 16.0 M PP-LCNet_x2_0Inference Model/Trained Model 75.18 4.07556 16.2752 23.2 M PP-LCNet_x2_5Inference Model/Trained Model 76.60 4.06028 21.5063 32.1 M PP-LCNetV2_baseInference Model/Trained Model 77.05 5.23428 19.6005 23.7 M The PP-LCNetV2 image classification model is the next-generation version of PP-LCNet, self-developed by Baidu PaddlePaddle's vision team. Based on PP-LCNet, it has undergone further optimization and improvements, primarily utilizing re-parameterization strategies to combine depthwise convolutions with varying kernel sizes and optimizing pointwise convolutions, Shortcuts, etc. Without using additional data, the PPLCNetV2_base model achieves over 77% Top-1 Accuracy on the ImageNet dataset for image classification, while maintaining an inference time of less than 4.4 ms on Intel CPU platforms. PP-LCNetV2_large Inference Model/Trained Model 78.51 6.78335 30.4378 37.3 M PP-LCNetV2_smallInference Model/Trained Model 73.97 3.89762 13.0273 14.6 M ResNet18_vdInference Model/Trained Model 72.3 3.53048 31.3014 41.5 M The ResNet series of models were introduced in 2015, winning the ILSVRC2015 competition with a top-5 error rate of 3.57%. This network innovatively proposed residual structures, which are stacked to construct the ResNet network. Experiments have shown that using residual blocks can effectively improve convergence speed and accuracy. ResNet18 Inference Model/Trained Model 71.0 2.4868 27.4601 41.5 M ResNet34_vdInference Model/Trained Model 76.0 5.60675 56.0653 77.3 M ResNet34Inference Model/Trained Model 74.6 4.16902 51.925 77.3 M ResNet50_vdInference Model/Trained Model 79.1 10.1885 68.446 90.8 M ResNet50Inference Model/Trained Model 76.5 9.62383 64.8135 90.8 M ResNet101_vdInference Model/Trained Model 80.2 20.0563 124.85 158.4 M ResNet101Inference Model/Trained Model 77.6 19.2297 121.006 158.4 M ResNet152_vdInference Model/Trained Model 80.6 29.6439 181.678 214.3 M ResNet152Inference Model/Trained Model 78.3 30.0461 177.707 214.2 M ResNet200_vdInference Model/Trained Model 80.9 39.1628 235.185 266.0 M StarNet-S1Inference Model/Trained Model 73.6 9.895 23.0465 11.2 M StarNet focuses on exploring the untapped potential of \"star operations\" (i.e., element-wise multiplication) in network design. It reveals that star operations can map inputs to high-dimensional, nonlinear feature spaces, a process akin to kernel tricks but without the need to expand the network size. Consequently, StarNet, a simple yet powerful prototype network, is further proposed, demonstrating exceptional performance and low latency under compact network structures and limited computational resources. StarNet-S2 Inference Model/Trained Model 74.8 7.91279 21.9571 14.3 M StarNet-S3Inference Model/Trained Model 77.0 10.7531 30.7656 22.2 M StarNet-S4Inference Model/Trained Model 79.0 15.2868 43.2497 28.9 M SwinTransformer_base_patch4_window7_224Inference Model/Trained Model 83.37 16.9848 383.83 310.5 M SwinTransformer is a novel vision Transformer network that can serve as a general-purpose backbone for computer vision tasks. SwinTransformer consists of a hierarchical Transformer structure represented by shifted windows. Shifted windows restrict self-attention computations to non-overlapping local windows while allowing cross-window connections, thereby enhancing network performance. SwinTransformer_base_patch4_window12_384Inference Model/Trained Model 84.17 37.2855 1178.63 311.4 M SwinTransformer_large_patch4_window7_224Inference Model/Trained Model 86.19 27.5498 689.729 694.8 M SwinTransformer_large_patch4_window12_384Inference Model/Trained Model 87.06 74.1768 2105.22 696.1 M SwinTransformer_small_patch4_window7_224Inference Model/Trained Model 83.21 16.3982 285.56 175.6 M SwinTransformer_tiny_patch4_window7_224Inference Model/Trained Model 81.10 8.54846 156.306 100.1 M <p>Note: The above accuracy metrics refer to Top-1 Accuracy on the ImageNet-1k validation set. All model GPU inference times are based on NVIDIA Tesla T4 machines, with precision type FP32. CPU inference speeds are based on Intel\u00ae Xeon\u00ae Gold 5117 CPU @ 2.00GHz, with 8 threads and precision type FP32.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX provides pre-trained model pipelines that can be quickly experienced. You can experience the effects of the General Image Classification Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Image Classification Pipeline using the demo images provided by the official. For example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model within the pipeline.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Image Classification Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>A single command is all you need to quickly experience the image classification pipeline, Use the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline image_classification --input general_image_classification_001.jpg --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the image classification pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default image classification pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config image_classification\n</code></pre> <p>After execution, the image classification pipeline configuration file will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config image_classification --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file's save path to make the configuration file take effect. For example, if the configuration file's save path is <code>./image_classification.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./image_classification.yaml --input general_image_classification_001.jpg --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If you still specify parameters, the specified parameters will take precedence.</p> <p>After running, the result will be:</p> <p><pre><code>{'input_path': 'general_image_classification_001.jpg', 'class_ids': [296, 170, 356, 258, 248], 'scores': [0.62736, 0.03752, 0.03256, 0.0323, 0.03194], 'label_names': ['ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus', 'Irish wolfhound', 'weasel', 'Samoyed, Samoyede', 'Eskimo dog, husky']}\n</code></pre> </p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#222-integration-via-python-script","title":"2.2.2 Integration via Python Script","text":"<p>A few lines of code can complete the quick inference of the pipeline. Taking the general image classification pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"image_classification\")\n\noutput = pipeline.predict(\"general_image_classification_001.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the visualization image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre> The results obtained are the same as those obtained through the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: The specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, which is only available when the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the image classification pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for the image classification task, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>, <code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>3\uff09Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/image_classification.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/image_classification.yaml\")\noutput = pipeline.predict(\"general_image_classification_001.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualization image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the pipeline directly in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Classify images.</p> <p><code>POST /image-classification</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>The properties of <code>inferenceParams</code> are as follows:</p> Name Type Description Required <code>topK</code> <code>integer</code> Only the top <code>topK</code> categories with the highest scores will be retained in the results. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>categories</code> <code>array</code> Image category information. <code>image</code> <code>string</code> The image classification result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>categories</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>id</code> <code>integer</code> Category ID. <code>name</code> <code>string</code> Category name. <code>score</code> <code>number</code> Category score. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"categories\": [\n{\n\"id\": 5,\n\"name\": \"Rabbit\",\n\"score\": 0.93\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/image-classification\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nCategories:\")\nprint(result[\"categories\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/image-classification\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto categories = result[\"categories\"];\n        std::cout &lt;&lt; \"\\nCategories:\" &lt;&lt; std::endl;\n        for (const auto&amp; category : categories) {\n            std::cout &lt;&lt; category &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/image-classification\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode categories = result.get(\"categories\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nCategories: \" + categories.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/image-classification\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Categories []map[string]interface{} `json:\"categories\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nCategories:\")\n    for _, category := range respData.Result.Categories {\n        fmt.Println(category)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/image-classification\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nCategories:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"categories\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/image-classification'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nCategories:\");\n    console.log(result[\"categories\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/image-classification\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nCategories:\\n\";\nprint_r($result[\"categories\"]);\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method for your model pipeline based on your needs and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general image classification pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using data from your specific domain or application scenario to improve the recognition performance of the general image classification pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general image classification pipeline includes an image classification module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Image Classification Module Development Tutorial and use your private dataset to fine-tune the image classification model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#42-model-application","title":"4.2 Model Application","text":"<p>After you have completed fine-tuning training using your private dataset, you will obtain local model weight files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: PP-LCNet_x1_0  # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Then, refer to the command line method or Python script method in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_classification.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference in the image classification pipeline, the Python command is:</p> <p><pre><code>paddlex --pipeline image_classification --input general_image_classification_001.jpg --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline image_classification --input general_image_classification_001.jpg --device npu:0\n</code></pre> If you want to use the General Image Classification Pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html","title":"General Image Multi-Label Classification Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#1-introduction-to-the-general-image-multi-label-classification-pipeline","title":"1. Introduction to the General Image Multi-Label Classification Pipeline","text":"<p>Image multi-label classification is a technique that assigns multiple relevant categories to a single image simultaneously, widely used in image annotation, content recommendation, and social media analysis. It can identify multiple objects or features present in an image, for example, an image containing both \"dog\" and \"outdoor\" labels. By leveraging deep learning models, image multi-label classification automatically extracts image features and performs accurate classification, providing users with more comprehensive information. This technology is of great significance in applications such as intelligent search engines and automatic content generation.</p> <p></p> <p>The General Image Multi-Label Classification Pipeline includes a module for image multi-label classification. If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, choose a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p> Model NameModel Download Link mAP (%) Model Storage Size (M) CLIP_vit_base_patch16_448_MLInference Model/Trained Model 89.15 - PP-HGNetV2-B0_MLInference Model/Trained Model 80.98 39.6 PP-HGNetV2-B4_MLInference Model/Trained Model 87.96 88.5 PP-HGNetV2-B6_MLInference Model/Trained Model 91.25 286.5 PP-LCNet_x1_0_MLInference Model/Trained Model 77.96 29.4 ResNet50_MLInference Model/Trained Model 83.50 108.9 <p>Note: The above accuracy metrics are mAP for the multi-label classification task on COCO2017. The GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. The CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX supports experiencing the effects of the General Image Multi-Label Classification Pipeline locally using command line or Python.</p> <p>Before using the General Image Multi-Label Classification Pipeline locally, please ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#21-experience-via-command-line","title":"2.1 Experience via Command Line","text":"<p>Experience the effects of the image multi-label classification pipeline with a single command:</p> <p><pre><code>paddlex --pipeline multi_label_image_classification --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the image multi-label classification pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default configuration file for the image multi-label classification pipeline is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config multi_label_image_classification\n</code></pre> <p>After execution, the configuration file for the image multi-label classification pipeline will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config multi_label_image_classification --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the saved path of the configuration file to make it effective. For example, if the configuration file is saved at <code>./multi_label_image_classification.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./multi_label_image_classification.yaml --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg --device gpu:0\n</code></pre> <p>Where <code>--model</code>, <code>--device</code>, and other parameters are not specified, the parameters in the configuration file will be used. If parameters are specified, the specified parameters will take precedence.</p> <p>After running, the result obtained is:</p> <p><pre><code>{'input_path': 'general_image_classification_001.jpg', 'class_ids': [21, 0, 30, 24], 'scores': [0.99257, 0.70596, 0.63001, 0.57852], 'label_names': ['bear', 'person', 'skis', 'backpack']}\n</code></pre> </p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#22-integration-via-python-script","title":"2.2 Integration via Python Script","text":"<p>A few lines of code can complete the rapid inference of the pipeline. Taking the general image multi-label classification pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"multi_label_image_classification\")\n\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the result visualization image\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre> The result obtained is the same as that of the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>pipeline</code> The name of the pipeline or the path of the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, which is only available when the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the multi-label classification pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the file path of the data file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the data file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain the data files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key of the dictionary needs to correspond to the specific task, such as \"img\" for image classification tasks, and the value of the dictionary supports the above data types, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/multi_label_image_classification.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/multi_label_image_classification.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualization image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have strict standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that aim to deeply optimize model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed to <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed to <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Classify images.</p> <p><code>POST /multilabel-image-classification</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of the image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>The properties of <code>inferenceParams</code> are as follows:</p> Name Type Description Required <code>topK</code> <code>integer</code> Only the top <code>topK</code> categories with the highest scores will be retained in the result. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>categories</code> <code>array</code> Image category information. <code>image</code> <code>string</code> Image classification result image. The image is in JPEG format and encoded in Base64. <p>Each element in <code>categories</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>id</code> <code>integer</code> Category ID. <code>name</code> <code>string</code> Category name. <code>score</code> <code>number</code> Category score. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"categories\": [\n{\n\"id\": 5,\n\"name\": \"Rabbit\",\n\"score\": 0.93\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/multilabel-image-classification\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nCategories:\")\nprint(result[\"categories\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/multilabel-image-classification\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto categories = result[\"categories\"];\n        std::cout &lt;&lt; \"\\nCategories:\" &lt;&lt; std::endl;\n        for (const auto&amp; category : categories) {\n            std::cout &lt;&lt; category &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/multilabel-image-classification\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode categories = result.get(\"categories\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nCategories: \" + categories.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/multilabel-image-classification\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Categories []map[string]interface{} `json:\"categories\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nCategories:\")\n    for _, category := range respData.Result.Categories {\n        fmt.Println(category)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/multilabel-image-classification\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nCategories:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"categories\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/multilabel-image-classification'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nCategories:\");\n    console.log(result[\"categories\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/multilabel-image-classification\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nCategories:\\n\";\nprint_r($result[\"categories\"]);\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a way to place computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method for your model pipeline based on your needs and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general image multi-label classification pipeline do not meet your requirements in terms of accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the general image multi-label classification pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general image multi-label classification pipeline includes an image multi-label classification module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Image Multi-Label Classification Module Development Tutorial to fine-tune the image multi-label classification model using your private dataset.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#42-model-application","title":"4.2 Model Application","text":"<p>After you have completed fine-tuning training using your private dataset, you will obtain local model weights files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: PP-LCNet_x1_0_ML   # Can be modified to the local path of the fine-tuned model\n  batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Then, refer to the command line method or Python script method in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference of the image multi-label classification pipeline, the Python command is:</p> <pre><code>paddlex --pipeline multi_label_image_classification --input https://paddle-model-ecology.bj.bcebos.com/padd\n</code></pre> <p>At this point, if you wish to switch the hardware to Ascend NPU, simply modify the <code>--device</code> in the Python command to <code>npu:0</code>:</p> <p><pre><code>paddlex --pipeline multi_label_image_classification --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_image_classification_001.jpg --device npu:0\n</code></pre> If you want to use the General Image Multi-label Classification Pipeline on more diverse hardware, please refer to the PaddleX Multi-device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html","title":"General Instance Segmentation Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#1-introduction-to-the-general-instance-segmentation-pipeline","title":"1. Introduction to the General Instance Segmentation Pipeline","text":"<p>Instance segmentation is a computer vision task that not only identifies the object categories in an image but also distinguishes the pixels of different instances within the same category, enabling precise segmentation of each object. Instance segmentation can separately label each car, person, or animal in an image, ensuring they are independently processed at the pixel level. For example, in a street scene image containing multiple cars and pedestrians, instance segmentation can clearly separate the contours of each car and person, forming multiple independent region labels. This technology is widely used in autonomous driving, video surveillance, and robotic vision, often relying on deep learning models (such as Mask R-CNN) to achieve efficient pixel classification and instance differentiation through Convolutional Neural Networks (CNNs), providing powerful support for understanding complex scenes.</p> <p></p> <p>The General Instance Segmentation Pipeline includes a Object Detection module. If you prioritize model precision, choose a model with higher precision. If you prioritize inference speed, choose a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p> ModelModel Download Link Mask AP GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Mask-RT-DETR-HInference Model/Trained Model 50.6 132.693 4896.17 449.9 M Mask-RT-DETR is an instance segmentation model based on RT-DETR. By adopting the high-performance PP-HGNetV2 as the backbone network and constructing a MaskHybridEncoder encoder, along with introducing IOU-aware Query Selection technology, it achieves state-of-the-art (SOTA) instance segmentation accuracy with the same inference time. Mask-RT-DETR-LInference Model/Trained Model 45.7 46.5059 2575.92 113.6 M <p>\u2757 The above list features the 2 core models that the image classification module primarily supports. In total, this module supports 15 models. The complete list of models is as follows:</p>  \ud83d\udc49Model List Details ModelModel Download Link Mask AP GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Cascade-MaskRCNN-ResNet50-FPNInference Model/Trained Model 36.3 - - 254.8 M Cascade-MaskRCNN is an improved Mask RCNN instance segmentation model that utilizes multiple detectors in a cascade, optimizing segmentation results by leveraging different IOU thresholds to address the mismatch between detection and inference stages, thereby enhancing instance segmentation accuracy. Cascade-MaskRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 39.1 - - 254.7 M Mask-RT-DETR-HInference Model/Trained Model 50.6 132.693 4896.17 449.9 M Mask-RT-DETR is an instance segmentation model based on RT-DETR. By adopting the high-performance PP-HGNetV2 as the backbone network and constructing a MaskHybridEncoder encoder, along with introducing IOU-aware Query Selection technology, it achieves state-of-the-art (SOTA) instance segmentation accuracy with the same inference time. Mask-RT-DETR-LInference Model/Trained Model 45.7 46.5059 2575.92 113.6 M Mask-RT-DETR-MInference Model/Trained Model 42.7 36.8329 - 66.6 M Mask-RT-DETR-SInference Model/Trained Model 41.0 33.5007 - 51.8 M Mask-RT-DETR-XInference Model/Trained Model 47.5 75.755 3358.04 237.5 M MaskRCNN-ResNet50-FPNInference Model/Trained Model 35.6 - - 157.5 M Mask R-CNN is a full-task deep learning model from Facebook AI Research (FAIR) that can perform object classification and localization in a single model, combined with image-level masks to complete segmentation tasks. MaskRCNN-ResNet50-vd-FPNInference Model/Trained Model 36.4 - - 157.5 M MaskRCNN-ResNet50Inference Model/Trained Model 32.8 - - 128.7 M MaskRCNN-ResNet101-FPNInference Model/Trained Model 36.6 - - 225.4 M MaskRCNN-ResNet101-vd-FPNInference Model/Trained Model 38.1 - - 225.1 M MaskRCNN-ResNeXt101-vd-FPNInference Model/Trained Model 39.5 - - 370.0 M PP-YOLOE_seg-SInference Model/Trained Model 32.5 - - 31.5 M PP-YOLOE_seg is an instance segmentation model based on PP-YOLOE. This model inherits PP-YOLOE's backbone and head, significantly enhancing instance segmentation performance and inference speed through the design of a PP-YOLOE instance segmentation head. SOLOv2Inference Model/Trained Model 35.5 - - 179.1 M  SOLOv2 is a real-time instance segmentation algorithm that segments objects by location. This model is an improved version of SOLO, achieving a good balance between accuracy and speed through the introduction of mask learning and mask NMS. <p>Note: The above accuracy metrics are based on the Mask AP of the COCO2017 validation set. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX allow for quick experience of the effects. You can experience the effects of the General Instance Segmentation Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Instance Segmentation Pipeline using the demo images provided by the official. For example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model within the pipeline.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Image Classification Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>A single command is all you need to quickly experience the image classification pipeline, Use the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <pre><code>paddlex --pipeline instance_segmentation --input  general_instance_segmentation_004.png --device gpu:0\n</code></pre> <p>Parameter Description:</p> <pre><code>--pipeline: The name of the pipeline, here it refers to the object detection pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 indicates using the first GPU, gpu:1,2 indicates using the second and third GPUs), or you can choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above Python script, the default instance segmentation pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config instance_segmentation\n</code></pre> <p>After execution, the instance segmentation pipeline configuration file will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config instance_segmentation --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./instance_segmentation.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./instance_segmentation.yaml --input general_instance_segmentation_004.png --device gpu:0\n</code></pre> <p>Where <code>--model</code>, <code>--device</code>, and other parameters do not need to be specified, and the parameters in the configuration file will be used. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <pre><code>{'input_path': 'general_instance_segmentation_004.png', 'boxes': [{'cls_id': 0, 'label': 'person', 'score': 0.8698326945304871, 'coordinate': [339, 0, 639, 575]}, {'cls_id': 0, 'label': 'person', 'score': 0.8571141362190247, 'coordinate': [0, 0, 195, 575]}, {'cls_id': 0, 'label': 'person', 'score': 0.8202633857727051, 'coordinate': [88, 113, 401, 574]}, {'cls_id': 0, 'label': 'person', 'score': 0.7108577489852905, 'coordinate': [522, 21, 767, 574]}, {'cls_id': 27, 'label': 'tie', 'score': 0.554280698299408, 'coordinate': [247, 311, 355, 574]}]}\n</code></pre> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code can complete the quick inference of the pipeline. Taking the general instance segmentation pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"instance_segmentation\")\n\noutput = pipeline.predict(\"general_instance_segmentation_004.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualization image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> The results obtained are the same as those obtained through the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: The specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, which is only available when the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the image classification pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for the image classification task, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>, <code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>3\uff09Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/instance_segmentation.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/instance_segmentation.yaml\")\noutput = pipeline.predict(\"general_instance_segmentation_004.png\")\nfor res in output:\n    res.print() # Print the structured output of prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, you can refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that aim to deeply optimize model inference and pre/post-processing for significant speedups in the end-to-end process. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs instance segmentation on an image.</p> <p><code>POST /instance-segmentation</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>instances</code> <code>array</code> Information about the locations and categories of instances. <code>image</code> <code>string</code> The result image of instance segmentation. The image is in JPEG format and encoded in Base64. <p>Each element in <code>instances</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> The location of the instance. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>categoryId</code> <code>integer</code> The ID of the instance category. <code>score</code> <code>number</code> The score of the instance. <code>mask</code> <code>object</code> The segmentation mask of the instance. <p>The properties of <code>mask</code> are as follows:</p> Name Type Description <code>rleResult</code> <code>str</code> The run-length encoding result of the mask. <code>size</code> <code>array</code> The shape of the mask. The elements in the array are the height and width of the mask, respectively. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"instances\": [\n{\n\"bbox\": [\n162.39381408691406,\n83.88176727294922,\n624.0797119140625,\n343.4986877441406\n],\n\"categoryId\": 33,\n\"score\": 0.8691174983978271,\n\"mask\": {\n\"rleResult\": \"xxxxxx\",\n\"size\": [\n259,\n462\n]\n}\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/instance-segmentation\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nInstances:\")\nprint(result[\"instances\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/instance-segmentation\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto instances = result[\"instances\"];\n        std::cout &lt;&lt; \"\\nInstances:\" &lt;&lt; std::endl;\n        for (const auto&amp; inst : instances) {\n            std::cout &lt;&lt; inst &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/instance-segmentation\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode instances = result.get(\"instances\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nInstances: \" + instances.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/instance-segmentation\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Instances []map[string]interface{} `json:\"instances\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nInstances:\")\n    for _, inst := range respData.Result.Instances {\n        fmt.Println(inst)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/instance-segmentation\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nInstances:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"instances\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/instance-segmentation'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nInstances:\");\n    console.log(result[\"instances\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/instance-segmentation\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nInstances:\\n\";\nprint_r($result[\"instances\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on the user's device itself, allowing the device to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method for your model pipeline based on your needs and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#4-ccustom-development","title":"4. CCustom Development","text":"<p>If the default model weights provided by the general instance segmentation pipeline do not meet your requirements for accuracy or speed in your scenario, you can try to further fine-tune the existing model using data specific to your domain or application scenario to improve the recognition effect of the general instance segmentation pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general instance segmentation pipeline includes an instance segmentation module, if the performance of the pipeline does not meet expectations, you need to refer to the Custom Development section in the Instance Segmentation Module Development Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#42-model-application","title":"4.2 Model Application","text":"<p>After you complete fine-tuning training using your private dataset, you will obtain local model weight files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <pre><code>......\nPipeline:\n  model: Mask-RT-DETR-S  # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> <p>Then, refer to the command line method or Python script method in the local experience to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for instance segmentation pipeline inference, the Python command is:</p> <pre><code>paddlex --pipeline instance_segmentation --input general_instance_segmentation_004.png --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline instance_segmentation --input general_instance_segmentation_004.png --device npu:0\n</code></pre> <p>If you want to use the General Instance Segmentation Pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html","title":"General Object Detection Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#1-introduction-to-general-object-detection-pipeline","title":"1. Introduction to General Object Detection Pipeline","text":"<p>Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. Unlike simple image classification, object detection not only requires recognizing what objects are present in an image, such as people, cars, and animals, but also accurately determining the specific position of each object within the image, typically represented by rectangular boxes. This technology is widely used in autonomous driving, surveillance systems, smart photo albums, and other fields, relying on deep learning models (e.g., YOLO, Faster R-CNN) that can efficiently extract features and perform real-time detection, significantly enhancing the computer's ability to understand image content.</p> <p></p> ModelModel Download Link mAP(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Storage Size (M) Description PicoDet-LInference Model/Trained Model 42.6 16.6715 169.904 20.9 M PP-PicoDet is a lightweight object detection algorithm for full-size, wide-angle targets, considering the computational capacity of mobile devices. Compared to traditional object detection algorithms, PP-PicoDet has a smaller model size and lower computational complexity, achieving higher speed and lower latency while maintaining detection accuracy. PicoDet-SInference Model/Trained Model 29.1 14.097 37.6563 4.4 M PP-YOLOE_plus-LInference Model/Trained Model 52.9 33.5644 814.825 185.3 M PP-YOLOE_plus is an upgraded version of the high-precision cloud-edge integrated model PP-YOLOE, developed by Baidu's PaddlePaddle vision team. By using the large-scale Objects365 dataset and optimizing preprocessing, it significantly enhances the model's end-to-end inference speed. PP-YOLOE_plus-SInference Model/Trained Model 43.7 16.8884 223.059 28.3 M RT-DETR-HInference Model/Trained Model 56.3 114.814 3933.39 435.8 M RT-DETR is the first real-time end-to-end object detector. The model features an efficient hybrid encoder to meet both model performance and throughput requirements, efficiently handling multi-scale features, and proposes an accelerated and optimized query selection mechanism to optimize the dynamics of decoder queries. RT-DETR supports flexible end-to-end inference speeds by using different decoders. RT-DETR-LInference Model/Trained Model 53.0 34.5252 1454.27 113.7 M <p>\u2757 The above list features the 6 core models that the image classification module primarily supports. In total, this module supports 37 models. The complete list of models is as follows:</p>  \ud83d\udc49Details of Model List ModelModel Download Link mAP(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description Cascade-FasterRCNN-ResNet50-FPNInference Model/Trained Model 41.1 - - 245.4 M Cascade-FasterRCNN is an improved version of the Faster R-CNN object detection model. By coupling multiple detectors and optimizing detection results using different IoU thresholds, it addresses the mismatch problem between training and prediction stages, enhancing the accuracy of object detection. Cascade-FasterRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 45.0 - - 246.2 M CenterNet-DLA-34Inference Model/Trained Model 37.6 - - 75.4 M CenterNet is an anchor-free object detection model that treats the keypoints of the object to be detected as a single point\u2014the center point of its bounding box, and performs regression through these keypoints. CenterNet-ResNet50Inference Model/Trained Model 38.9 - - 319.7 M DETR-R50Inference Model/Trained Model 42.3 59.2132 5334.52 159.3 M DETR is a transformer-based object detection model proposed by Facebook. It achieves end-to-end object detection without the need for predefined anchor boxes or NMS post-processing strategies. FasterRCNN-ResNet34-FPNInference Model/Trained Model 37.8 - - 137.5 M Faster R-CNN is a typical two-stage object detection model that first generates region proposals and then performs classification and regression on these proposals. Compared to its predecessors R-CNN and Fast R-CNN, Faster R-CNN's main improvement lies in the region proposal aspect, using a Region Proposal Network (RPN) to provide region proposals instead of traditional selective search. RPN is a Convolutional Neural Network (CNN) that shares convolutional features with the detection network, reducing the computational overhead of region proposals. FasterRCNN-ResNet50-FPNInference Model/Trained Model 38.4 - - 148.1 M FasterRCNN-ResNet50-vd-FPNInference Model/Trained Model 39.5 - - 148.1 M FasterRCNN-ResNet50-vd-SSLDv2-FPNInference Model/Trained Model 41.4 - - 148.1 M FasterRCNN-ResNet50Inference Model/Trained Model 36.7 - - 120.2 M FasterRCNN-ResNet101-FPNInference Model/Trained Model 41.4 - - 216.3 M FasterRCNN-ResNet101Inference Model/Trained Model 39.0 - - 188.1 M FasterRCNN-ResNeXt101-vd-FPNInference Model/Trained Model 43.4 - - 360.6 M FasterRCNN-Swin-Tiny-FPNInference Model/Trained Model 42.6 - - 159.8 M FCOS-ResNet50Inference Model/Trained Model 39.6 103.367 3424.91 124.2 M FCOS is an anchor-free object detection model that performs dense predictions. It uses the backbone of RetinaNet and directly regresses the width and height of the target object on the feature map, predicting the object's category and centerness (the degree of offset of pixels on the feature map from the object's center), which is eventually used as a weight to adjust the object score. PicoDet-LInference Model/Trained Model 42.6 16.6715 169.904 20.9 M PP-PicoDet is a lightweight object detection algorithm designed for full-size and wide-aspect-ratio targets, with a focus on mobile device computation. Compared to traditional object detection algorithms, PP-PicoDet boasts smaller model sizes and lower computational complexity, achieving higher speeds and lower latency while maintaining detection accuracy. PicoDet-MInference Model/Trained Model 37.5 16.2311 71.7257 16.8 M PicoDet-SInference Model/Trained Model 29.1 14.097 37.6563 4.4 M PicoDet-XSInference Model/Trained Model 26.2 13.8102 48.3139 5.7 M PP-YOLOE_plus-LInference Model/Trained Model 52.9 33.5644 814.825 185.3 M PP-YOLOE_plus is an iteratively optimized and upgraded version of PP-YOLOE, a high-precision cloud-edge integrated model developed by Baidu PaddlePaddle's Vision Team. By leveraging the large-scale Objects365 dataset and optimizing preprocessing, it significantly enhances the end-to-end inference speed of the model. PP-YOLOE_plus-MInference Model/Trained Model 49.8 19.843 449.261 82.3 M PP-YOLOE_plus-SInference Model/Trained Model 43.7 16.8884 223.059 28.3 M PP-YOLOE_plus-XInference Model/Trained Model 54.7 57.8995 1439.93 349.4 M RT-DETR-HInference Model/Trained Model 56.3 114.814 3933.39 435.8 M RT-DETR is the first real-time end-to-end object detector. It features an efficient hybrid encoder that balances model performance and throughput, efficiently processes multi-scale features, and introduces an accelerated and optimized query selection mechanism to dynamize decoder queries. RT-DETR supports flexible end-to-end inference speeds through the use of different decoders. RT-DETR-LInference Model/Trained Model 53.0 34.5252 1454.27 113.7 M RT-DETR-R18Inference Model/Trained Model 46.5 19.89 784.824 70.7 M RT-DETR-R50Inference Model/Trained Model 53.1 41.9327 1625.95 149.1 M RT-DETR-XInference Model/Trained Model 54.8 61.8042 2246.64 232.9 M YOLOv3-DarkNet53Inference Model/Trained Model 39.1 40.1055 883.041 219.7 M YOLOv3 is a real-time end-to-end object detector that utilizes a unique single Convolutional Neural Network (CNN) to frame the object detection problem as a regression task, enabling real-time detection. The model employs multi-scale detection to enhance performance across different object sizes. YOLOv3-MobileNetV3Inference Model/Trained Model 31.4 18.6692 267.214 83.8 M YOLOv3-ResNet50_vd_DCNInference Model/Trained Model 40.6 31.6276 856.047 163.0 M YOLOX-LInference Model/Trained Model 50.1 185.691 1250.58 192.5 M Building upon YOLOv3's framework, YOLOX significantly boosts detection performance in complex scenarios by incorporating Decoupled Head, Data Augmentation, Anchor Free, and SimOTA components. YOLOX-MInference Model/Trained Model 46.9 123.324 688.071 90.0 M YOLOX-NInference Model/Trained Model 26.1 79.1665 155.59 3.4 M YOLOX-SInference Model/Trained Model 40.4 184.828 474.446 32.0 M YOLOX-TInference Model/Trained Model 32.9 102.748 212.52 18.1 M YOLOX-XInference Model/Trained Model 51.8 227.361 2067.84 351.5 M <p>Note: The precision metrics mentioned are based on the COCO2017 validation set mAP(0.5:0.95). All model GPU inference times are measured on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX's pre-trained model pipelines allow for quick experience of their effects. You can experience the effects of the General Object Detection Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience the General Object Detection Pipeline online using the demo images provided by the official source, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model within the pipeline.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Object Detection Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>A single command can quickly experience the effects of the object detection pipeline, Use the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline object_detection --input general_object_detection_002.png --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the object detection pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 indicates using the first GPU, gpu:1,2 indicates using the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default object detection pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  Click to expand \ud83d\udc49 <pre><code>paddlex --get_pipeline_config object_detection\n</code></pre> <p>After execution, the object detection pipeline configuration file will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config object_detection --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file save path to make the configuration file effective. For example, if the configuration file save path is <code>./object_detection.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./object_detection.yaml --input general_object_detection_002.png --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If these parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result will be:</p> <pre><code>{'input_path': 'general_object_detection_002.png', 'boxes': [{'cls_id': 49, 'label': 'orange', 'score': 0.8188097476959229, 'coordinate': [661, 93, 870, 305]}, {'cls_id': 47, 'label': 'apple', 'score': 0.7743489146232605, 'coordinate': [76, 274, 330, 520]}, {'cls_id': 47, 'label': 'apple', 'score': 0.7270504236221313, 'coordinate': [285, 94, 469, 297]}, {'cls_id': 46, 'label': 'banana', 'score': 0.5570532083511353, 'coordinate': [310, 361, 685, 712]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5484835505485535, 'coordinate': [764, 285, 924, 440]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5160726308822632, 'coordinate': [853, 169, 987, 303]}, {'cls_id': 60, 'label': 'dining table', 'score': 0.5142655968666077, 'coordinate': [0, 0, 1072, 720]}, {'cls_id': 47, 'label': 'apple', 'score': 0.5101479291915894, 'coordinate': [57, 23, 213, 176]}]}\n</code></pre> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#222-integration-via-python-scripts","title":"2.2.2 Integration via Python Scripts","text":"<p>A few lines of code are all you need to quickly perform inference on your production line. Taking General Object Detection as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"object_detection\")\n\noutput = pipeline.predict(\"general_object_detection_002.png\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the visualized image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre> The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: The specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it's a pipeline name, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>enable_hpi</code> Whether to enable high-performance inference, only available if the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pipeline object to perform inference: The <code>predict</code> method parameter <code>x</code> is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for image classification tasks, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be of the above types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/object_detection.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/object_detection.yaml\")\noutput = pipeline.predict(\"general_object_detection_002.png\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualized image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies, especially response speed, to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. Refer to the PaddleX High-Performance Inference Guide for detailed high-performance inference procedures.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. Refer to the PaddleX Service-Oriented Deployment Guide for detailed service-oriented deployment procedures.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs object detection on an image.</p> <p><code>POST /object-detection</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>detectedObjects</code> <code>array</code> Information about the location and category of the detected objects. <code>image</code> <code>string</code> The image of the object detection result. The image is in JPEG format and encoded in Base64. <p>Each element in <code>detectedObjects</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> The location of the object. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>categoryId</code> <code>integer</code> The ID of the object category. <code>score</code> <code>number</code> The score of the object. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"detectedObjects\": [\n{\n\"bbox\": [\n404.4967956542969,\n90.15770721435547,\n506.2465515136719,\n285.4187316894531\n],\n\"categoryId\": 0,\n\"score\": 0.7418514490127563\n},\n{\n\"bbox\": [\n155.33145141601562,\n81.10954284667969,\n199.71136474609375,\n167.4235382080078\n],\n\"categoryId\": 1,\n\"score\": 0.7328268885612488\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/object-detection\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected objects:\")\nprint(result[\"detectedObjects\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/object-detection\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto detectedObjects = result[\"detectedObjects\"];\n        std::cout &lt;&lt; \"\\nDetected objects:\" &lt;&lt; std::endl;\n        for (const auto&amp; obj : detectedObjects) {\n            std::cout &lt;&lt; obj &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/object-detection\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode detectedObjects = result.get(\"detectedObjects\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nDetected objects: \" + detectedObjects.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/object-detection\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            DetectedObjects []map[string]interface{} `json:\"detectedObjects\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nDetected objects:\")\n    for _, obj := range respData.Result.DetectedObjects {\n        fmt.Println(obj)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/object-detection\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nDetected objects:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"detectedObjects\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/object-detection'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nDetected objects:\");\n    console.log(result[\"detectedObjects\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/object-detection\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nDetected objects:\\n\";\nprint_r($result[\"detectedObjects\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. Refer to the PaddleX Edge Deployment Guide for detailed edge deployment procedures.</p> <p>Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Object Detection pipeline do not meet your requirements for precision or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the General Object Detection pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the General Object Detection pipeline includes an object detection module, if the performance of the pipeline does not meet expectations, you need to refer to the Custom Development section in the Object Detection Module Development Tutorial and use your private dataset to fine-tune the object detection model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning the model with your private dataset, you will obtain local model weights files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: PicoDet-S  # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Then, refer to the command line method or Python script method in the local experience, and load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/object_detection.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference of the General Object Detection pipeline, the Python command is:</p> <p><pre><code>paddlex --pipeline object_detection --input general_object_detection_002.png --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline object_detection --input general_object_detection_002.png --device npu:0\n</code></pre> If you want to use the General Object Detection Pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html","title":"Pedestrian Attribute Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#1-introduction-to-pedestrian-attribute-recognition-pipeline","title":"1. Introduction to Pedestrian Attribute Recognition Pipeline","text":"<p>Pedestrian attribute recognition is a key function in computer vision systems, used to locate and label specific characteristics of pedestrians in images or videos, such as gender, age, clothing color, and style. This task not only requires accurately detecting pedestrians but also identifying detailed attribute information for each pedestrian. The pedestrian attribute recognition pipeline is an end-to-end serial system for locating and recognizing pedestrian attributes, widely used in smart cities, security surveillance, and other fields, significantly enhancing the system's intelligence level and management efficiency.</p> <p></p> <p>The pedestrian attribute recognition pipeline includes a pedestrian detection module and a pedestrian attribute recognition module, with several models in each module. Which models to use specifically can be selected based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy; if you prioritize inference speed, choose models with faster inference; if you prioritize model storage size, choose models with smaller storage.</p> <p>Pedestrian Detection Module:</p> ModelModel Download Link mAP(0.5:0.95) mAP(0.5) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-YOLOE-L_humanInference Model/Trained Model 48.0 81.9 32.8 777.7 196.02 Pedestrian detection model based on PP-YOLOE PP-YOLOE-S_humanInference Model/Trained Model 42.5 77.9 15.0 179.3 28.79 <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the CrowdHuman dataset. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Pedestrian Attribute Recognition Module:</p> ModelModel Download Link mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_pedestrian_attributeInference Model/Trained Model 92.2 3.84845 9.23735 6.7 M PP-LCNet_x1_0_pedestrian_attribute is a lightweight pedestrian attribute recognition model based on PP-LCNet, covering 26 categories. <p>Note: The above accuracy metrics are mA on PaddleX's internally built dataset. GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX can quickly demonstrate their effectiveness. You can experience the pedestrian attribute recognition pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#21-online-experience","title":"2.1 Online Experience","text":"<p>Not supported yet.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the pedestrian attribute recognition pipeline locally, ensure you have completed the installation of the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>You can quickly experience the pedestrian attribute recognition pipeline with a single command. Use the test file and replace <code>--input</code> with the local path for prediction.</p> <p><pre><code>paddlex --pipeline pedestrian_attribute_recognition --input pedestrian_attribute_002.jpg --device gpu:0\n</code></pre> Parameter Description:</p> <pre><code>--pipeline: The name of the pipeline, here it is the pedestrian attribute recognition pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 means using the first GPU, gpu:1,2 means using the second and third GPUs), or you can choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above Python script, the default pedestrian attribute recognition pipeline configuration file is loaded. If you need a custom configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config pedestrian_attribute_recognition\n</code></pre> <p>After execution, the pedestrian attribute recognition pipeline configuration file will be saved in the current path. If you wish to specify a custom save location, you can run the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config pedestrian_attribute_recognition --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the saved path of the configuration file to make it effective. For example, if the configuration file is saved at <code>./pedestrian_attribute_recognition.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./pedestrian_attribute_recognition.yaml --input pedestrian_attribute_002.jpg --device gpu:0\n</code></pre> <p>Among them, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, and the parameters in the configuration file will be used. If parameters are still specified, the specified parameters will take precedence.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code are sufficient for quick inference of the pipeline. Taking the pedestrian attribute recognition pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"pedestrian_attribute_recognition\")\n\noutput = pipeline.predict(\"pedestrian_attribute_002.jpg\")\nfor res in output:\n    res.print()  ## Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  ## Save the visualized image of the result\n    res.save_to_json(\"./output/\")  ## Save the structured output of the prediction\n</code></pre> The results obtained are the same as those from the command line approach.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Parameter Type Default Value <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available when the pipeline supports high-performance inference. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pedestrian attribute recognition pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods. Specific examples are as follows:</p> Parameter Type Description Python Var Supports directly passing in Python variables, such as image data represented by numpy.ndarray. <code>str</code> Supports passing in the file path of the data to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing in the URL of the data file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing in a local directory, which should contain the data files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing in a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for the pedestrian attribute recognition task, and the value of the dictionary supports the above data types, for example: <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing in a list, where the elements of the list need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>(4) Process the prediction results: The prediction result for each sample is of type <code>dict</code> and supports printing or saving as a file. The supported save types are related to the specific pipeline, such as:</p> Method Description Method Parameters <code>print</code> Print the results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only effective when <code>format_json</code> is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only effective when <code>format_json</code> is True, default is False; <code>save_to_json</code> Save the results as a json-formatted file <code>- save_path</code>: str, the path to save the file, when it is a directory, the saved file name is consistent with the input file name;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; <code>save_to_img</code> Save the results as an image file <p>If you have obtained the configuration file, you can customize various configurations for the pedestrian attribute recognition pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of your pipeline configuration file.</p> <p>For example, if your configuration file is saved as <code>./my_path/pedestrian_attribute_recognition*.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/pedestrian_attribute_recognition.yaml\")\noutput = pipeline.predict(\"pedestrian_attribute_002.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the visualized result image\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pedestrian attribute recognition pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to directly apply the pedestrian attribute recognition pipeline in your Python project, you can refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API reference and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Get pedestrian attribute recognition results.</p> <p><code>POST /pedestrian-attribute-recognition</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>pedestrians</code> <code>array</code> Information about the pedestrian's location and attributes. <code>image</code> <code>string</code> The pedestrian attribute recognition result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>pedestrians</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> The location of the pedestrian. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>attributes</code> <code>array</code> The pedestrian attributes. <code>score</code> <code>number</code> The detection score. <p>Each element in <code>attributes</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>label</code> <code>string</code> The label of the attribute. <code>score</code> <code>number</code> The classification score. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/pedestrian-attribute-recognition\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected pedestrians:\")\nprint(result[\"pedestrians\"])\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method where computing and data processing functions are placed on the user's device itself, allowing the device to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide. You can choose an appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the pedestrian attribute recognition pipeline includes both a pedestrian attribute recognition module and a pedestrian detection module, the unexpected performance of the pipeline may stem from either module. You can analyze images with poor recognition results. If during the analysis, you find that many main targets are not detected, it may indicate deficiencies in the pedestrian detection model. In this case, you need to refer to the Secondary Development section in the Human Detection Module Development Tutorial and use your private dataset to fine-tune the pedestrian detection model. If the detected main attributes are incorrectly recognized, you need to refer to the Secondary Development section in the Pedestrian Attribute Recognition Module Development Tutorial and use your private dataset to fine-tune the pedestrian attribute recognition model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning training with your private dataset, you will obtain local model weight files.</p> <p>If you need to use the fine-tuned model weights, you only need to modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  det_model: PP-YOLOE-L_human\n  cls_model: PP-LCNet_x1_0_pedestrian_attribute  # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Subsequently, refer to the command-line method or Python script method in the local experience, and load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/pedestrian_attribute_recognition.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports multiple mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modifying the <code>--device</code> parameter  allows seamless switching between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference in the pedestrian attribute recognition pipeline, the command used is:</p> <p><pre><code>paddlex --pipeline pedestrian_attribute_recognition --input pedestrian_attribute_002.jpg --device gpu:0\n</code></pre> At this point, if you want to switch the hardware to Ascend NPU, you only need to change <code>--device</code> to npu:0:</p> <p><pre><code>paddlex --pipeline pedestrian_attribute_recognition --input pedestrian_attribute_002.jpg --device npu:0\n</code></pre> If you want to use the pedestrian attribute recognition pipeline on more types of hardware, please refer to the PaddleX Multi-device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html","title":"General Semantic Segmentation Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#1-introduction-to-the-general-semantic-segmentation-pipeline","title":"1. Introduction to the General Semantic Segmentation Pipeline","text":"<p>Semantic segmentation is a computer vision technique that aims to assign each pixel in an image to a specific category, enabling a detailed understanding of the image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing regions of the same category to be fully labeled. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, the sky, and roads pixel by pixel, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (such as SegFormer, etc.) to extract features by CNN or Transformer, and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis.</p> <p></p> Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) OCRNet_HRNet-W48Inference Model/Trained Model 82.15 78.9976 2226.95 249.8 M PP-LiteSeg-TInference Model/Trained Model 73.10 7.6827 138.683 28.5 M <p>\u2757 The above list features the 2 core models that the image classification module primarily supports. In total, this module supports 18 models. The complete list of models is as follows:</p>  \ud83d\udc49Model List Details Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Deeplabv3_Plus-R50Inference Model/Trained Model 80.36 61.0531 1513.58 94.9 M Deeplabv3_Plus-R101Inference Model/Trained Model 81.10 100.026 2460.71 162.5 M Deeplabv3-R50Inference Model/Trained Model 79.90 82.2631 1735.83 138.3 M Deeplabv3-R101Inference Model/Trained Model 80.85 121.492 2685.51 205.9 M OCRNet_HRNet-W18Inference Model/Trained Model 80.67 48.2335 906.385 43.1 M OCRNet_HRNet-W48Inference Model/Trained Model 82.15 78.9976 2226.95 249.8 M PP-LiteSeg-TInference Model/Trained Model 73.10 7.6827 138.683 28.5 M PP-LiteSeg-BInference Model/Trained Model 75.25 10.9935 194.727 47.0 M SegFormer-B0 (slice)Inference Model/Trained Model 76.73 11.1946 268.929 13.2 M SegFormer-B1 (slice)Inference Model/Trained Model 78.35 17.9998 403.393 48.5 M SegFormer-B2 (slice)Inference Model/Trained Model 81.60 48.0371 1248.52 96.9 M SegFormer-B3 (slice)Inference Model/Trained Model 82.47 64.341 1666.35 167.3 M SegFormer-B4 (slice)Inference Model/Trained Model 82.38 82.4336 1995.42 226.7 M SegFormer-B5 (slice)Inference Model/Trained Model 82.58 97.3717 2420.19 229.7 M <p>The accuracy metrics of the above models are measured on the Cityscapes dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> Model NameModel Download Link mIoU (%) GPU Inference Time (ms) CPU Inference Time Model Size (M) SeaFormer_base(slice)Inference Model/Trained Model 40.92 24.4073 397.574 30.8 M SeaFormer_large (slice)Inference Model/Trained Model 43.66 27.8123 550.464 49.8 M SeaFormer_small (slice)Inference Model/Trained Model 38.73 19.2295 358.343 14.3 M SeaFormer_tiny (slice)Inference Model/Trained Model 34.58 13.9496 330.132 6.1M <p>The accuracy metrics of the SeaFormer series models are measured on the ADE20k dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX's pre-trained model pipelines can be quickly experienced. You can experience the effects of the General Semantic Segmentation Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Semantic Segmentation Pipeline, using the official demo images for recognition, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model in the pipeline online.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Semantic Segmentation Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>Experience the semantic segmentation pipeline with a single command, Use the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <pre><code>paddlex --pipeline semantic_segmentation --input makassaridn-road_demo.png --device gpu:0\n</code></pre> <p>Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the semantic segmentation pipeline\n--input: The local path or URL of the input image to be processed\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs), or choose to use CPU (--device cpu)\n</code></pre> <p>When executing the above command, the default semantic segmentation pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config semantic_segmentation\n</code></pre> <p>After execution, the semantic segmentation pipeline configuration file will be saved in the current path. If you wish to customize the save location, execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config semantic_segmentation --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./semantic_segmentation.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./semantic_segmentation.yaml --input makassaridn-road_demo.png --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, and the parameters in the configuration file will be used. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <p>{'input_path': 'makassaridn-road_demo.png', 'pred': '...'}</p> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code can complete the quick inference of the pipeline. Taking the general semantic segmentation pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"semantic_segmentation\")\n\noutput = pipeline.predict(\"makassaridn-road_demo.png\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the visualization image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre> The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: The specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>enable_hpi</code> Whether to enable high-performance inference, which is only available when the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained by calling it. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list representing a set of prediction results.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/semantic_segmentation.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/semantic_segmentation.yaml\")\noutput = pipeline.predict(\"makassaridn-road_demo.png\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualized image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that aim to deeply optimize model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs semantic segmentation on an image.</p> <p><code>POST /semantic-segmentation</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>labelMap</code> <code>array</code> Records the class label of each pixel in the image (arranged in row-major order). <code>size</code> <code>array</code> Image shape. The elements in the array are the height and width of the image, respectively. <code>image</code> <code>string</code> The semantic segmentation result image. The image is in JPEG format and encoded using Base64. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"labelMap\": [\n0,\n0,\n1,\n2\n],\n\"size\": [\n2,\n2\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/semantic-segmentation\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/semantic-segmentation\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/semantic-segmentation\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode labelMap = result.get(\"labelMap\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/semantic-segmentation\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Labelmap []map[string]interface{} `json:\"labelMap\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/semantic-segmentation\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/semantic-segmentation'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/semantic-segmentation\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general semantic segmentation pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the general semantic segmentation pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general semantic segmentation pipeline includes a semantic segmentation module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Semantic Segmentation Module Development Tutorial (GitHub can directly link to headings) and use your private dataset to fine-tune the semantic segmentation model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#42-model-application","title":"4.2 Model Application","text":"<p>After you complete fine-tuning training using your private dataset, you will obtain local model weight files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: PP-LiteSeg-T  # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Then, refer to the command line method or Python script method in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/semantic_segmentation.html#multi-hardware-support","title":"Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for semantic segmentation pipeline inference, the Python command is:</p> <p><pre><code>paddlex --pipeline semantic_segmentation --input makassaridn-road_demo.png --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` flag in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline semantic_segmentation --input makassaridn-road_demo.png --device npu:0\n</code></pre> If you want to use the General Semantic Segmentation Pipeline on a wider range of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html","title":"Small Object Detection Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#1-introduction-to-small-object-detection-pipeline","title":"1. Introduction to Small Object Detection Pipeline","text":"<p>Small object detection is a specialized technique for identifying tiny objects within images, widely applied in fields such as surveillance, autonomous driving, and satellite image analysis. It can accurately locate and classify small-sized objects like pedestrians, traffic signs, or small animals within complex scenes. By leveraging deep learning algorithms and optimized Convolutional Neural Networks (CNNs), small object detection significantly enhances the recognition capabilities for small objects, ensuring no critical information is overlooked in practical applications. This technology plays a pivotal role in enhancing safety and automation levels.</p> <p></p> <p>The small object detection pipeline includes a small object detection module. If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model storage size, opt for a model with a smaller storage size.</p> Model NameModel Download Link mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) PP-YOLOE_plus_SOD-SInference Model/Trained Model 25.1 65.4608 324.37 77.3 M PP-YOLOE_plus_SOD-LInference Model/Trained Model 31.9 57.1448 1006.98 325.0 M PP-YOLOE_plus_SOD-largesize-LInference Model/Trained Model 42.7 458.521 11172.7 340.5 M <p>Note: The above accuracy metrics are based on the VisDrone-DET validation set mAP(0.5:0.95). All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX supports experiencing the small object detection pipeline's effects through command line or Python locally.</p> <p>Before using the small object detection pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#21-experience-via-command-line","title":"2.1 Experience via Command Line","text":"<p>Experience the small object detection pipeline with a single command, Use the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline small_object_detection --input small_object_detection.jpg --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it's the small object detection pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). Alternatively, you can choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default small object detection pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config small_object_detection\n</code></pre> <p>After execution, the small object detection pipeline configuration file will be saved in the current directory. If you wish to customize the save location, execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config small_object_detection --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file's save path to make the configuration file effective. For example, if the configuration file's save path is <code>./small_object_detection.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./small_object_detection.yaml --input small_object_detection.jpg --device gpu:0\n</code></pre> <p>Here, parameters like <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file.```markdown</p> <p>After running, the result will be:</p> <pre><code>{'input_path': 'small_object_detection.jpg', 'boxes': [{'cls_id': 3, 'label': 'car', 'score': 0.9243856072425842, 'coordinate': [624, 638, 682, 741]}, {'cls_id': 3, 'label': 'car', 'score': 0.9206348061561584, 'coordinate': [242, 561, 356, 613]}, {'cls_id': 3, 'label': 'car', 'score': 0.9194547533988953, 'coordinate': [670, 367, 705, 400]}, {'cls_id': 3, 'label': 'car', 'score': 0.9162291288375854, 'coordinate': [459, 259, 523, 283]}, {'cls_id': 4, 'label': 'van', 'score': 0.9075379371643066, 'coordinate': [467, 213, 498, 242]}, {'cls_id': 4, 'label': 'van', 'score': 0.9066920876502991, 'coordinate': [547, 351, 577, 397]}, {'cls_id': 3, 'label': 'car', 'score': 0.9041045308113098, 'coordinate': [502, 632, 562, 736]}, {'cls_id': 3, 'label': 'car', 'score': 0.8934890627861023, 'coordinate': [613, 383, 647, 427]}, {'cls_id': 3, 'label': 'car', 'score': 0.8803309202194214, 'coordinate': [640, 280, 671, 309]}, {'cls_id': 3, 'label': 'car', 'score': 0.8727016448974609, 'coordinate': [1199, 256, 1259, 281]}, {'cls_id': 3, 'label': 'car', 'score': 0.8705748915672302, 'coordinate': [534, 410, 570, 461]}, {'cls_id': 3, 'label': 'car', 'score': 0.8654043078422546, 'coordinate': [669, 248, 702, 271]}, {'cls_id': 3, 'label': 'car', 'score': 0.8555219769477844, 'coordinate': [525, 243, 550, 270]}, {'cls_id': 3, 'label': 'car', 'score': 0.8522038459777832, 'coordinate': [526, 220, 553, 243]}, {'cls_id': 3, 'label': 'car', 'score': 0.8392605185508728, 'coordinate': [557, 141, 575, 158]}, {'cls_id': 3, 'label': 'car', 'score': 0.8353804349899292, 'coordinate': [537, 120, 553, 133]}, {'cls_id': 3, 'label': 'car', 'score': 0.8322211503982544, 'coordinate': [585, 132, 603, 147]}, {'cls_id': 3, 'label': 'car', 'score': 0.8298957943916321, 'coordinate': [701, 283, 736, 313]}, {'cls_id': 3, 'label': 'car', 'score': 0.8217393159866333, 'coordinate': [885, 347, 943, 377]}, {'cls_id': 3, 'label': 'car', 'score': 0.820313572883606, 'coordinate': [493, 150, 511, 168]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.8183429837226868, 'coordinate': [203, 701, 224, 743]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.815082848072052, 'coordinate': [185, 710, 201, 744]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.7892289757728577, 'coordinate': [311, 371, 344, 407]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.7812919020652771, 'coordinate': [345, 380, 388, 405]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.7748346328735352, 'coordinate': [295, 500, 309, 532]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.7688500285148621, 'coordinate': [851, 436, 863, 466]}, {'cls_id': 3, 'label': 'car', 'score': 0.7466475367546082, 'coordinate': [565, 114, 580, 128]}, {'cls_id': 3, 'label': 'car', 'score': 0.7156463265419006, 'coordinate': [483, 66, 495, 78]}, {'cls_id': 3, 'label': 'car', 'score': 0.704211950302124, 'coordinate': [607, 138, 642, 152]}, {'cls_id': 3, 'label': 'car', 'score': 0.7021926045417786, 'coordinate': [505, 72, 518, 83]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.6897469162940979, 'coordinate': [802, 460, 815, 488]}, {'cls_id': 3, 'label': 'car', 'score': 0.671891450881958, 'coordinate': [574, 123, 593, 136]}, {'cls_id': 9, 'label': 'motorcycle', 'score': 0.6712754368782043, 'coordinate': [445, 317, 472, 334]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.6695684790611267, 'coordinate': [479, 309, 489, 332]}, {'cls_id': 3, 'label': 'car', 'score': 0.6273623704910278, 'coordinate': [654, 210, 677, 234]}, {'cls_id': 3, 'label': 'car', 'score': 0.6070230603218079, 'coordinate': [640, 166, 667, 185]}, {'cls_id': 3, 'label': 'car', 'score': 0.6064521670341492, 'coordinate': [461, 59, 476, 71]}, {'cls_id': 3, 'label': 'car', 'score': 0.5860581398010254, 'coordinate': [464, 87, 484, 100]}, {'cls_id': 9, 'label': 'motorcycle', 'score': 0.5792551636695862, 'coordinate': [390, 390, 419, 408]}, {'cls_id': 3, 'label': 'car', 'score': 0.5559225678443909, 'coordinate': [481, 125, 496, 140]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.5531904697418213, 'coordinate': [869, 306, 880, 331]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.5468509793281555, 'coordinate': [895, 294, 904, 319]}, {'cls_id': 3, 'label': 'car', 'score': 0.5451828241348267, 'coordinate': [505, 94, 518, 108]}, {'cls_id': 3, 'label': 'car', 'score': 0.5398445725440979, 'coordinate': [657, 188, 681, 208]}, {'cls_id': 4, 'label': 'van', 'score': 0.5318890810012817, 'coordinate': [518, 88, 534, 102]}, {'cls_id': 3, 'label': 'car', 'score': 0.5296525359153748, 'coordinate': [527, 71, 540, 81]}, {'cls_id': 6, 'label': 'tricycle', 'score': 0.5168400406837463, 'coordinate': [528, 320, 563, 346]}, {'cls_id': 3, 'label': 'car', 'score': 0.5088561177253723, 'coordinate': [511, 84, 530, 95]}, {'cls_id': 0, 'label': 'pedestrian', 'score': 0.502006471157074, 'coordinate': [841, 266, 850, 283]}]}\n</code></pre> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#22-integration-via-python-script","title":"2.2 Integration via Python Script","text":"<p>A few lines of code can quickly enable inference on the production line. Taking the General Small Object Detection Pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"small_object_detection\")\n\noutput = pipeline.predict(\"small_object_detection.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualized image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre> The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it's a pipeline name, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available when the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above data types, for example: <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>, <code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained by iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list representing a set of prediction results.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/small_object_detection</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/small_object_detection.yaml\")\noutput = pipeline.predict(\"small_object_detection.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save the visualization image of the result\n    res.save_to_json(\"./output/\")  # Save the structured output of prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed with development integration/deployment directly.</p> <p>If you need to apply the pipeline directly in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs object detection on an image.</p> <p><code>POST /small-object-detection</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>detectedObjects</code> <code>array</code> Information about the location and category of the detected objects. <code>image</code> <code>string</code> The image of the object detection result. The image is in JPEG format and encoded in Base64. <p>Each element in <code>detectedObjects</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> The location of the object. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>categoryId</code> <code>integer</code> The ID of the object category. <code>score</code> <code>number</code> The score of the object. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"detectedObjects\": [\n{\n\"bbox\": [\n404.4967956542969,\n90.15770721435547,\n506.2465515136719,\n285.4187316894531\n],\n\"categoryId\": 0,\n\"score\": 0.7418514490127563\n},\n{\n\"bbox\": [\n155.33145141601562,\n81.10954284667969,\n199.71136474609375,\n167.4235382080078\n],\n\"categoryId\": 1,\n\"score\": 0.7328268885612488\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/small-object-detection\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected objects:\")\nprint(result[\"detectedObjects\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/small-object-detection\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto detectedObjects = result[\"detectedObjects\"];\n        std::cout &lt;&lt; \"\\nDetected objects:\" &lt;&lt; std::endl;\n        for (const auto&amp; obj : detectedObjects) {\n            std::cout &lt;&lt; obj &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/small-object-detection\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode detectedObjects = result.get(\"detectedObjects\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nDetected objects: \" + detectedObjects.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/small-object-detection\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            DetectedObjects []map[string]interface{} `json:\"detectedObjects\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nDetected objects:\")\n    for _, obj := range respData.Result.DetectedObjects {\n        fmt.Println(obj)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/small-object-detection\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nDetected objects:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"detectedObjects\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/small-object-detection'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nDetected objects:\");\n    console.log(result[\"detectedObjects\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/small-object-detection\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nDetected objects:\\n\";\nprint_r($result[\"detectedObjects\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Small Object Detection Pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the small object detection pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the General Small Object Detection Pipeline includes a small object detection module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Small Object Detection Module Tutorial and use your private dataset to fine-tune the small object detection model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weight files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: PP-YOLOE_plus_SOD-L  # Can be modified to the local path of the fine-tuned model\n  batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Then, refer to the command line or Python script methods in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/small_object_detection.html#multi-hardware-support","title":"Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference with the small object detection pipeline, the Python command would be:</p> <pre><code>paddlex --pipeline multilabel_classification --input small_object_detection.jpg --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline multilabel_classification --input small_object_detection.jpg --device npu:0\n</code></pre> <p>If you want to use the General Small Object Detection Pipeline on a wider range of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html","title":"Vehicle Attribute Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#1-introduction-to-vehicle-attribute-recognition-pipeline","title":"1. Introduction to Vehicle Attribute Recognition Pipeline","text":"<p>Vehicle attribute recognition is a crucial component in computer vision systems. Its primary task is to locate and label specific attributes of vehicles in images or videos, such as vehicle type, color, and license plate number. This task not only requires accurately detecting vehicles but also identifying detailed attribute information for each vehicle. The vehicle attribute recognition pipeline is an end-to-end serial system for locating and recognizing vehicle attributes, widely used in traffic management, intelligent parking, security surveillance, autonomous driving, and other fields. It significantly enhances system efficiency and intelligence levels, driving the development and innovation of related industries.</p> <p></p> <p>The vehicle attribute recognition pipeline includes a vehicle detection module and a vehicle attribute recognition module, with several models in each module. Which models to use can be selected based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy; if you prioritize inference speed, choose models with faster inference; if you prioritize model storage size, choose models with smaller storage.</p> <p>Vehicle Detection Module:</p> ModelModel Download Link mAP 0.5:0.95 GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-YOLOE-S_vehicleInference Model/Trained Model 61.3 15.4 178.4 28.79 Vehicle detection model based on PP-YOLOE PP-YOLOE-L_vehicleInference Model/Trained Model 63.9 32.6 775.6 196.02 <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the PPVehicle validation set. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Vehicle Attribute Recognition Module:</p> ModelModel Download Link mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_vehicle_attributeInference Model/Trained Model 91.7 3.84845 9.23735 6.7 M PP-LCNet_x1_0_vehicle_attribute is a lightweight vehicle attribute recognition model based on PP-LCNet. <p>Note: The above accuracy metrics are mA on the VeRi dataset. GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained models provided by PaddleX can quickly demonstrate results. You can experience the effects of the vehicle attribute recognition pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#21-online-experience","title":"2.1 Online Experience","text":"<p>Not supported yet.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the vehicle attribute recognition pipeline locally, ensure you have installed the PaddleX wheel package according to the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#221-experience-via-command-line","title":"2.2.1 Experience via Command Line","text":"<p>You can quickly experience the vehicle attribute recognition pipeline with a single command. Use the test file and replace <code>--input</code> with the local path for prediction.</p> <p><pre><code>paddlex --pipeline vehicle_attribute_recognition --input vehicle_attribute_002.jpg --device gpu:0\n</code></pre> Parameter Description:</p> <pre><code>--pipeline: The name of the pipeline, here it is the vehicle attribute recognition pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The index of the GPU to use (e.g., gpu:0 means using the first GPU, gpu:1,2 means using the second and third GPUs). You can also choose to use the CPU (--device cpu).\n</code></pre> <p>When executing the above Python script, the default vehicle attribute recognition pipeline configuration file is loaded. If you need a custom configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config vehicle_attribute_recognition\n</code></pre> <p>After execution, the vehicle attribute recognition pipeline configuration file will be saved in the current directory. If you wish to specify a custom save location, you can run the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config vehicle_attribute_recognition --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the saved path of the configuration file to make it effective. For example, if the saved path of the configuration file is <code>./vehicle_attribute_recognition.yaml</code>, just execute:</p> <pre><code>paddlex --pipeline ./vehicle_attribute_recognition.yaml --input vehicle_attribute_002.jpg --device gpu:0\n</code></pre> <p>Among them, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, and the parameters in the configuration file will be used. If parameters are still specified, the specified parameters will take precedence.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#222-integrating-via-python-script","title":"2.2.2 Integrating via Python Script","text":"<p>A few lines of code suffice for rapid inference on the production line, taking the vehicle attribute recognition pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"vehicle_attribute_recognition\")\n\noutput = pipeline.predict(\"vehicle_attribute_002.jpg\")\nfor res in output:\n    res.print()  ## Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  ## Save the visualized result image\n    res.save_to_json(\"./output/\")  ## Save the structured output of the prediction\n</code></pre> The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Parameter Type Default Value <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be a pipeline supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available when the pipeline supports high-performance inference. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the vehicle attribute recognition pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods. Specific examples are as follows:</p> Parameter Type Description Python Var Supports directly passing in Python variables, such as image data represented by numpy.ndarray. <code>str</code> Supports passing in the file path of the data to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing in the URL of the data file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing in a local directory, which should contain data files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing in a dictionary type, where the key needs to correspond to the specific task, such as \"img\" for the vehicle attribute recognition task, and the value of the dictionary supports the above data types, for example: <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing in a list, where the elements of the list need to be the above data types, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list representing a set of prediction results.</p> <p>(4) Processing the Prediction Results: The prediction result for each sample is in <code>dict</code> format, which supports printing or saving to a file. The supported file types for saving depend on the specific pipeline, such as:</p> Method Description Method Parameters print Print results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only effective when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only effective when format_json is True, default is False; save_to_json Save results as a json file <code>- save_path</code>: str, the path to save the file, when it is a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Save results as an image file <code>- save_path</code>: str, the path to save the file, when it is a directory, the saved file name is consistent with the input file type; <p>If you have obtained the configuration file, you can customize the configurations for the vehicle attribute recognition pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of your pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/vehicle_attribute_recognition.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/vehicle_attribute_recognition.yaml\")\noutput = pipeline.predict(\"vehicle_attribute_002.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the visualized result image\n    res.save_to_json(\"./output/\")  # Save the structured output of the prediction\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the vehicle attribute recognition pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to directly apply the vehicle attribute recognition pipeline in your Python project, you can refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API reference and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Get vehicle attribute recognition results.</p> <p><code>POST /vehicle-attribute-recognition</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>vehicles</code> <code>array</code> Information about the vehicle's location and attributes. <code>image</code> <code>string</code> The vehicle attribute recognition result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>vehicles</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> The location of the vehicle. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>attributes</code> <code>array</code> The vehicle attributes. <code>score</code> <code>number</code> The detection score. <p>Each element in <code>attributes</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>label</code> <code>string</code> The label of the attribute. <code>score</code> <code>number</code> The classification score. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/vehicle-attribute-recognition\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected vehicles:\")\nprint(result[\"vehicles\"])\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method where computing and data processing functions are placed on the user's device itself, allowing the device to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide. You can choose an appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the vehicle attribute recognition pipeline do not meet your expectations in terms of accuracy or speed for your specific scenario, you can try to further fine-tune the existing models using your own data from specific domains or application scenarios to enhance the recognition performance of the vehicle attribute recognition pipeline in your context.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the vehicle attribute recognition pipeline includes both a vehicle attribute recognition module and a vehicle detection module, the suboptimal performance of the pipeline may stem from either module. You can analyze images with poor recognition results. If during the analysis, you find that many main targets are not detected, it may indicate deficiencies in the vehicle detection model. In this case, you need to refer to the Custom Development section in the Vehicle Detection Module Development Tutorial and use your private dataset to fine-tune the vehicle detection model. If the detected main attributes are incorrectly recognized, you need to refer to the Custom Development section in the Vehicle Attribute Recognition Module Development Tutorial and use your private dataset to fine-tune the vehicle attribute recognition model.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weight files.</p> <p>To use the fine-tuned model weights, you only need to modify the pipeline configuration file by replacing the path to the default model weights with the local path to the fine-tuned model weights:</p> <p><pre><code>......\nPipeline:\n  det_model: PP-YOLOE-L_vehicle\n  cls_model: PP-LCNet_x1_0_vehicle_attribute   # Can be modified to the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Subsequently, refer to the command-line or Python script methods in the local experience, and load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modifying the <code>--device</code> parameter allows seamless switching between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference with the vehicle attribute recognition pipeline, the command is:</p> <p><pre><code>paddlex --pipeline vehicle_attribute_recognition --input vehicle_attribute_002.jpg --device gpu:0\n</code></pre> At this point, if you want to switch the hardware to Ascend NPU, you only need to change <code>--device</code> to npu:0:</p> <p><pre><code>paddlex --pipeline vehicle_attribute_recognition --input vehicle_attribute_002.jpg --device npu:0\n</code></pre> If you want to use the vehicle attribute recognition pipeline on more types of hardware, please refer to the PaddleX Multi-device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html","title":"PP-ChatOCRv3-doc Pipeline utorial","text":""},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#1-introduction-to-pp-chatocrv3-doc-pipeline","title":"1. Introduction to PP-ChatOCRv3-doc Pipeline","text":"<p>PP-ChatOCRv3-doc is a unique intelligent analysis solution for documents and images developed by PaddlePaddle. It combines Large Language Models (LLM) and OCR technology to provide a one-stop solution for complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition. By integrating with ERNIE Bot, it fuses massive data and knowledge to achieve high accuracy and wide applicability.</p> <p></p> <p>The PP-ChatOCRv3-doc pipeline includes modules for Table Structure Recognition, Layout Region Detection, Text Detection, Text Recognition, Seal Text Detection, Text Image Rectification, and Document Image Orientation Classification.</p> <p>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, choose a model with faster inference speed. If you prioritize model storage size, choose a model with a smaller storage size. Some benchmarks for these models are as follows:</p>  \ud83d\udc49Model List Details <p>Table Structure Recognition Module Models:</p> ModelModel Download Link Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description SLANetInference Model/Trained Model 59.52 522.536 1845.37 6.9 M SLANet is a table structure recognition model developed by Baidu PaddleX Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information. SLANet_plusInference Model/Trained Model 63.69 522.536 1845.37 6.9 M SLANet_plus is an enhanced version of SLANet, the table structure recognition model developed by Baidu PaddleX Team. Compared to SLANet, SLANet_plus significantly improves the recognition ability for wireless and complex tables and reduces the model's sensitivity to the accuracy of table positioning, enabling more accurate recognition even with offset table positioning. <p>Note: The above accuracy metrics are measured on PaddleX's internally built English table recognition dataset. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Layout Detection Module Models:</p> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1xInference Model/Trained Model 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_tableInference Model/Trained Model 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3clsInference Model/Trained Model 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17clsInference Model/Trained Model 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3clsInference Model/Trained Model 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17clsInference Model/Trained Model 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3clsInference Model/Trained Model 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout region analysis dataset, containing 10,000 images of common document types, including English and Chinese papers, magazines, research reports, etc. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_detInference Model/Trained Model 82.69 83.3501 2434.01 109 PP-OCRv4's server-side text detection model, featuring higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Trained Model 77.79 10.6923 120.177 4.7 PP-OCRv4's mobile text detection model, optimized for efficiency, suitable for deployment on edge devices <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten texts, with 500 images for detection. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Recognition Module Models:</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4 is the next version of Baidu PaddlePaddle's self-developed text recognition model PP-OCRv3. By introducing data augmentation schemes and GTC-NRTR guidance branches, it further improves text recognition accuracy without compromising inference speed. The model offers both server (server) and mobile (mobile) versions to meet industrial needs in different scenarios. PP-OCRv4_server_recInference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten texts, with 11,000 images for text recognition. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description ch_SVTRv2_recInference Model/Trained Model 68.81 8.36801 165.706 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team at the Vision and Learning Lab (FVL) of Fudan University. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 6% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the A-list.  <p>Note: The evaluation set for the above accuracy metrics is the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task A-list. GPU inference time is based on NVIDIA Tesla T4 with FP32 precision. CPU inference speed is based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description ch_RepSVTR_recInference Model/Trained Model 65.07 10.5047 51.5647 22.1 M  The RepSVTR text recognition model is a mobile-oriented text recognition model based on SVTRv2. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 2.5% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the B-list, while maintaining similar inference speed.  <p>Note: The evaluation set for the above accuracy metrics is the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task B-list. GPU inference time is based on NVIDIA Tesla T4 with FP32 precision. CPU inference speed is based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Seal Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Trained Model 98.21 84.341 2425.06 109 PP-OCRv4's server-side seal text detection model, featuring higher accuracy, suitable for deployment on better-equipped servers PP-OCRv4_mobile_seal_detInference Model/Trained Model 96.47 10.5878 131.813 4.6 PP-OCRv4's mobile seal text detection model, offering higher efficiency, suitable for deployment on edge devices <p>Note: The above accuracy metrics are evaluated on a self-built dataset containing 500 circular seal images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Image Rectification Module Models:</p> ModelModel Download Link MS-SSIM (%) Model Size (M) Description UVDocInference Model/Trained Model 54.40 30.3 M High-precision text image rectification model <p>The accuracy metrics of the models are measured from the DocUNet benchmark.</p> <p>Document Image Orientation Classification Module Models:</p> ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Trained Model 99.06 3.84845 9.23735 7 A document image classification model based on PP-LCNet_x1_0, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 <p>Note: The above accuracy metrics are evaluated on a self-built dataset covering various scenarios such as certificates and documents, containing 1000 images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX's pre-trained model pipelines can be quickly experienced. You can experience the effect of the Document Scene Information Extraction v3 pipeline online or locally using Python.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effect of the Document Scene Information Extraction v3 pipeline, using the official demo images for recognition, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the models in the pipeline online.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the PP-ChatOCRv3 pipeline locally, please ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Guide.</p> <p>A few lines of code are all you need to complete the quick inference of the pipeline. Using the test file, taking the PP-ChatOCRv3-doc pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"PP-ChatOCRv3-doc\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # Please enter your ak and sk; otherwise, the large model cannot be invoked.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Please enter your access_token; otherwise, the large model cannot be invoked.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/contract.pdf\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\n\nchat_result = pipeline.chat(\n    key_list=[\"\u200b\u4e59\u65b9\u200b\", \"\u200b\u624b\u673a\u53f7\u200b\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> Note: Currently, the large language model only supports Ernie. You can obtain the relevant ak/sk (access_token) on the Baidu Cloud Qianfan Platform or Baidu AIStudio Community. If you use the Baidu Cloud Qianfan Platform, you can refer to the AK and SK Authentication API Calling Process to obtain ak/sk. If you use Baidu AIStudio Community, you can obtain the access_token from the Baidu AIStudio Community Access Token.</p> <p>After running, the output is as follows:</p> <pre><code>{'chat_res': {'\u200b\u4e59\u65b9\u200b': '\u200b\u80a1\u4efd\u200b\u6d4b\u8bd5\u200b\u6709\u9650\u516c\u53f8\u200b', '\u200b\u624b\u673a\u53f7\u200b': '19331729920'}, 'prompt': ''}\n</code></pre> <p>In the above Python script, the following steps are executed:</p> <p>(1) Call the <code>create_pipeline</code> to instantiate a PP-ChatOCRv3-doc pipeline object, related parameters descriptions are as follows:</p> Parameter Type Default Description <code>pipeline</code> str None Pipeline name or pipeline configuration file path. If it's a pipeline name, it must be supported by PaddleX; <code>llm_name</code> str \"ernie-3.5\" Large Language Model name, we support <code>ernie-4.0</code> and <code>ernie-3.5</code>, with more models on the way. <code>llm_params</code> dict <code>{}</code> API configuration; <code>device(kwargs)</code> str/<code>None</code> <code>None</code> Running device, support <code>cpu</code>, <code>gpu</code>, <code>gpu:0</code>, etc. <code>None</code> meaning automatic selection; <p>(2) Call the <code>visual_predict</code> of the PP-ChatOCRv3-doc pipeline object to visual predict, related parameters descriptions are as follows:</p> Parameter Type Default Description <code>input</code> Python Var - Support to pass Python variables directly, such as <code>numpy.ndarray</code> representing image data; <code>input</code> str - Support to pass the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>; <code>input</code> str - Support to pass the URL of the file to be predicted, such as: <code>https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/contract.pdf</code>; <code>input</code> str - Support to pass the local directory, which should contain files to be predicted, such as: <code>/root/data/</code>; <code>input</code> dict - Support to pass a dictionary, where the key needs to correspond to the specific pipeline, such as: <code>{\"img\": \"/root/data1\"}</code>\uff1b <code>input</code> list - Support to pass a list, where the elements must be of the above types of data, such as: <code>[numpy.ndarray, numpy.ndarray]</code>\uff0c<code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>\uff0c<code>[\"/root/data1\", \"/root/data2\"]</code>\uff0c<code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>\uff1b <code>use_doc_image_ori_cls_model</code> bool <code>True</code> Whether or not to use the orientation classification model; <code>use_doc_image_unwarp_model</code> bool <code>True</code> Whether or not to use the unwarp model; <code>use_seal_text_det_model</code> bool <code>True</code> Whether or not to use the seal text detection model; <p>(3) Call the relevant functions of prediction object to save the prediction results. The related functions are as follows:</p> Function Parameter Description <code>save_to_img</code> <code>save_path</code> Save OCR prediction results, layout results, and table recognition results as image files, with the parameter <code>save_path</code> used to specify the save path; <code>save_to_html</code> <code>save_path</code> Save the table recognition results as an HTML file, with the parameter 'save_path' used to specify the save path; <code>save_to_xlsx</code> <code>save_path</code> Save the table recognition results as an Excel file, with the parameter 'save_path' used to specify the save path; <p>(4) Call the <code>chat</code> of PP-ChatOCRv3-doc pipeline object to query information with LLM, related parameters are described as follows:</p> Parameter Type Default Description <code>key_list</code> str - Keywords used to query. A string composed of multiple keywords with \",\" as separators, such as \"Party B, phone number\"; <code>key_list</code> list - Keywords used to query. A list composed of multiple keywords. <p>(3) Obtain prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through calls. The <code>predict</code> method predicts data in batches, so the prediction results are represented as a list of prediction results.</p> <p>(4) Interact with the large model by calling the <code>predict.chat</code> method, which takes as input keywords (multiple keywords are supported) for information extraction. The prediction results are represented as a list of information extraction results.</p> <p>(5) Process the prediction results: The prediction result for each sample is in the form of a dict, which supports printing or saving to a file. The supported file types depend on the specific pipeline, such as:</p> Method Description Method Parameters save_to_img Saves layout analysis, table recognition, etc. results as image files. <code>save_path</code>: str, the file path to save. save_to_html Saves table recognition results as HTML files. <code>save_path</code>: str, the file path to save. save_to_xlsx Saves table recognition results as Excel files. <code>save_path</code>: str, the file path to save. <p>When executing the above command, the default Pipeline configuration file is loaded. If you need to customize the configuration file, you can use the following command to obtain it:</p> <pre><code>paddlex --get_pipeline_config PP-ChatOCRv3-doc\n</code></pre> <p>After execution, the configuration file for the PP-ChatOCRv3-doc pipeline will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <p><pre><code>paddlex --get_pipeline_config PP-ChatOCRv3-doc --save_path ./my_path\n</code></pre> After obtaining the configuration file, you can customize the various configurations of the PP-ChatOCRv3-doc pipeline:</p> <pre><code>Pipeline:\n  layout_model: RT-DETR-H_layout_3cls\n  table_model: SLANet_plus\n  text_det_model: PP-OCRv4_server_det\n  text_rec_model: PP-OCRv4_server_rec\n  seal_text_det_model: PP-OCRv4_server_seal_det\n  doc_image_ori_cls_model: null\n  doc_image_unwarp_model: null\n  llm_name: \"ernie-3.5\"\n  llm_params:\n    api_type: qianfan\n    ak:\n    sk:\n</code></pre> <p>In the above configuration, you can modify the models loaded by each module of the pipeline, as well as the large language model used. Please refer to the module documentation for the list of supported models for each module, and the list of supported large language models includes: ernie-4.0, ernie-3.5, ernie-3.5-8k, ernie-lite, ernie-tiny-8k, ernie-speed, ernie-speed-128k, ernie-char-8k.</p> <p>After making modifications, simply update the <code>pipeline</code> parameter value in the <code>create_pipeline</code> method to the path of your pipeline configuration file to apply the configuration.</p> <p>For example, if your configuration file is saved at <code>./my_path/PP-ChatOCRv3-doc.yaml</code>, you would execute:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"./my_path/PP-ChatOCRv3-doc.yaml\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # Please enter your ak and sk; otherwise, the large model cannot be invoked.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Please enter your access_token; otherwise, the large model cannot be invoked.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/contract.pdf\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\n\nchat_result = pipeline.chat(\n    key_list=[\"\u200b\u4e59\u65b9\u200b\", \"\u200b\u624b\u673a\u53f7\u200b\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, you can refer to the example code in 2.2 Local Experience.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics (especially response speed) of deployment strategies to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing to significantly speed up the end-to-end process. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>analyzeImages</code></li> </ul> <p>Analyze images using computer vision models to obtain OCR, table recognition results, and extract key information from the images.</p> <p><code>POST /chatocr-vision</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> The URL of an accessible image file or PDF file, or the Base64 encoded content of the above file types. For PDF files with more than 10 pages, only the first 10 pages will be used. Yes <code>fileType</code> <code>integer</code> File type. <code>0</code> represents PDF files, <code>1</code> represents image files. If this property is not present in the request body, the service will attempt to infer the file type automatically based on the URL. No <code>useImgOrientationCls</code> <code>boolean</code> Whether to enable document image orientation classification. This feature is enabled by default. No <code>useImgUnwrapping</code> <code>boolean</code> Whether to enable text image correction. This feature is enabled by default. No <code>useSealTextDet</code> <code>boolean</code> Whether to enable seal text detection. This feature is enabled by default. No <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Properties of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the text detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>visionResults</code> <code>array</code> Analysis results obtained using the computer vision model. The array length is 1 (for image input) or the smaller of the number of document pages and 10 (for PDF input). For PDF input, each element in the array represents the processing result of each page in the PDF file in sequence. <code>visionInfo</code> <code>object</code> Key information in the image, which can be used as input for other operations. <p>Each element in <code>visionResults</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>texts</code> <code>array</code> Text locations, contents, and scores. <code>tables</code> <code>array</code> Table locations and contents. <code>inputImage</code> <code>string</code> Input image. The image is in JPEG format and encoded in Base64. <code>ocrImage</code> <code>string</code> OCR result image. The image is in JPEG format and encoded in Base64. <code>layoutImage</code> <code>string</code> Layout area detection result image. The image is in JPEG format and encoded in Base64. <p>Each element in <code>texts</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>poly</code> <code>array</code> Text location. The elements in the array are the vertex coordinates of the polygon enclosing the text in sequence. <code>text</code> <code>string</code> Text content. <code>score</code> <code>number</code> Text recognition score. <p>Each element in <code>tables</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> Table location. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box in sequence. <code>html</code> <code>string</code> Table recognition result in HTML format. <ul> <li><code>buildVectorStore</code></li> </ul> <p>Builds a vector database.</p> <p><code>POST /chatocr-vector</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>visionInfo</code> <code>object</code> Key information from the image. Provided by the <code>analyzeImages</code> operation. Yes <code>minChars</code> <code>integer</code> Minimum data length to enable the vector database. No <code>llmRequestInterval</code> <code>number</code> Interval time for calling the large language model API. No <code>llmName</code> <code>string</code> Name of the large language model. No <code>llmParams</code> <code>object</code> Parameters for the large language model API. No <p>Currently, <code>llmParams</code> can take one of the following forms:</p> <pre><code>{\n\"apiType\": \"qianfan\",\n\"apiKey\": \"{Qianfan Platform API key}\",\n\"secretKey\": \"{Qianfan Platform secret key}\"\n}\n</code></pre> <pre><code>{\n\"apiType\": \"aistudio\",\n\"accessToken\": \"{AI Studio access token}\"\n}\n</code></pre> <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>vectorStore</code> <code>string</code> Serialized result of the vector database, which can be used as input for other operations. <ul> <li><code>retrieveKnowledge</code></li> </ul> <p>Perform knowledge retrieval.</p> <p><code>POST /chatocr-retrieval</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>keys</code> <code>array</code> List of keywords. Yes <code>vectorStore</code> <code>string</code> Serialized result of the vector database. Provided by the <code>buildVectorStore</code> operation. Yes <code>llmName</code> <code>string</code> Name of the large language model. No <code>llmParams</code> <code>object</code> API parameters for the large language model. No <p>Currently, <code>llmParams</code> can take one of the following forms:</p> <pre><code>{\n\"apiType\": \"qianfan\",\n\"apiKey\": \"{Qianfan Platform API key}\",\n\"secretKey\": \"{Qianfan Platform secret key}\"\n}\n</code></pre> <pre><code>{\n\"apiType\": \"aistudio\",\n\"accessToken\": \"{AI Studio access token}\"\n}\n</code></pre> <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>retrievalResult</code> <code>string</code> The result of knowledge retrieval, which can be used as input for other operations. <ul> <li><code>chat</code></li> </ul> <p>Interact with large language models to extract key information.</p> <p><code>POST /chatocr-chat</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>keys</code> <code>array</code> List of keywords. Yes <code>visionInfo</code> <code>object</code> Key information from images. Provided by the <code>analyzeImages</code> operation. Yes <code>vectorStore</code> <code>string</code> Serialized result of the vector database. Provided by the <code>buildVectorStore</code> operation. No <code>retrievalResult</code> <code>string</code> Results of knowledge retrieval. Provided by the <code>retrieveKnowledge</code> operation. No <code>taskDescription</code> <code>string</code> Task prompt. No <code>rules</code> <code>string</code> Custom extraction rules, e.g., for output formatting. No <code>fewShot</code> <code>string</code> Example prompts. No <code>llmName</code> <code>string</code> Name of the large language model. No <code>llmParams</code> <code>object</code> API parameters for the large language model. No <code>returnPrompts</code> <code>boolean</code> Whether to return the prompts used. Enabled by default. No <p>Currently, <code>llmParams</code> can take one of the following forms:</p> <pre><code>{\n\"apiType\": \"qianfan\",\n\"apiKey\": \"{Qianfan Platform API key}\",\n\"secretKey\": \"{Qianfan Platform secret key}\"\n}\n</code></pre> <pre><code>{\n\"apiType\": \"aistudio\",\n\"accessToken\": \"{AI Studio access token}\"\n}\n</code></pre> <ul> <li>On successful request processing, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>chatResult</code> <code>object</code> Extracted key information. <code>prompts</code> <code>object</code> Prompts used. <p>Properties of <code>prompts</code>:</p> Name Type Description <code>ocr</code> <code>string</code> OCR prompt. <code>table</code> <code>string</code> Table prompt. <code>html</code> <code>string</code> HTML prompt. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport pprint\nimport sys\n\nimport requests\n\nAPI_BASE_URL = \"http://0.0.0.0:8080\"\nAPI_KEY = \"{Qianfan Platform API key}\"\nSECRET_KEY = \"{Qianfan Platform secret key}\"\nLLM_NAME = \"ernie-3.5\"\nLLM_PARAMS = {\n    \"apiType\": \"qianfan\",\n    \"apiKey\": API_KEY,\n    \"secretKey\": SECRET_KEY,\n}\n\nfile_path = \"./demo.jpg\"\nkeys = [\"\u200b\u7535\u8bdd\u200b\"]\n\nwith open(file_path, \"rb\") as file:\n    file_bytes = file.read()\n    file_data = base64.b64encode(file_bytes).decode(\"ascii\")\n\npayload = {\n    \"file\": file_data,\n    \"fileType\": 1,\n    \"useImgOrientationCls\": True,\n    \"useImgUnwrapping\": True,\n    \"useSealTextDet\": True,\n}\nresp_vision = requests.post(url=f\"{API_BASE_URL}/chatocr-vision\", json=payload)\nif resp_vision.status_code != 200:\n    print(\n        f\"Request to chatocr-vision failed with status code {resp_vision.status_code}.\"\n    )\n    pprint.pp(resp_vision.json())\n    sys.exit(1)\nresult_vision = resp_vision.json()[\"result\"]\n\nfor i, res in enumerate(result_vision[\"visionResults\"]):\n    print(\"Texts:\")\n    pprint.pp(res[\"texts\"])\n    print(\"Tables:\")\n    pprint.pp(res[\"tables\"])\n    ocr_img_path = f\"ocr_{i}.jpg\"\n    with open(ocr_img_path, \"wb\") as f:\n        f.write(base64.b64decode(res[\"ocrImage\"]))\n    layout_img_path = f\"layout_{i}.jpg\"\n    with open(layout_img_path, \"wb\") as f:\n        f.write(base64.b64decode(res[\"layoutImage\"]))\n    print(f\"Output images saved at {ocr_img_path} and {layout_img_path}\")\n\npayload = {\n    \"visionInfo\": result_vision[\"visionInfo\"],\n    \"minChars\": 200,\n    \"llmRequestInterval\": 1000,\n    \"llmName\": LLM_NAME,\n    \"llmParams\": LLM_PARAMS,\n}\nresp_vector = requests.post(url=f\"{API_BASE_URL}/chatocr-vector\", json=payload)\nif resp_vector.status_code != 200:\n    print(\n        f\"Request to chatocr-vector failed with status code {resp_vector.status_code}.\"\n    )\n    pprint.pp(resp_vector.json())\n    sys.exit(1)\nresult_vector = resp_vector.json()[\"result\"]\n\npayload = {\n    \"keys\": keys,\n    \"vectorStore\": result_vector[\"vectorStore\"],\n    \"llmName\": LLM_NAME,\n    \"llmParams\": LLM_PARAMS,\n}\nresp_retrieval = requests.post(url=f\"{API_BASE_URL}/chatocr-retrieval\", json=payload)\nif resp_retrieval.status_code != 200:\n    print(\n        f\"Request to chatocr-retrieval failed with status code {resp_retrieval.status_code}.\"\n    )\n    pprint.pp(resp_retrieval.json())\n    sys.exit(1)\nresult_retrieval = resp_retrieval.json()[\"result\"]\n\npayload = {\n    \"keys\": keys,\n    \"visionInfo\": result_vision[\"visionInfo\"],\n    \"vectorStore\": result_vector[\"vectorStore\"],\n    \"retrievalResult\": result_retrieval[\"retrievalResult\"],\n    \"taskDescription\": \"\",\n    \"rules\": \"\",\n    \"fewShot\": \"\",\n    \"llmName\": LLM_NAME,\n    \"llmParams\": LLM_PARAMS,\n    \"returnPrompts\": True,\n}\nresp_chat = requests.post(url=f\"{API_BASE_URL}/chatocr-chat\", json=payload)\nif resp_chat.status_code != 200:\n    print(\n        f\"Request to chatocr-chat failed with status code {resp_chat.status_code}.\"\n    )\n    pprint.pp(resp_chat.json())\n    sys.exit(1)\nresult_chat = resp_chat.json()[\"result\"]\nprint(\"\\nPrompts:\")\npprint.pp(result_chat[\"prompts\"])\nprint(\"Final result:\")\nprint(result_chat[\"chatResult\"])\n</code></pre> Note: Please fill in your API key and secret key at `API_KEY` and `SECRET_KEY`. <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the PP-ChatOCRv3-doc Pipeline do not meet your requirements in terms of accuracy or speed for your specific scenario, you can attempt to further fine-tune the existing models using your own domain-specific or application-specific data to enhance the recognition performance of the general table recognition pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the PP-ChatOCRv3-doc Pipeline comprises six modules, unsatisfactory performance may stem from any of these modules (note that the text image rectification module does not support customization at this time).</p> <p>You can analyze images with poor recognition results and follow the guidelines below for analysis and model fine-tuning:</p> <ul> <li> <p>Incorrect table structure detection (e.g., row/column misidentification, cell position errors) may indicate deficiencies in the table structure recognition module. You need to refer to the Customization section in the Table Structure Recognition Module Development Tutorial and fine-tune the table structure recognition model using your private dataset.</p> </li> <li> <p>Misplaced layout elements (e.g., incorrect positioning of tables or seals) may suggest issues with the layout detection module. Consult the Customization section in the Layout Detection Module Development Tutorial and fine-tune the layout detection model with your private dataset.</p> </li> <li> <p>Frequent undetected text (i.e., text leakage) may indicate limitations in the text detection model. Refer to the Customization section in the Text Detection Module Development Tutorial and fine-tune the text detection model using your private dataset.</p> </li> <li> <p>High text recognition errors (i.e., recognized text content does not match the actual text) suggest that the text recognition model requires improvement. Follow the Customization section in the Text Recognition Module Development Tutorial to fine-tune the text recognition model.</p> </li> <li> <p>Frequent recognition errors in detected seal text indicate that the seal text detection model needs further refinement. Consult the Customization section in the Seal Text Detection Module Development Tutorials to fine-tune the seal text detection model.</p> </li> <li> <p>Frequent misidentifications of document or certificate orientations with text regions suggest that the document image orientation classification model requires improvement. Refer to the Customization section in the Document Image Orientation Classification Module Development Tutorial to fine-tune the document image orientation classification model.</p> </li> </ul>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#42-model-deployment","title":"4.2 Model Deployment","text":"<p>After fine-tuning your models using your private dataset, you will obtain local model weights files.</p> <p>To use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local paths of the default model weights with those of your fine-tuned models:</p> <pre><code>......\nPipeline:\n  layout_model: RT-DETR-H_layout_3cls  # Replace with the local path of your fine-tuned model\n  table_model: SLANet_plus  # Replace with the local path of your fine-tuned model\n  text_det_model: PP-OCRv4_server_det  # Replace with the local path of your fine-tuned model\n  text_rec_model: PP-OCRv4_server_rec  # Replace with the local path of your fine-tuned model\n  seal_text_det_model: PP-OCRv4_server_seal_det  # Replace with the local path of your fine-tuned model\n  doc_image_ori_cls_model: null   # Replace with the local path of your fine-tuned model if applicable\n  doc_image_unwarp_model: null   # Replace with the local path of your fine-tuned model if applicable\n......\n</code></pre> <p>Subsequently, load the modified pipeline configuration file using the command-line interface or Python script as described in the local experience section.</p>"},{"location":"en/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Seamless switching between different hardware can be achieved by simply setting the <code>--device</code> parameter.</p> <p>For example, to perform inference using the PP-ChatOCRv3-doc Pipeline on an NVIDIA GPU. At this point, if you wish to switch the hardware to Ascend NPU, simply modify the <code>--device</code> in the script to <code>npu</code>:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(\n    pipeline=\"PP-ChatOCRv3-doc\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"},\n    device=\"npu:0\" # gpu:0 --&gt; npu:0\n    )\n</code></pre> <p>If you want to use the PP-ChatOCRv3-doc Pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html","title":"General OCR Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#1-introduction-to-ocr-pipeline","title":"1. Introduction to OCR Pipeline","text":"<p>OCR (Optical Character Recognition) is a technology that converts text in images into editable text. It is widely used in document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols.</p> <p>The General OCR Pipeline is designed to solve text recognition tasks, extracting text information from images and outputting it in text form. PP-OCRv4 is an end-to-end OCR system that achieves millisecond-level text content prediction on CPUs, reaching state-of-the-art (SOTA) performance in open-source projects for general scenarios. Based on this project, developers from academia, industry, and research have rapidly deployed various OCR applications across fields such as general use, manufacturing, finance, transportation, and more.</p> <p></p> <p>The General OCR Pipeline comprises a text detection module and a text recognition module, each containing several models. The specific models to use can be selected based on the benchmark data provided below. If you prioritize model accuracy, choose models with higher accuracy. If you prioritize inference speed, choose models with faster inference. If you prioritize model size, choose models with smaller storage requirements.</p> <p>Text detection module:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_detInference Model/Trained Model 82.69 83.3501 2434.01 109 The server-side text detection model of PP-OCRv4, featuring higher accuracy and suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Trained Model 77.79 10.6923 120.177 4.7 The mobile text detection model of PP-OCRv4, optimized for efficiency and suitable for deployment on edge devices <p>Text recognition module:</p> ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4, developed by Baidu's PaddlePaddle Vision Team, is the next version of the PP-OCRv3 text recognition model. By introducing data augmentation schemes, GTC-NRTR guidance branches, and other strategies, it further improves text recognition accuracy without compromising model inference speed. The model offers both server and mobile versions to meet industrial needs in different scenarios. PP-OCRv4_server_rec Inference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more, with 1.1w images for text recognition. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time Model Size (M) Description ch_SVTRv2_recInference Model/Trained Model 68.81 8.36801 165.706 73.9 M SVTRv2, a server-side text recognition model developed by the OpenOCR team at the Vision and Learning Lab (FVL) of Fudan University, also won first place in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge. Its A-rank end-to-end recognition accuracy is 6% higher than PP-OCRv4.  <p>Note: The evaluation set for the above accuracy metrics is the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge - Track 1 A-rank. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy(%) GPU Inference Time (ms) CPU Inference Time Model Size (M) Description ch_RepSVTR_recInference Model/Trained Model 65.07 10.5047 51.5647 22.1 M   RepSVTR, a mobile text recognition model based on SVTRv2, won first place in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge. Its B-rank end-to-end recognition accuracy is 2.5% higher than PP-OCRv4, with comparable inference speed. <p>Note: The evaluation set for the above accuracy metrics is the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge - Track 1 B-rank. GPU inference time for all models is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX provides pre-trained models for the OCR Pipeline, allowing you to quickly experience its effects. You can try the General OCR Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience the General OCR Pipeline online using the official demo images for recognition, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. You can download the deployment package from the cloud or use the local experience method in Section 2.2. If not satisfied, you can also use your private data to fine-tune the models in the pipeline online.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#22-local-experience","title":"2.2 Local Experience","text":"<p>\u2757 Before using the General OCR Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<ul> <li>Experience the OCR Pipeline with a single command:</li> </ul> <p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline OCR --input general_ocr_002.png --device gpu:0\n</code></pre> Parameter explanations:</p> <pre><code>--pipeline: The name of the pipeline, here it is OCR.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default OCR Pipeline configuration file is loaded. If you need to customize the configuration file, you can use the following command to obtain it:</p>  \ud83d\udc49 Click to expand <pre><code>paddlex --get_pipeline_config OCR\n</code></pre> <p>After execution, the OCR Pipeline configuration file will be saved in the current directory. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config OCR --save_path ./my_path --device gpu:0\n</code></pre> <p>After obtaining the Pipeline configuration file, replace <code>--pipeline</code> with the configuration file's save path to make the configuration file effective. For example, if the configuration file is saved as <code>./OCR.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./OCR.yaml --input general_ocr_002.png --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <pre><code>{'input_path': 'general_ocr_002.png', 'dt_polys': [[[5, 12], [88, 10], [88, 29], [5, 31]], [[208, 14], [249, 14], [249, 22], [208, 22]], [[695, 15], [824, 15], [824, 60], [695, 60]], [[158, 27], [355, 23], [356, 70], [159, 73]], [[421, 25], [659, 19], [660, 59], [422, 64]], [[337, 104], [460, 102], [460, 127], [337, 129]], [[486, 103], [650, 100], [650, 125], [486, 128]], [[675, 98], [835, 94], [835, 119], [675, 124]], [[64, 114], [192, 110], [192, 131], [64, 134]], [[210, 108], [318, 106], [318, 128], [210, 130]], [[82, 140], [214, 138], [214, 163], [82, 165]], [[226, 136], [328, 136], [328, 161], [226, 161]], [[404, 134], [432, 134], [432, 161], [404, 161]], [[509, 131], [570, 131], [570, 158], [509, 158]], [[730, 138], [771, 138], [771, 154], [730, 154]], [[806, 136], [817, 136], [817, 146], [806, 146]], [[342, 175], [470, 173], [470, 197], [342, 199]], [[486, 173], [616, 171], [616, 196], [486, 198]], [[677, 169], [813, 166], [813, 191], [677, 194]], [[65, 181], [170, 177], [171, 202], [66, 205]], [[96, 208], [171, 205], [172, 230], [97, 232]], [[336, 220], [476, 215], [476, 237], [336, 242]], [[507, 217], [554, 217], [554, 236], [507, 236]], [[87, 229], [204, 227], [204, 251], [87, 254]], [[344, 240], [483, 236], [483, 258], [344, 262]], [[66, 252], [174, 249], [174, 271], [66, 273]], [[75, 279], [264, 272], [265, 297], [76, 303]], [[459, 297], [581, 295], [581, 320], [459, 322]], [[101, 314], [210, 311], [210, 337], [101, 339]], [[68, 344], [165, 340], [166, 365], [69, 368]], [[345, 350], [662, 346], [662, 368], [345, 371]], [[100, 459], [832, 444], [832, 465], [100, 480]]], 'dt_scores': [0.8183103704439653, 0.7609575621092027, 0.8662357274035412, 0.8619508290334809, 0.8495855993183273, 0.8676840017933314, 0.8807986687956436, 0.822308525056085, 0.8686617037621976, 0.8279022169854463, 0.952332847006758, 0.8742692553015098, 0.8477013022907575, 0.8528771493227294, 0.7622965906848765, 0.8492388224448705, 0.8344203789965632, 0.8078477124353284, 0.6300434587457232, 0.8359967356998494, 0.7618617265751318, 0.9481573079350023, 0.8712182945408912, 0.837416955846334, 0.8292475059403851, 0.7860382856406026, 0.7350527486717117, 0.8701022267947695, 0.87172526903969, 0.8779847108088126, 0.7020437651809734, 0.6611684983372949], 'rec_text': ['www.997', '151', 'PASS', '\u200b\u767b\u673a\u724c\u200b', 'BOARDING', '\u200b\u8231\u4f4d\u200b CLASS', '\u200b\u5e8f\u53f7\u200bSERIALNO.', '\u200b\u5ea7\u4f4d\u53f7\u200bSEATNO', '\u200b\u822a\u73ed\u200b FLIGHT', '\u200b\u65e5\u671f\u200bDATE', 'MU 2379', '03DEC', 'W', '035', 'F', '1', '\u200b\u59cb\u53d1\u5730\u200bFROM', '\u200b\u767b\u673a\u53e3\u200b GATE', '\u200b\u767b\u673a\u200b\u65f6\u95f4\u200bBDT', '\u200b\u76ee\u7684\u5730\u200bTO', '\u200b\u798f\u5dde\u200b', 'TAIYUAN', 'G11', 'FUZHOU', '\u200b\u8eab\u4efd\u200b\u8bc6\u522b\u200bIDNO.', '\u200b\u59d3\u540d\u200bNAME', 'ZHANGQIWEI', '\u200b\u7968\u53f7\u200bTKTNO.', '\u200b\u5f20\u797a\u4f1f\u200b', '\u200b\u7968\u4ef7\u200bFARE', 'ETKT7813699238489/1', '\u200b\u767b\u673a\u53e3\u200b\u4e8e\u200b\u8d77\u98de\u524d\u200b10\u200b\u5206\u949f\u200b\u5173\u95ed\u200bGATESCLOSE1OMINUTESBEFOREDEPARTURETIME'], 'rec_score': [0.9617719054222107, 0.4199012815952301, 0.9652514457702637, 0.9978302121162415, 0.9853208661079407, 0.9445787072181702, 0.9714463949203491, 0.9841841459274292, 0.9564052224159241, 0.9959094524383545, 0.9386572241783142, 0.9825271368026733, 0.9356589317321777, 0.9985442161560059, 0.3965512812137604, 0.15236201882362366, 0.9976775050163269, 0.9547433257102966, 0.9974752068519592, 0.9646636843681335, 0.9907559156417847, 0.9895358681678772, 0.9374122023582458, 0.9909093379974365, 0.9796401262283325, 0.9899340271949768, 0.992210865020752, 0.9478569626808167, 0.9982215762138367, 0.9924325942993164, 0.9941263794898987, 0.96443772315979]}\n......\n</code></pre> <p>Among them, <code>dt_polys</code> is the detected text box coordinates, <code>dt_polys</code> is the detected text box coordinates, <code>dt_scores</code> is the confidence of the detected text box, <code>rec_text</code> is the detected text, <code>rec_score</code> is the detection Confidence in the text.</p> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#222-integration-via-python-script","title":"2.2.2 Integration via Python Script","text":"<ul> <li>Quickly perform inference on the production line with just a few lines of code, taking the general OCR production line as an example:</li> </ul> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"OCR\")\n\noutput = pipeline.predict(\"general_ocr_002.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> <p>\u2757 The results obtained from running the Python script are the same as those from the command line.</p> <p>The Python script above executes the following steps:</p> <p>\uff081\uff09Instantiate the OCR production line object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the production line or the path to the production line configuration file. If it is the name of the production line, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for production line model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the production line supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the OCR production line object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/OCR.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/OCR.yaml\")\noutput = pipeline.predict(\"general_ocr_002.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the general OCR pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the general OCR pipeline directly in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain OCR results from an image.</p> <p><code>POST /ocr</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Properties of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the text detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>texts</code> <code>array</code> Positions, contents, and scores of texts. <code>image</code> <code>string</code> OCR result image with detected text positions annotated. The image is in JPEG format and encoded in Base64. <p>Each element in <code>texts</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>poly</code> <code>array</code> Text position. Elements in the array are the vertex coordinates of the polygon enclosing the text. <code>text</code> <code>string</code> Text content. <code>score</code> <code>number</code> Text recognition score. <p>Example of <code>result</code>:</p> <pre><code>{\n\"texts\": [\n{\n\"poly\": [\n[\n444,\n244\n],\n[\n705,\n244\n],\n[\n705,\n311\n],\n[\n444,\n311\n]\n],\n\"text\": \"Beijing South Railway Station\",\n\"score\": 0.9\n},\n{\n\"poly\": [\n[\n992,\n248\n],\n[\n1263,\n251\n],\n[\n1263,\n318\n],\n[\n992,\n315\n]\n],\n\"text\": \"Tianjin Railway Station\",\n\"score\": 0.5\n}\n],\n\"image\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/ocr\"\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected texts:\")\nprint(result[\"texts\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string outputImagePath = \"./out.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/ocr\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"image\"];\n        std::string decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputImage(outPutImagePath, std::ios::binary | std::ios::out);\n        if (outputImage.is_open()) {\n            outputImage.write(reinterpret_cast&lt;char*&gt;(decodedImage.data()), decodedImage.size());\n            outputImage.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outPutImagePath &lt;&lt; std::endl;\n        }\n\n        auto texts = result[\"texts\"];\n        std::cout &lt;&lt; \"\\nDetected texts:\" &lt;&lt; std::endl;\n        for (const auto&amp; text : texts) {\n            std::cout &lt;&lt; text &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/ocr\";\n        String imagePath = \"./demo.jpg\";\n        String outputImagePath = \"./out.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String base64Image = result.get(\"image\").asText();\n                JsonNode texts = result.get(\"texts\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(base64Image);\n                try (FileOutputStream fos = new FileOutputStream(outputImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + outputImagePath);\n                System.out.println(\"\\nDetected texts: \" + texts.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/ocr\"\n    imagePath := \"./demo.jpg\"\n    outputImagePath := \"./out.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Image      string   `json:\"image\"`\n            Texts []map[string]interface{} `json:\"texts\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputImageData, err := base64.StdEncoding.DecodeString(respData.Result.Image)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputImagePath, outputImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", outputImagePath)\n    fmt.Println(\"\\nDetected texts:\")\n    for _, text := range respData.Result.Texts {\n        fmt.Println(text)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/ocr\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string outputImagePath = \"./out.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Image = jsonResponse[\"result\"][\"image\"].ToString();\n        byte[] outputImageBytes = Convert.FromBase64String(base64Image);\n\n        File.WriteAllBytes(outputImagePath, outputImageBytes);\n        Console.WriteLine($\"Output image saved at {outputImagePath}\");\n        Console.WriteLine(\"\\nDetected texts:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"texts\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/ocr'\nconst imagePath = './demo.jpg'\nconst outputImagePath = \"./out.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    const imageBuffer = Buffer.from(result[\"image\"], 'base64');\n    fs.writeFile(outputImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${outputImagePath}`);\n    });\n    console.log(\"\\nDetected texts:\");\n    console.log(result[\"texts\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/ocr\";\n$image_path = \"./demo.jpg\";\n$output_image_path = \"./out.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($output_image_path, base64_decode($result[\"image\"]));\necho \"Output image saved at \" . $output_image_path . \"\\n\";\necho \"\\nDetected texts:\\n\";\nprint_r($result[\"texts\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method based on your needs to proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general OCR pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing models using your own domain-specific or application-specific data to improve the recognition performance of the general OCR pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general OCR pipeline consists of two modules (text detection and text recognition), unsatisfactory performance may stem from either module.</p> <p>You can analyze images with poor recognition results. If you find that many texts are undetected (i.e., text miss detection), it may indicate that the text detection model needs improvement. You should refer to the Customization section in the Text Detection Module Development Tutorial and use your private dataset to fine-tune the text detection model. If many recognition errors occur in detected texts (i.e., the recognized text content does not match the actual text content), it suggests that the text recognition model requires further refinement. You should refer to the Customization section in the Text Recognition Module Development Tutorial and fine-tune the text recognition model.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weights files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local paths of the fine-tuned model weights to the corresponding positions in the pipeline configuration file:</p> <pre><code>......\nPipeline:\n  det_model: PP-OCRv4_server_det  # Can be replaced with the local path of the fine-tuned text detection model\n  det_device: \"gpu\"\n  rec_model: PP-OCRv4_server_rec  # Can be replaced with the local path of the fine-tuned text recognition model\n  rec_batch_size: 1\n  rec_device: \"gpu\"\n......\n</code></pre> <p>Then, refer to the command line method or Python script method in 2.2 Local Experience to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/OCR.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPU, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modifying the <code>--device</code> parameter allows seamless switching between different hardware.</p> <p>For example, if you are using an NVIDIA GPU for OCR pipeline inference, the Python command would be:</p> <p><pre><code>paddlex --pipeline OCR --input general_ocr_002.png --device gpu:0\n</code></pre> Now, if you want to switch the hardware to Ascend NPU, you only need to modify the <code>--device</code> in the Python command:</p> <pre><code>paddlex --pipeline OCR --input general_ocr_002.png --device npu:0\n</code></pre> <p>If you want to use the General OCR pipeline on more types of hardware, please refer to the PaddleX Multi-Hardware Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html","title":"Formula Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#1-introduction-to-the-formula-recognition-pipeline","title":"1. Introduction to the Formula Recognition Pipeline","text":"<p>Formula recognition is a technology that automatically identifies and extracts LaTeX formula content and its structure from documents or images. It is widely used in document editing and data analysis in fields such as mathematics, physics, and computer science. Leveraging computer vision and machine learning algorithms, formula recognition converts complex mathematical formula information into editable LaTeX format, facilitating further data processing and analysis for users.</p> <p></p> <p>The Formula Recognition Pipeline comprises a layout detection module and a formula recognition module.</p> <p>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model size, choose a model with a smaller storage footprint.</p> <p>Layout Detection Module Models:</p> Model NameModel Download Link mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.126 3827.25 470.2M <p>Note: The above accuracy metrics are evaluated on PaddleX's self-built layout detection dataset, containing 10,000 images. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Formula Recognition Module Models:</p> Model NameModel Download Link BLEU Score Normed Edit Distance ExpRate (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size LaTeX_OCR_recInference Model/Trained Model 0.8821 0.0823 40.01 - - 89.7 M <p>Note: The above accuracy metrics are measured on the LaTeX-OCR Formula Recognition Test Set. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX supports experiencing the effects of the formula recognition pipeline through command line or Python locally.</p> <p>Before using the formula recognition pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#21-experience-via-command-line","title":"2.1 Experience via Command Line","text":"<p>Experience the formula recognition pipeline with a single command, using the test file, and replace <code>--input</code> with your local path for prediction:</p> <pre><code>paddlex --pipeline formula_recognition --input general_formula_recognition.png --device gpu:0\n</code></pre> <p>Parameter Explanation:</p> <pre><code>--pipeline: The pipeline name, which is formula_recognition for this case.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). Alternatively, use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default formula recognition pipeline configuration file is loaded. If you need to customize the configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_config formula_recognition\n</code></pre> <p>After execution, the formula recognition pipeline configuration file will be saved in the current directory. If you wish to customize the save location, you can run the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config formula_recognition --save_path ./my_path\n</code></pre> <p>After obtaining the Pipeline configuration file, replace <code>--pipeline</code> with the configuration file's save path to make the configuration file effective. For example, if the configuration file is saved as  <code>./formula_recognition.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./formula_recognition.yaml --input general_formula_recognition.png --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If parameters are still specified, the specified parameters will take precedence.</p> <p>After execution, the result is:</p>  \ud83d\udc49Click to Expand <pre><code>{'input_path': 'general_formula_recognition.png', 'layout_result': {'input_path': 'general_formula_recognition.png', 'boxes': [{'cls_id': 3, 'label': 'number', 'score': 0.7580855488777161, 'coordinate': [1028.3635, 205.46213, 1038.953, 222.99033]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.8882032632827759, 'coordinate': [272.75305, 204.50894, 433.7473, 226.17996]}, {'cls_id': 2, 'label': 'text', 'score': 0.9685840606689453, 'coordinate': [272.75928, 282.17773, 1041.9316, 374.44687]}, {'cls_id': 2, 'label': 'text', 'score': 0.9559416770935059, 'coordinate': [272.39056, 385.54114, 1044.1521, 443.8598]}, {'cls_id': 2, 'label': 'text', 'score': 0.9610629081726074, 'coordinate': [272.40817, 467.2738, 1045.1033, 563.4855]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8916195034980774, 'coordinate': [503.45743, 594.6236, 1040.6804, 619.73895]}, {'cls_id': 2, 'label': 'text', 'score': 0.973675549030304, 'coordinate': [272.32007, 648.8599, 1040.8702, 775.15686]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9038916230201721, 'coordinate': [554.2307, 803.5825, 1040.4657, 855.3159]}, {'cls_id': 2, 'label': 'text', 'score': 0.9025381803512573, 'coordinate': [272.535, 875.1402, 573.1086, 898.3587]}, {'cls_id': 2, 'label': 'text', 'score': 0.8336610794067383, 'coordinate': [317.48013, 909.60864, 966.8498, 933.7868]}, {'cls_id': 2, 'label': 'text', 'score': 0.8779091238975525, 'coordinate': [19.704018, 653.322, 72.433235, 1215.1992]}, {'cls_id': 2, 'label': 'text', 'score': 0.8832409977912903, 'coordinate': [272.13028, 958.50806, 1039.7928, 1019.476]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9088466167449951, 'coordinate': [517.1226, 1042.3978, 1040.2208, 1095.7457]}, {'cls_id': 2, 'label': 'text', 'score': 0.9587949514389038, 'coordinate': [272.03336, 1112.9269, 1041.0201, 1206.8417]}, {'cls_id': 2, 'label': 'text', 'score': 0.8885666131973267, 'coordinate': [271.7495, 1231.8752, 710.44495, 1255.7981]}, {'cls_id': 7, 'label': 'formula', 'score': 0.8907185196876526, 'coordinate': [581.2295, 1287.4525, 1039.8014, 1312.772]}, {'cls_id': 2, 'label': 'text', 'score': 0.9559596180915833, 'coordinate': [273.1827, 1341.421, 1041.0299, 1401.7255]}, {'cls_id': 2, 'label': 'text', 'score': 0.875311553478241, 'coordinate': [272.8338, 1427.3711, 789.7108, 1451.1359]}, {'cls_id': 7, 'label': 'formula', 'score': 0.9152213931083679, 'coordinate': [524.9582, 1474.8136, 1041.6333, 1530.7142]}, {'cls_id': 2, 'label': 'text', 'score': 0.9584835767745972, 'coordinate': [272.81665, 1549.524, 1042.9962, 1608.7157]}]}, 'ocr_result': {}, 'table_result': [], 'dt_polys': [array([[ 503.45743,  594.6236 ],\n       [1040.6804 ,  594.6236 ],\n       [1040.6804 ,  619.73895],\n       [ 503.45743,  619.73895]], dtype=float32), array([[ 554.2307,  803.5825],\n       [1040.4657,  803.5825],\n       [1040.4657,  855.3159],\n       [ 554.2307,  855.3159]], dtype=float32), array([[ 517.1226, 1042.3978],\n       [1040.2208, 1042.3978],\n       [1040.2208, 1095.7457],\n       [ 517.1226, 1095.7457]], dtype=float32), array([[ 581.2295, 1287.4525],\n       [1039.8014, 1287.4525],\n       [1039.8014, 1312.772 ],\n       [ 581.2295, 1312.772 ]], dtype=float32), array([[ 524.9582, 1474.8136],\n       [1041.6333, 1474.8136],\n       [1041.6333, 1530.7142],\n       [ 524.9582, 1530.7142]], dtype=float32)], 'rec_formula': ['F({\\bf x})=C(F_{1}(x_{1}),\\cdot\\cdot\\cdot,F_{N}(x_{N})).\\qquad\\qquad\\qquad(1)', 'p(\\mathbf{x})=c(\\mathbf{u})\\prod_{i}p(x_{i}).\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\quad~~\\quad~~~~~~~~~~~~~~~(2)', 'H_{c}({\\bf x})=-\\int_{{\\bf{u}}}c({\\bf{u}})\\log c({\\bf{u}})d{\\bf{u}}.~~~~~~~~~~~~~~~~~~~~~(3)', 'I({\\bf x})=-H_{c}({\\bf x}).\\qquad\\qquad\\qquad\\qquad(4)', 'H({\\bf x})=\\sum_{i}H(x_{i})+H_{c}({\\bf x}).\\eqno\\qquad\\qquad\\qquad(5)']}\n</code></pre> <p>Where <code>dt_polys</code> represents the coordinates of the detected formula area, and <code>rec_formula</code> is the detected formula.</p> <p>The visualization result is as follows: </p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path. Formula recognition visualization requires a separate environment configuration. Please refer to 2.3 Formula Recognition Pipeline Visualization to install the LaTeX rendering engine.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<ul> <li>Quickly perform inference on the pipeline with just a few lines of code, taking the formula recognition pipeline as an example:</li> </ul> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"formula_recognition\")\n\noutput = pipeline.predict(\"general_formula_recognition.png\")\nfor res in output:\n    res.print()\n</code></pre> <p>\u2757 The results obtained from running the Python script are the same as those from the command line.</p> <p>The Python script above executes the following steps:</p> <p>\uff081\uff09Instantiate the formula recognition pipeline object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it is the name of the pipeline, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the pipeline supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the formula recognition pipeline object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the formula recognition pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/formula_recognition.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/formula_recognition.yaml\")\noutput = pipeline.predict(\"general_formula_recognition.png\")\nfor res in output:\n    res.print()\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#23-formula-recognition-pipeline-visualization","title":"2.3 Formula Recognition Pipeline Visualization","text":"<p>If you need to visualize the formula recognition pipeline, you need to run the following command to install the LaTeX rendering environment: <pre><code>apt-get install sudo\nsudo apt-get update\nsudo apt-get install texlive\nsudo apt-get install texlive-latex-base\nsudo apt-get install texlive-latex-extra\n</code></pre> After that, use the <code>save_to_img</code> method to save the visualization image. The specific command is as follows: <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"formula_recognition\")\n\noutput = pipeline.predict(\"general_formula_recognition.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\")\n</code></pre> Note: Since the formula recognition visualization process requires rendering each formula image, it may take a relatively long time. Please be patient.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the formula recognition pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <p>If you need to apply the formula recognition pipeline directly in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain formula recognition results from an image.</p> <p><code>POST /formula-recognition</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Properties of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the layout detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>formulas</code> <code>array</code> Positions and contents of formulas. <code>layoutImage</code> <code>string</code> Layout area detection result image. The image is in JPEG format and encoded using Base64. <code>ocrImage</code> <code>string</code> OCR result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>formulas</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>poly</code> <code>array</code> Formula position. Elements in the array are the vertex coordinates of the polygon enclosing the formula. <code>latex</code> <code>string</code> Formula content. <p>Example of <code>result</code>:</p> <pre><code>{\n\"formulas\": [\n{\n\"poly\": [\n[\n444.0,\n244.0\n],\n[\n705.4,\n244.5\n],\n[\n705.8,\n311.3\n],\n[\n444.1,\n311.0\n]\n],\n\"latex\": \"F({\\bf x})=C(F_{1}(x_{1}),\\cdot\\cdot\\cdot,F_{N}(x_{N})).\\qquad\\qquad\\qquad(1)\"\n}\n],\n\"layoutImage\": \"xxxxxx\",\n\"ocrImage\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/formula-recognition\"\nimage_path = \"./demo.jpg\"\nlayout_image_path = \"./layout.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(layout_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"layoutImage\"]))\nprint(f\"Output image saved at {layout_image_path}\")\nprint(\"\\nDetected formulas:\")\nprint(result[\"formulas\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string layoutImagePath = \"./layout.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/formula-recognition\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"layoutImage\"];\n        decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedLayoutImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputLayoutFile(layoutImagePath, std::ios::binary | std::ios::out);\n        if (outputLayoutFile.is_open()) {\n            outputLayoutFile.write(reinterpret_cast&lt;char*&gt;(decodedLayoutImage.data()), decodedLayoutImage.size());\n            outputLayoutFile.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        }\n\n        auto formulas = result[\"formulas\"];\n        std::cout &lt;&lt; \"\\nDetected formulas:\" &lt;&lt; std::endl;\n        for (const auto&amp; formula : formulas) {\n            std::cout &lt;&lt; formula &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/formula-recognition\";\n        String imagePath = \"./demo.jpg\";\n        String layoutImagePath = \"./layout.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String layoutBase64Image = result.get(\"layoutImage\").asText();\n                JsonNode formulas = result.get(\"formulas\");\n\n                imageBytes = Base64.getDecoder().decode(layoutBase64Image);\n                try (FileOutputStream fos = new FileOutputStream(layoutImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + layoutImagePath);\n\n                System.out.println(\"\\nDetected formulas: \" + formulas.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/formula-recognition\"\n    imagePath := \"./demo.jpg\"\n    layoutImagePath := \"./layout.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            LayoutImage      string   `json:\"layoutImage\"`\n            Formulas []map[string]interface{} `json:\"formulas\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    layoutImageData, err := base64.StdEncoding.DecodeString(respData.Result.LayoutImage)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(layoutImagePath, layoutImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", layoutImagePath)\n\n    fmt.Println(\"\\nDetected formulas:\")\n    for _, formula := range respData.Result.Formulas {\n        fmt.Println(formula)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/formula-recognition\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string layoutImagePath = \"./layout.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string layoutBase64Image = jsonResponse[\"result\"][\"layoutImage\"].ToString();\n        byte[] layoutImageBytes = Convert.FromBase64String(layoutBase64Image);\n        File.WriteAllBytes(layoutImagePath, layoutImageBytes);\n        Console.WriteLine($\"Output image saved at {layoutImagePath}\");\n\n        Console.WriteLine(\"\\nDetected formulas:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"formulas\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/formula-recognition'\nconst imagePath = './demo.jpg'\nconst layoutImagePath = \"./layout.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n\n    imageBuffer = Buffer.from(result[\"layoutImage\"], 'base64');\n    fs.writeFile(layoutImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${layoutImagePath}`);\n    });\n\n    console.log(\"\\nDetected formulas:\");\n    console.log(result[\"formulas\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/formula-recognition\"\n$image_path = \"./demo.jpg\";\n$layout_image_path = \"./layout.jpg\"\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\n\nfile_put_contents($layout_image_path, base64_decode($result[\"layoutImage\"]));\necho \"Output image saved at \" . $layout_image_path . \"\\n\";\n\necho \"\\nDetected formulas:\\n\";\nprint_r($result[\"formulas\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. You can choose the appropriate deployment method based on your needs to proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the formula recognition pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing models using your own domain-specific or application-specific data to improve the recognition performance of the formula recognition pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the formula recognition pipeline consists of two modules (layout detection and formula recognition), unsatisfactory performance may stem from either module.</p> <p>You can analyze images with poor recognition results. If you find that many formula are undetected (i.e., formula miss detection), it may indicate that the layout detection model needs improvement. You should refer to the Customization section in the Layout Detection Module Development Tutorial and use your private dataset to fine-tune the layout detection model. If many recognition errors occur in detected formula (i.e., the recognized formula content does not match the actual formula content), it suggests that the formula recognition model requires further refinement. You should refer to the Customization section in the Formula Recognition Module Development Tutorial and fine-tune the formula recognition model.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weights files.</p> <p>If you need to use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local paths of the fine-tuned model weights to the corresponding positions in the pipeline configuration file:</p> <pre><code>......\nPipeline:\n  layout_model: RT-DETR-H_layout_17cls # Can be replaced with the local path of the fine-tuned layout detection model\n  formula_rec_model: LaTeX_OCR_rec # Can be replaced with the local path of the fine-tuned formula recognition model\n  formula_rec_batch_size: 5\n  device: \"gpu:0\"\n......\n</code></pre> <p>Then, refer to the command line method or Python script method in 2. Quick Start to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPU, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modifying the <code>--device</code> parameter allows seamless switching between different hardware.</p> <p>For example, if you are using an NVIDIA GPU for formula pipeline inference, the Python command would be:</p> <p><pre><code>paddlex --pipeline formula_recognition --input general_formula_recognition.png --device gpu:0\n</code></pre> Now, if you want to switch the hardware to Ascend NPU, you only need to modify the <code>--device</code> in the Python command:</p> <pre><code>paddlex --pipeline formula_recognition --input general_formula_recognition.png --device npu:0\n</code></pre> <p>If you want to use the formula recognition pipeline on more types of hardware, please refer to the PaddleX Multi-Hardware Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html","title":"General Layout Parsing Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#1-introduction-to-the-general-layout-parsing-pipeline","title":"1. Introduction to the General Layout Parsing Pipeline","text":"<p>Layout parsing is a technology that extracts structured information from document images, primarily used to convert complex document layouts into machine-readable data formats. This technology has extensive applications in document management, information extraction, and data digitization. By combining Optical Character Recognition (OCR), image processing, and machine learning algorithms, layout parsing can identify and extract text blocks, titles, paragraphs, images, tables, and other layout elements from documents. The process typically involves three main steps: layout analysis, element analysis, and data formatting, ultimately generating structured document data to improve data processing efficiency and accuracy.</p> <p>The General Layout Parsing Pipeline includes modules for table structure recognition, layout region analysis, text detection, text recognition, formula recognition, seal text detection, text image rectification, and document image orientation classification.</p> <p>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, choose a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p>  \ud83d\udc49Model List Details <p>Table Structure Recognition Module Models:</p> ModelModel Download Link Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description SLANetInference Model/Trained Model 59.52 522.536 1845.37 6.9 M SLANet is a table structure recognition model developed by Baidu PaddleX Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information. SLANet_plusInference Model/Trained Model 63.69 522.536 1845.37 6.9 M SLANet_plus is an enhanced version of SLANet, the table structure recognition model developed by Baidu PaddleX Team. Compared to SLANet, SLANet_plus significantly improves the recognition ability for wireless and complex tables and reduces the model's sensitivity to the accuracy of table positioning, enabling more accurate recognition even with offset table positioning. <p>Note: The above accuracy metrics are measured on PaddleX's internally built English table recognition dataset. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Layout Detection Module Models:</p> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1xInference Model/Trained Model 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_tableInference Model/Trained Model 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3clsInference Model/Trained Model 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17clsInference Model/Trained Model 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3clsInference Model/Trained Model 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17clsInference Model/Trained Model 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3clsInference Model/Trained Model 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout region analysis dataset, containing 10,000 images of common document types, including English and Chinese papers, magazines, research reports, etc. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_detInference Model/Trained Model 82.69 83.3501 2434.01 109 PP-OCRv4's server-side text detection model, featuring higher accuracy, suitable for deployment on high-performance servers PP-OCRv4_mobile_detInference Model/Trained Model 77.79 10.6923 120.177 4.7 PP-OCRv4's mobile text detection model, optimized for efficiency, suitable for deployment on edge devices <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten texts, with 500 images for detection. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Recognition Module Models:</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4 is the next version of Baidu PaddlePaddle's self-developed text recognition model PP-OCRv3. By introducing data augmentation schemes and GTC-NRTR guidance branches, it further improves text recognition accuracy without compromising inference speed. The model offers both server (server) and mobile (mobile) versions to meet industrial needs in different scenarios. PP-OCRv4_server_recInference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten texts, with 11,000 images for text recognition. All GPU inference times are based on NVIDIA Tesla T4 machines with FP32 precision. CPU inference speeds are based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description ch_SVTRv2_recInference Model/Trained Model 68.81 8.36801 165.706 73.9 M  SVTRv2 is a server-side text recognition model developed by the OpenOCR team at the Vision and Learning Lab (FVL) of Fudan University. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 6% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the A-list.  <p>Note: The evaluation set for the above accuracy metrics is the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task A-list. GPU inference time is based on NVIDIA Tesla T4 with FP32 precision. CPU inference speed is based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> ModelModel Download Link Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description ch_RepSVTR_recInference Model/Trained Model 65.07 10.5047 51.5647 22.1 M  The RepSVTR text recognition model is a mobile-oriented text recognition model based on SVTRv2. It won the first prize in the OCR End-to-End Recognition Task of the PaddleOCR Algorithm Model Challenge, with a 2.5% improvement in end-to-end recognition accuracy compared to PP-OCRv4 on the B-list, while maintaining similar inference speed.  <p>Note: The evaluation set for the above accuracy metrics is the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition Task B-list. GPU inference time is based on NVIDIA Tesla T4 with FP32 precision. CPU inference speed is based on Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Formula Recognition Module Models:</p> Model NameModel Download Link BLEU Score Normed Edit Distance ExpRate (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size LaTeX_OCR_recInference Model/Trained Model 0.8821 0.0823 40.01 - - 89.7 M <p>Note: The above accuracy metrics are measured on the LaTeX-OCR Formula Recognition Test Set. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Seal Text Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Trained Model 98.21 84.341 2425.06 109 PP-OCRv4's server-side seal text detection model, featuring higher accuracy, suitable for deployment on better-equipped servers PP-OCRv4_mobile_seal_detInference Model/Trained Model 96.47 10.5878 131.813 4.6 PP-OCRv4's mobile seal text detection model, offering higher efficiency, suitable for deployment on edge devices <p>Note: The above accuracy metrics are evaluated on a self-built dataset containing 500 circular seal images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Image Rectification Module Models:</p> ModelModel Download Link MS-SSIM (%) Model Size (M) Description UVDocInference Model/Trained Model 54.40 30.3 M High-precision text image rectification model <p>The accuracy metrics of the models are measured from the DocUNet benchmark.</p> <p>Document Image Orientation Classification Module Models:</p> ModelModel Download Link Top-1 Acc (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-LCNet_x1_0_doc_oriInference Model/Trained Model 99.06 3.84845 9.23735 7 A document image classification model based on PP-LCNet_x1_0, with four categories: 0\u00b0, 90\u00b0, 180\u00b0, 270\u00b0 <p>Note: The above accuracy metrics are evaluated on a self-built dataset covering various scenarios such as certificates and documents, containing 1000 images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX provides pre-trained model pipelines that can be quickly experienced. You can experience the effect of the General Image Classification pipeline online, or locally using command line or Python.</p> <p>Before using the General Layout Parsing pipeline locally, please ensure you have completed the installation of the PaddleX wheel package according to the PaddleX Local Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#21-experience-via-command-line","title":"2.1 Experience via Command Line","text":"<p>One command is all you need to quickly experience the effect of the Layout Parsing pipeline. Use the test file and replace <code>--input</code> with your local path to make predictions.</p> <p><pre><code>paddlex --pipeline layout_parsing --input demo_paper.png --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the Layout Parsing pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 indicates using the first GPU, gpu:1,2 indicates using the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default Layout Parsing pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config layout_parsing\n</code></pre> <p>After execution, the layout parsing pipeline configuration file will be saved in the current directory. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config layout_parsing --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the saved path of the configuration file to make it take effect. For example, if the configuration file is saved as <code>./layout_parsing.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./layout_parsing.yaml --input layout_parsing.jpg\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If these parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result will be:</p>  \ud83d\udc49Click to expand <pre><code>{'input_path': PosixPath('/root/.paddlex/temp/tmp5jmloefs.png'), 'parsing_result': [{'input_path': PosixPath('/root/.paddlex/temp/tmpshsq8_w0.png'), 'layout_bbox': [51.46833, 74.22329, 542.4082, 232.77504], 'image': {'img': array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [213, 221, 238],\n        [217, 223, 240],\n        [233, 234, 241]],\n\n       [[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8), 'image_text': ''}, 'layout': 'single'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpcd2q9uyu.png'), 'layout_bbox': [47.68295, 243.08054, 546.28253, 295.71045], 'figure_title': 'Overview of RT-DETR, We feed th', 'layout': 'single'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpr_iqa8b3.png'), 'layout_bbox': [58.416977, 304.1531, 275.9134, 400.07513], 'image': {'img': array([[[255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255],\n        ...,\n        [255, 255, 255],\n        [255, 255, 255],\n        [255, 255, 255]]], dtype=uint8), 'image_text': ''}, 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmphpblxl3p.png'), 'layout_bbox': [100.62961, 405.97458, 234.79774, 414.77414], 'figure_title': 'Figure 5. The fusion block in CCFF.', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmplgnczrsf.png'), 'layout_bbox': [47.81724, 421.9041, 288.01566, 550.538], 'text': 'D, Ds, not only significantly reduces latency (35% faster),\\nRut\\nnproves accuracy (0.4% AP higher), CCFF is opti\\nased on the cross-scale fusion module, which\\nnsisting of convolutional lavers intc\\npath.\\nThe role of the fusion block is t\\n into a new feature, and its\\nFigure 5. The f\\nblock contains tw\\n1 x1\\nchannels, /V RepBlock\\n. anc\\n: two-path outputs are fused by element-wise add. We\\ntormulate the calculation ot the hvbrid encoder as:', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpsq0ey9md.png'), 'layout_bbox': [94.60716, 558.703, 288.04193, 600.19434], 'formula': '\\\\begin{array}{l}{{\\\\Theta=K=\\\\mathrm{p.s.sp{\\\\pm}}\\\\mathrm{i.s.s.}(\\\\mathrm{l.s.}(\\\\mathrm{l.s.}(\\\\mathrm{l.s.}}),{\\\\qquad\\\\mathrm{{a.s.}}\\\\mathrm{s.}}}\\\\\\\\ {{\\\\tau_{\\\\mathrm{{s.s.s.s.s.}}(\\\\mathrm{l.s.},\\\\mathrm{l.s.},\\\\mathrm{s.s.}}\\\\mathrm{s.}\\\\mathrm{s.}}\\\\end{array}),}}\\\\\\\\ {{\\\\bar{\\\\mathrm{e-c.c.s.s.}(\\\\mathrm{s.},\\\\mathrm{s.s.},\\\\ s_{s}}\\\\mathrm{s.s.},\\\\tau),}}\\\\end{array}', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpv30qy0v4.png'), 'layout_bbox': [47.975555, 607.12024, 288.5776, 629.1252], 'text': 'tened feature to the same shape as Ss.\\nwhere Re shape represents restoring the shape of the flat-', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp0jejzwwv.png'), 'layout_bbox': [48.383354, 637.581, 245.96404, 648.20496], 'paragraph_title': '4.3. Uncertainty-minimal Query Selection', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpushex416.png'), 'layout_bbox': [47.80134, 656.002, 288.50192, 713.24994], 'text': 'To reduce the difficulty of optimizing object queries in\\nDETR, several subsequent works [42, 44, 45] propose query\\nselection schemes, which have in common that they use the\\nconfidence score to select the top K\u2019 features from the en-\\ncoder to initialize object queries (or just position queries).', 'layout': 'left'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpki7e_6wc.png'), 'layout_bbox': [306.6371, 302.1026, 546.3772, 419.76724], 'text': 'The confidence score represents the likelihood that the fea\\nture includes foreground objects. Nevertheless, the \\nare required to simultaneously model the category\\nojects, both of which determine the quality of the\\npertor\\ncore of the fes\\nBased on the analysis, the current query\\n considerable level of uncertainty in the\\nresulting in sub-optimal initialization for\\nand hindering the performance of the detector.', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmppbxrfehp.png'), 'layout_bbox': [306.0642, 422.7347, 546.9216, 539.45734], 'text': 'To address this problem, we propose the uncertainty mini\\nmal query selection scheme, which explicitly const\\noptim\\n the epistemic uncertainty to model the\\nfeatures, thereby providing \\nhigh-quality\\nr the decoder. Specifically,\\n the discrepancy between i\\nalization P\\nand classificat\\n.(2\\ntunction for the gradie', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp1mgiyd21.png'), 'layout_bbox': [331.52808, 549.32635, 546.5229, 586.15546], 'formula': '\\\\begin{array}{c c c}{{}}&amp;{{}}&amp;{{\\\\begin{array}{c}{{i\\\\langle X\\\\rangle=({\\\\bar{Y}}({\\\\bar{X}})+{\\\\bar{Z}}({\\\\bar{X}})\\\\mid X\\\\in{\\\\bar{\\\\pi}}^{\\\\prime}}}&amp;{{}}\\\\\\\\ {{}}&amp;{{}}&amp;{{}}\\\\end{array}}}&amp;{{\\\\emptyset}}\\\\\\\\ {{}}&amp;{{}}&amp;{{C(\\\\bar{X},{\\\\bar{X}})=C..\\\\scriptstyle(\\\\bar{0},{\\\\bar{Y}})+{\\\\mathcal{L}}_{{\\\\mathrm{s}}}({\\\\bar{X}}),\\\\ 6)}}&amp;{{}}\\\\end{array}', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmp8t73dpym.png'), 'layout_bbox': [306.44016, 592.8762, 546.84314, 630.60126], 'text': 'where  and y denote the prediction and ground truth,\\n= (c, b), c and b represent the category and bounding\\nbox respectively, X represent the encoder feature.', 'layout': 'right'}, {'input_path': PosixPath('/root/.paddlex/temp/tmpftnxeyjm.png'), 'layout_bbox': [306.15652, 632.3142, 546.2463, 713.19073], 'text': 'Effectiveness analysis. To analyze the effectiveness of the\\nuncertainty-minimal query selection, we visualize the clas-\\nsificatior\\nscores and IoU scores of the selected fe\\nCOCO\\na 12017, Figure 6. We draw the scatterplo\\nt with\\ndots\\nrepresent the selected features from the model trained\\nwith uncertainty-minimal query selection and vanilla query', 'layout': 'right'}]}\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>A few lines of code are all you need to quickly perform inference on your production line. Taking the general layout parsing pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"layout_parsing\")\n\noutput = pipeline.predict(\"demo_paper.png\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the result as an image file\n    res.save_to_xlsx(\"./output/\")  # Save the result as an Excel file\n    res.save_to_html(\"./output/\")  # Save the result as an HTML file\n</code></pre> The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it's a pipeline name, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available if the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pipeline object to perform inference: The <code>predict</code> method takes <code>x</code> as a parameter, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to the specific task, e.g., \"img\" for image classification tasks, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be of the above data types, e.g., <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>, <code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>(4) Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving as files, with the supported file types depending on the specific pipeline, such as:</p> Method Description Method Parameters <code>save_to_img</code> Saves the result as an image file. <code>- save_path</code>: <code>str</code> type, the path to save the file. When it's a directory, the saved file name is consistent with the input file name. <code>save_to_html</code> Saves the result as an HTML file. <code>- save_path</code>: <code>str</code> type, the path to save the file. When it's a directory, the saved file name is consistent with the input file name. <p>| <code>save_to_xlsx</code> | Saves the result as an Excel file. | <code>- save_path</code>: <code>str</code> type, the path to save the file. When it's a directory, the saved file name is consistent with the input file name.</p> <p>Within this tutorial on Artificial Intelligence and Computer Vision, we will explore the capabilities of saving and exporting results from various processes, including OCR (Optical Character Recognition), layout analysis, and table structure recognition. Specifically, the <code>save_to_img</code> function enables saving visualization results, <code>save_to_html</code> converts tables directly into HTML files, and <code>save_to_xlsx</code> exports tables as Excel files.</p> <p>Upon obtaining the configuration file, you can customize various settings for the layout parsing pipeline by simply modifying the <code>pipeline</code> parameter within the <code>create_pipeline</code> method to point to your configuration file path.</p> <p>For instance, if your configuration file is saved at <code>./my_path/layout_parsing.yaml</code>, you can execute the following code:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/layout_parsing.yaml\")\noutput = pipeline.predict(\"layout_parsing.jpg\")\nfor res in output:\n    res.print()  # Prints the structured output of the layout parsing prediction\n    res.save_to_img(\"./output/\")  # Saves the img format results from each submodule of the pipeline\n    res.save_to_xlsx(\"./output/\")  # Saves the xlsx format results from the table recognition module\n    res.save_to_html(\"./output/\")  # Saves the html results from the table recognition module\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements in terms of inference speed and accuracy, you can proceed with development integration or deployment.</p> <p>To directly apply the pipeline in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In production environments, many applications require stringent performance metrics, especially response speed, to ensure efficient operation and smooth user experience. PaddleX provides a high-performance inference plugin that deeply optimizes model inference and pre/post-processing for significant end-to-end speedups. For detailed instructions on high-performance inference, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service Deployment: Service deployment is a common form in production environments, where reasoning functions are encapsulated as services accessible via network requests. PaddleX enables cost-effective service deployment of pipelines. For detailed instructions on service deployment, refer to the PaddleX Service Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body attributes are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> attribute, of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body attributes are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs layout parsing.</p> <p><code>POST /layout-parsing</code></p> <ul> <li>Request body attributes:</li> </ul> Name Type Description Required <code>file</code> <code>string</code> The URL of an image file or PDF file accessible by the service, or the Base64 encoded result of the content of the above-mentioned file types. For PDF files with more than 10 pages, only the content of the first 10 pages will be used. Yes <code>fileType</code> <code>integer</code> File type. <code>0</code> indicates a PDF file, <code>1</code> indicates an image file. If this attribute is not present in the request body, the service will attempt to infer the file type automatically based on the URL. No <code>useImgOrientationCls</code> <code>boolean</code> Whether to enable document image orientation classification. This function is enabled by default. No <code>useImgUnwrapping</code> <code>boolean</code> Whether to enable text image rectification. This function is enabled by default. No <code>useSealTextDet</code> <code>boolean</code> Whether to enable seal text detection. This function is enabled by default. No <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Attributes of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the text detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following attributes:</li> </ul> Name Type Description <code>layoutParsingResults</code> <code>array</code> Layout parsing results. The array length is 1 (for image input) or the smaller of the number of document pages and 10 (for PDF input). For PDF input, each element in the array represents the processing result of each page in the PDF file. <p>Each element in <code>layoutParsingResults</code> is an <code>object</code> with the following attributes:</p> Name Type Description <code>layoutElements</code> <code>array</code> Layout element information. <p>Each element in <code>layoutElements</code> is an <code>object</code> with the following attributes:</p> Name Type Description <code>bbox</code> <code>array</code> Position of the layout element. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>label</code> <code>string</code> Label of the layout element. <code>text</code> <code>string</code> Text contained in the layout element. <code>layoutType</code> <code>string</code> Arrangement of the layout element. <code>image</code> <code>string</code> Image of the layout element, in JPEG format, encoded using Base64. Multi-language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/layout-parsing\" # \u200b\u670d\u52a1\u200bURL\n\n# \u200b\u5bf9\u200b\u672c\u5730\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200bBase64\u200b\u7f16\u7801\u200b\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\n    \"file\": image_data, # Base64\u200b\u7f16\u7801\u200b\u7684\u200b\u6587\u4ef6\u200b\u5185\u5bb9\u200b\u6216\u8005\u200b\u6587\u4ef6\u200bURL\n    \"fileType\": 1,\n    \"useImgOrientationCls\": True,\n    \"useImgUnwrapping\": True,\n    \"useSealTextDet\": True,\n}\n\n# \u200b\u8c03\u7528\u200bAPI\nresponse = requests.post(API_URL, json=payload)\n\n# \u200b\u5904\u7406\u200b\u63a5\u53e3\u200b\u8fd4\u56de\u200b\u6570\u636e\u200b\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nprint(\"\\nDetected layout elements:\")\nfor res in result[\"layoutParsingResults\"]:\n    for ele in res[\"layoutElements\"]:\n        print(\"===============================\")\n        print(\"bbox:\", ele[\"bbox\"])\n        print(\"label:\", ele[\"label\"])\n        print(\"text:\", repr(ele[\"text\"]))\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment refers to placing computational and data processing capabilities directly on user devices, enabling them to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</p> <p>You can choose an appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general layout parsing pipeline do not meet your requirements in terms of accuracy or speed for your specific scenario, you can try to further fine-tune the existing models using your own domain-specific or application-specific data to improve the recognition performance of the general layout parsing pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general layout parsing pipeline consists of 7 modules, unsatisfactory performance may stem from any of these modules.</p> <p>You can analyze images with poor recognition results and follow the guidelines below for analysis and model fine-tuning:</p> <ul> <li>Incorrect table structure detection (e.g., wrong row/column recognition, incorrect cell positions) may indicate deficiencies in the table structure recognition module. You need to refer to the Customization section in the Table Structure Recognition Module Development Tutorial and fine-tune the table structure recognition model using your private dataset.</li> <li>Misplaced layout elements (e.g., incorrect positioning of tables, seals) may suggest issues with the layout detection module. You should consult the Customization section in the Layout Detection Module Development Tutorial and fine-tune the layout detection model with your private dataset.</li> <li>Frequent undetected texts (i.e., text missing detection) indicate potential weaknesses in the text detection model. Follow the Customization section in the Text Detection Module Development Tutorial to fine-tune the text detection model using your private dataset.</li> <li>High text recognition errors (i.e., recognized text content does not match the actual text) suggest further improvements to the text recognition model. Refer to the Customization section in the Text Recognition Module Development Tutorial to fine-tune the text recognition model.</li> <li>Frequent recognition errors in detected seal texts indicate the need for improvements to the seal text detection model. Consult the Customization section in the Seal Text Detection Module Development Tutorials to fine-tune the seal text detection model.</li> <li>High recognition errors in detected formulas (i.e., recognized formula content does not match the actual formula) suggest further enhancements to the formula recognition model. Follow the Customization section in the Formula Recognition Module Development Tutorial to fine-tune the formula recognition model.</li> <li>Frequent misclassifications of document or certificate orientations with text areas indicate the need for improvements to the document image orientation classification model. Refer to the Customization section in the Document Image Orientation Classification Module Development Tutorial to fine-tune the model.</li> </ul>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning your model with a private dataset, you will obtain local model weights files.</p> <p>To use the fine-tuned model weights, simply modify the production line configuration file by replacing the local paths of the fine-tuned model weights to the corresponding positions in the configuration file:</p> <p><pre><code>......\n Pipeline:\n  layout_model: PicoDet_layout_1x  # Can be modified to the local path of the fine-tuned model\n  table_model: SLANet_plus  # Can be modified to the local path of the fine-tuned model\n  text_det_model: PP-OCRv4_server_det  # Can be modified to the local path of the fine-tuned model\n  text_rec_model: PP-OCRv4_server_rec  # Can be modified to the local path of the fine-tuned model\n  formula_rec_model: LaTeX_OCR_rec  # Can be modified to the local path of the fine-tuned model\n  seal_text_det_model: PP-OCRv4_server_seal_det   # Can be modified to the local path of the fine-tuned model\n  doc_image_unwarp_model: UVDoc  # Can be modified to the local path of the fine-tuned model\n  doc_image_ori_cls_model: PP-LCNet_x1_0_doc_ori  # Can be modified to the local path of the fine-tuned model\n  layout_batch_size: 1\n  text_rec_batch_size: 1\n  table_batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Subsequently, refer to the command line or Python script methods in the local experience to load the modified production line configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference in the layout parsing pipeline, the Python command is:</p> <p><pre><code>paddlex --pipeline layout_parsing --input layout_parsing.jpg --device gpu:0\n</code></pre> At this point, if you want to switch the hardware to Ascend NPU, simply modify <code>--device</code> to npu in the Python command:</p> <p><pre><code>paddlex --pipeline layout_parsing --input layout_parsing.jpg --device npu:0\n</code></pre> If you want to use the general layout parsing pipeline on more types of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html","title":"Seal Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#1-introduction-to-the-seal-recognition-pipeline","title":"1. Introduction to the Seal Recognition Pipeline","text":"<p>Seal recognition is a technology that automatically extracts and recognizes seal content from documents or images. The recognition of seal is part of document processing and has various applications in many scenarios, such as contract comparison, inventory access approval, and invoice reimbursement approval.</p> <p></p> <p>The Seal Recognition pipeline includes a layout area analysis module, a seal detection module, and a text recognition module.</p> <p>If you prioritize model accuracy, please choose a model with higher accuracy. If you prioritize inference speed, please choose a model with faster inference. If you prioritize model storage size, please choose a model with a smaller storage footprint.</p> <p>Layout Analysis Module Models:</p> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1xInference Model/Trained Model 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_tableInference Model/Trained Model 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3clsInference Model/Trained Model 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17clsInference Model/Trained Model 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3clsInference Model/Trained Model 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17clsInference Model/Trained Model 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3clsInference Model/Trained Model 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout region analysis dataset, containing 10,000 images of common document types, including English and Chinese papers, magazines, research reports, etc. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Seal Detection Module Models:</p> ModelModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PP-OCRv4_server_seal_detInference Model/Trained Model 98.21 84.341 2425.06 109 PP-OCRv4's server-side seal detection model, featuring higher accuracy, suitable for deployment on better-equipped servers PP-OCRv4_mobile_seal_detInference Model/Trained Model 96.47 10.5878 131.813 4.6 PP-OCRv4's mobile seal detection model, offering higher efficiency, suitable for deployment on edge devices <p>Note: The above accuracy metrics are evaluated on a self-built dataset containing 500 circular seal images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Recognition Module Models:</p> Model NameModel Download Link Average Recognition Accuracy (%) GPU Inference Time (ms) CPU Inference Time Model Size (M) PP-OCRv4_mobile_recInference Model/Trained Model 78.20 7.95018 46.7868 10.6 M PP-OCRv4_server_recInference Model/Trained Model 79.20 7.19439 140.179 71.2 M <p>Note: The evaluation set for the above accuracy indicators is a self-built Chinese dataset from PaddleOCR, covering various scenarios such as street scenes, web images, documents, and handwriting. The text recognition subset includes 11,000 images. The GPU inference time for all models above is based on an NVIDIA Tesla T4 machine with a precision type of FP32. The CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads, and the precision type is also FP32.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#2-quick-start","title":"2.  Quick Start","text":"<p>The pre trained model production line provided by PaddleX can quickly experience the effect. You can experience the effect of the seal recognition production line online, or use the command line or Python locally to experience the effect of the seal recognition production line.</p> <p>Before using the seal recognition production line locally, please ensure that you have completed the wheel package installation of PaddleX according to the  PaddleX Local Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#21-command-line-experience","title":"2.1 Command line experience","text":"<p>One command can quickly experience the effect of seal recognition production line, use test file, and replace <code>--input</code> with the local path for prediction</p> <pre><code>paddlex --pipeline seal_recognition --input seal_text_det.png --device gpu:0 --save_path output\n</code></pre> <p>Parameter description:</p> <pre><code>--Pipeline: Production line name, here is the seal recognition production line\n--Input: The local path or URL of the input image to be processed\n--The GPU serial number used by the device (e.g. GPU: 0 indicates the use of the 0th GPU, GPU: 1,2 indicates the use of the 1st and 2nd GPUs), or the CPU (-- device CPU) can be selected for use\n</code></pre> <p>When executing the above Python script, the default seal recognition production line configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>   \ud83d\udc49 Click to expand <pre><code>paddlex --get_pipeline_config seal_recognition\n</code></pre> <p>After execution, the seal recognition production line configuration file will be saved in the current path. If you want to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config seal_recognition --save_path ./my_path --save_path output\n</code></pre> <p>After obtaining the production line configuration file, you can replace '-- pipeline' with the configuration file save path to make the configuration file effective. For example, if the configuration file save path is <code>/ seal_recognition.yaml</code>\uff0c Just need to execute:</p> <pre><code>paddlex --pipeline ./seal_recognition.yaml --input seal_text_det.png --save_path output\n</code></pre> <p>Among them, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified and will use the parameters in the configuration file. If the parameters are still specified, the specified parameters will prevail.</p> <p>After running, the result obtained is:</p>   \ud83d\udc49 Click to expand <pre><code>\n{'input_path': PosixPath('/root/.paddlex/temp/tmpa8eqnpus.png'), 'layout_result': {'input_path': PosixPath('/root/.paddlex/temp/tmpa8eqnpus.png'), 'boxes': [{'cls_id': 2, 'label': 'seal', 'score': 0.9813321828842163, 'coordinate': [0, 5.1820183, 639.59314, 637.7533]}]}, 'ocr_result': {'dt_polys': [array([[166, 468],\n                        [206, 503],\n                    [249, 523],\n                    [312, 535],\n                    [364, 529],\n                    [390, 521],\n                    [428, 505],\n                    [465, 476],\n                    [468, 474],\n                    [473, 474],\n                    [476, 475],\n                    [478, 477],\n                    [508, 507],\n                    [510, 510],\n                    [511, 514],\n                    [509, 518],\n                    [507, 521],\n                    [458, 559],\n                    [455, 560],\n                    [399, 584],\n                    [399, 584],\n                    [369, 591],\n                    [367, 592],\n                    [308, 597],\n                    [305, 596],\n                    [240, 584],\n                    [239, 584],\n                    [220, 577],\n                    [169, 552],\n                    [166, 551],\n                    [120, 510],\n                    [117, 507],\n                    [116, 503],\n                    [117, 499],\n                    [121, 495],\n                    [153, 468],\n                    [156, 467],\n                    [161, 467]]), array([[439, 444],\n                    [443, 444],\n                    [446, 446],\n                    [448, 448],\n                    [450, 451],\n                    [450, 454],\n                    [448, 498],\n                    [448, 502],\n                    [445, 505],\n                    [442, 507],\n                    [439, 507],\n                    [399, 505],\n                    [196, 506],\n                    [192, 505],\n                    [189, 503],\n                    [187, 500],\n                    [187, 497],\n                    [186, 458],\n                    [186, 456],\n                    [187, 451],\n                    [188, 448],\n                    [192, 444],\n                    [194, 444],\n                    [198, 443]]), array([[463, 347],\n                    [468, 347],\n                    [472, 350],\n                    [474, 353],\n                    [476, 360],\n                    [477, 425],\n                    [476, 429],\n                    [474, 433],\n                    [470, 436],\n                    [466, 438],\n                    [463, 438],\n                    [175, 439],\n                    [170, 438],\n                    [166, 435],\n                    [163, 432],\n                    [161, 426],\n                    [161, 361],\n                    [161, 356],\n                    [163, 352],\n                    [167, 349],\n                    [172, 347],\n                    [184, 346],\n                    [186, 346]]), array([[325,  38],\n                    [485,  91],\n                    [489,  94],\n                    [493,  96],\n                    [587, 225],\n                    [588, 230],\n                    [589, 234],\n                    [592, 384],\n                    [591, 389],\n                    [588, 393],\n                    [585, 397],\n                    [581, 399],\n                    [576, 399],\n                    [572, 398],\n                    [508, 380],\n                    [503, 379],\n                    [499, 375],\n                    [498, 370],\n                    [497, 367],\n                    [493, 258],\n                    [428, 171],\n                    [421, 165],\n                    [323, 136],\n                    [225, 165],\n                    [207, 175],\n                    [144, 260],\n                    [141, 365],\n                    [141, 370],\n                    [138, 374],\n                    [134, 378],\n                    [131, 379],\n                    [ 66, 398],\n                    [ 61, 398],\n                    [ 56, 398],\n                    [ 52, 395],\n                    [ 48, 391],\n                    [ 47, 386],\n                    [ 47, 384],\n                    [ 47, 235],\n                    [ 48, 230],\n                    [ 50, 226],\n                    [146,  96],\n                    [151,  92],\n                    [154,  91],\n                    [315,  38],\n                    [320,  37]])], 'dt_scores': [0.99375725701319, 0.9871711582010613, 0.9937523531067023, 0.9911629231838204], 'rec_text': ['5263647368706', '\u200b\u5417\u200b\u7e41\u7269\u200b', '\u200b\u53d1\u7968\u200b\u4e13\u200b\u5929\u6d25\u200b\u541b\u548c\u7f18\u200b\u5546\u8d38\u200b\u6709\u9650\u516c\u53f8\u200b'], 'rec_score': [0.9933745265007019, 0.998288631439209, 0.9999362230300903, 0.9923253655433655], 'input_path': PosixPath('/Users/chenghong0temp/tmpa8eqnpus.png')}, 'src_file_name': 'https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/seal_text_det.png', 'page_id': 0}  \n</code></pre> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>A few lines of code can complete the fast inference of the production line. Taking the seal recognition production line as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"seal_recognition\")\n\noutput = pipeline.predict(\"seal_text_det.png\")\nfor res in output:\n    res.print()\n    res.save_to_img(\"./output/\") # Save the results in img\n</code></pre> <p>The result obtained is the same as the command line method.</p> <p>In the above Python script, the following steps were executed:</p> <p>\uff081\uff09Instantiate the  production line object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the production line or the path to the production line configuration file. If it is the name of the production line, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for production line model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the production line supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the  production line object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters save_to_img Save the results as an img format file <code>- save_path</code>: str, the path to save the file. When it's a directory, the saved file name will be consistent with the input file type; <p>Where <code>save_to_img</code> can save visualization results (including OCR result images, layout analysis result images).</p> <p>If you have a configuration file, you can customize the configurations of the seal recognition  pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved in <code>/ my_path/seal_recognition.yaml</code> \uff0c Then only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/seal_recognition.yaml\")\noutput = pipeline.predict(\"seal_text_det.png\")\nfor res in output:\n    res.print() ## \u200b\u6253\u5370\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u8f93\u51fa\u200b\n    res.save_to_img(\"./output/\") ## \u200b\u4fdd\u5b58\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#3-development-integrationdeployment","title":"3. Development integration/deployment","text":"<p>If the production line can meet your requirements for inference speed and accuracy, you can directly develop integration/deployment.</p> <p>If you need to directly apply the production line to your Python project, you can refer to the example code in [2.2.2 Python scripting] (# 222 python scripting integration).</p> <p>In addition, PaddleX also offers three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 ** High performance deployment: In actual production environments, many applications have strict standards for the performance indicators of deployment strategies, especially response speed, to ensure efficient system operation and smooth user experience. To this end, PaddleX provides a high-performance inference plugin aimed at deep performance optimization of model inference and pre-processing, achieving significant acceleration of end-to-end processes. For a detailed high-performance deployment process, please refer to the [PaddleX High Performance Deployment Guide] (../../../pipelin_deploy/high_performance_deploy. md).</p> <p>\u2601\ufe0f ** Service deployment * *: Service deployment is a common form of deployment in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users to achieve service-oriented deployment of production lines at low cost. For detailed service-oriented deployment processes, please refer to the PaddleX Service Deployment Guide (../../../ipeline_deploy/service_deploy. md).</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Obtain seal recognition results from an image.</p> <p><code>POST /seal-recognition</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Properties of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the text detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>texts</code> <code>array</code> Positions, contents, and scores of texts. <code>layoutImage</code> <code>string</code> Layout area detection result image. The image is in JPEG format and encoded using Base64. <code>ocrImage</code> <code>string</code> OCR result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>texts</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>poly</code> <code>array</code> Text position. Elements in the array are the vertex coordinates of the polygon enclosing the text. <code>text</code> <code>string</code> Text content. <code>score</code> <code>number</code> Text recognition score. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/seal-recognition\"\nimage_path = \"./demo.jpg\"\nocr_image_path = \"./ocr.jpg\"\nlayout_image_path = \"./layout.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(ocr_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"ocrImage\"]))\nprint(f\"Output image saved at {ocr_image_path}\")\nwith open(layout_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"layoutImage\"]))\nprint(f\"Output image saved at {layout_image_path}\")\nprint(\"\\nDetected texts:\")\nprint(result[\"texts\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string ocrImagePath = \"./ocr.jpg\";\n    const std::string layoutImagePath = \"./layout.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/seal-recognition\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"ocrImage\"];\n        std::string decoded_string = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedOcrImage(decoded_string.begin(), decoded_string.end());\n        std::ofstream outputOcrFile(ocrImagePath, std::ios::binary | std::ios::out);\n        if (outputOcrFile.is_open()) {\n            outputOcrFile.write(reinterpret_cast&lt;char*&gt;(decodedOcrImage.data()), decodedOcrImage.size());\n            outputOcrFile.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; ocrImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; ocrImagePath &lt;&lt; std::endl;\n        }\n\n        encodedImage = result[\"layoutImage\"];\n        decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedLayoutImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputLayoutFile(layoutImagePath, std::ios::binary | std::ios::out);\n        if (outputLayoutFile.is_open()) {\n            outputLayoutFile.write(reinterpret_cast&lt;char*&gt;(decodedLayoutImage.data()), decodedLayoutImage.size());\n            outputLayoutFile.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        }\n\n        auto texts = result[\"texts\"];\n        std::cout &lt;&lt; \"\\nDetected texts:\" &lt;&lt; std::endl;\n        for (const auto&amp; text : texts) {\n            std::cout &lt;&lt; text &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/seal-recognition\";\n        String imagePath = \"./demo.jpg\";\n        String ocrImagePath = \"./ocr.jpg\";\n        String layoutImagePath = \"./layout.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String ocrBase64Image = result.get(\"ocrImage\").asText();\n                String layoutBase64Image = result.get(\"layoutImage\").asText();\n                JsonNode texts = result.get(\"texts\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(ocrBase64Image);\n                try (FileOutputStream fos = new FileOutputStream(ocrImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + ocrBase64Image);\n\n                imageBytes = Base64.getDecoder().decode(layoutBase64Image);\n                try (FileOutputStream fos = new FileOutputStream(layoutImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + layoutImagePath);\n\n                System.out.println(\"\\nDetected texts: \" + texts.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/seal-recognition\"\n    imagePath := \"./demo.jpg\"\n    ocrImagePath := \"./ocr.jpg\"\n    layoutImagePath := \"./layout.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            OcrImage      string   `json:\"ocrImage\"`\n            LayoutImage      string   `json:\"layoutImage\"`\n            Texts []map[string]interface{} `json:\"texts\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    ocrImageData, err := base64.StdEncoding.DecodeString(respData.Result.OcrImage)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(ocrImagePath, ocrImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", ocrImagePath)\n\n    layoutImageData, err := base64.StdEncoding.DecodeString(respData.Result.LayoutImage)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(layoutImagePath, layoutImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", layoutImagePath)\n\n    fmt.Println(\"\\nDetected texts:\")\n    for _, text := range respData.Result.Texts {\n        fmt.Println(text)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/seal-recognition\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string ocrImagePath = \"./ocr.jpg\";\n    static readonly string layoutImagePath = \"./layout.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string ocrBase64Image = jsonResponse[\"result\"][\"ocrImage\"].ToString();\n        byte[] ocrImageBytes = Convert.FromBase64String(ocrBase64Image);\n        File.WriteAllBytes(ocrImagePath, ocrImageBytes);\n        Console.WriteLine($\"Output image saved at {ocrImagePath}\");\n\n        string layoutBase64Image = jsonResponse[\"result\"][\"layoutImage\"].ToString();\n        byte[] layoutImageBytes = Convert.FromBase64String(layoutBase64Image);\n        File.WriteAllBytes(layoutImagePath, layoutImageBytes);\n        Console.WriteLine($\"Output image saved at {layoutImagePath}\");\n\n        Console.WriteLine(\"\\nDetected texts:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"texts\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/seal-recognition'\nconst imagePath = './demo.jpg'\nconst ocrImagePath = \"./ocr.jpg\";\nconst layoutImagePath = \"./layout.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n\n    const imageBuffer = Buffer.from(result[\"ocrImage\"], 'base64');\n    fs.writeFile(ocrImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${ocrImagePath}`);\n    });\n\n    imageBuffer = Buffer.from(result[\"layoutImage\"], 'base64');\n    fs.writeFile(layoutImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${layoutImagePath}`);\n    });\n\n    console.log(\"\\nDetected texts:\");\n    console.log(result[\"texts\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/seal-recognition\";\n$image_path = \"./demo.jpg\";\n$ocr_image_path = \"./ocr.jpg\";\n$layout_image_path = \"./layout.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($ocr_image_path, base64_decode($result[\"ocrImage\"]));\necho \"Output image saved at \" . $ocr_image_path . \"\\n\";\n\nfile_put_contents($layout_image_path, base64_decode($result[\"layoutImage\"]));\necho \"Output image saved at \" . $layout_image_path . \"\\n\";\n\necho \"\\nDetected texts:\\n\";\nprint_r($result[\"texts\"]);\n\n?&gt;\n</code></pre> <p></p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#4-secondary-development","title":"4.  Secondary development","text":"<p>If the default model weights provided by the seal recognition production line are not satisfactory in terms of accuracy or speed in your scenario, you can try using your own specific domain or application scenario data to further fine tune the existing model to improve the recognition performance of the seal recognition production line in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#41-model-fine-tuning","title":"4.1 Model fine-tuning","text":"<p>Due to the fact that the seal recognition production line consists of three modules, the performance of the model production line may not be as expected due to any of these modules.</p> <p>You can analyze images with poor recognition performance and refer to the following rules for analysis and model fine-tuning:</p> <ul> <li>If the seal area is incorrectly located within the overall layout, the layout detection module may be insufficient. You need to refer to the Customization section in the Layout Detection Module Development Tutorial and use your private dataset to fine-tune the layout detection model.</li> <li>If there is a significant amount of text that has not been detected (i.e. text miss detection phenomenon), it may be due to the shortcomings of the text detection model. You need to refer to the Secondary Development section in the Seal Text Detection Module Development Tutorial to fine tune the text detection model using your private dataset.</li> <li> <p>If seal texts are undetected (i.e., text miss detection), the text detection model may be insufficient. You need to refer to the Customization section in the Text Detection Module Development Tutorial and use your private dataset to fine-tune the text detection model.</p> </li> <li> <p>If many detected texts contain recognition errors (i.e., the recognized text content does not match the actual text content), the text recognition model requires further improvement. You need to refer to the Customization section.</p> </li> </ul>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After completing fine-tuning training using a private dataset, you can obtain a local model weight file.</p> <p>If you need to use the fine tuned model weights, simply modify the production line configuration file and replace the local path of the fine tuned model weights with the corresponding position in the production line configuration file</p> <p><pre><code>......\n Pipeline:\n  layout_model: RT-DETR-H_layout_3cls #can be modified to the local path of the fine tuned model\n  text_det_model: PP-OCRv4_server_seal_det  #can be modified to the local path of the fine tuned model\n  text_rec_model: PP-OCRv4_server_rec #can be modified to the local path of the fine tuned model\n  layout_batch_size: 1\n  text_rec_batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Subsequently, refer to the command line or Python script in the local experience to load the modified production line configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html#5-multiple-hardware-support","title":"5.  Multiple hardware support","text":"<p>PaddleX supports various mainstream hardware devices such as Nvidia GPU, Kunlun Core XPU, Ascend NPU, and Cambrian MLU, and can seamlessly switch between different hardware devices by simply modifying the <code>--device</code> parameter.</p> <p>For example, if you use Nvidia GPU for inference on a seal recognition production line, the Python command you use is:</p> <pre><code>paddlex --pipeline seal_recognition --input seal_text_det.png --device gpu:0 --save_path output\n</code></pre> <p>At this point, if you want to switch the hardware to Ascend NPU, simply modify the <code>--device</code> in the Python command to NPU:</p> <pre><code>paddlex --pipeline seal_recognition --input seal_text_det.png --device npu:0 --save_path output\n</code></pre> <p>If you want to use the seal recognition production line on a wider range of hardware, please refer to the PaddleX Multi Hardware Usage Guide\u3002</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html","title":"General Table Recognition Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#1-introduction-to-the-general-table-recognition-pipeline","title":"1. Introduction to the General Table Recognition Pipeline","text":"<p>Table recognition is a technology that automatically identifies and extracts table content and its structure from documents or images. It is widely used in data entry, information retrieval, and document analysis. By leveraging computer vision and machine learning algorithms, table recognition can convert complex table information into editable formats, facilitating further data processing and analysis for users.</p> <p></p> <p>The General Table Recognition Pipeline comprises modules for table structure recognition, layout analysis, text detection, and text recognition.</p> <p>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model size, choose a model with a smaller storage footprint.</p>  \ud83d\udc49Model List Details <p>Table Recognition Module Models:</p> ModelModel Download Link Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description SLANetInference Model/Trained Model 59.52 522.536 1845.37 6.9 M SLANet is a table structure recognition model developed by Baidu PaddleX Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information. SLANet_plusInference Model/Trained Model 63.69 522.536 1845.37 6.9 M  SLANet_plus is an enhanced version of SLANet, a table structure recognition model developed by Baidu PaddleX Team. Compared to SLANet, SLANet_plus significantly improves its recognition capabilities for wireless and complex tables, while reducing the model's sensitivity to the accuracy of table localization. Even when there are offsets in table localization, it can still perform relatively accurate recognition.  <p>Note: The above accuracy metrics are measured on PaddleX's internal self-built English table recognition dataset. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Layout Analysis Module Models:</p> ModelModel Download Link mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1xInference Model/Trained Model 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_tableInference Model/Trained Model 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3clsInference Model/Trained Model 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17clsInference Model/Trained Model 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3clsInference Model/Trained Model 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17clsInference Model/Trained Model 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3clsInference Model/Trained Model 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17clsInference Model/Trained Model 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The above accuracy metrics are evaluated on PaddleX's self-built layout analysis dataset containing 10,000 images. All GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>Text Detection Module Models:</p> Model NameModel Download Link Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) PP-OCRv4_mobile_detInference Model/Trained Model 77.79 10.6923 120.177 4.2 M PP-OCRv4_server_detInference Model/Trained Model 82.69 83.3501 2434.01 100.1M"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX's pre-trained model pipelines allow for quick experience of their effects. You can experience the effects of the General Image Classification pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Table Recognition pipeline by using the demo images provided by the official. For example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the models in the pipeline online.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Table Recognition pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Guide.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#21-command-line-experience","title":"2.1 Command Line Experience","text":"<p>Experience the effects of the table recognition pipeline with a single command:</p> <p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline table_recognition --input table_recognition.jpg --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it's the table recognition pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the 1st and 2nd GPUs). CPU can also be selected (--device cpu).\n</code></pre> <p>When executing the above command, the default table recognition pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config table_recognition\n</code></pre> <p>After execution, the table recognition pipeline configuration file will be saved in the current directory. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config table_recognition --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./table_recognition.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./table_recognition.yaml --input table_recognition.jpg --device gpu:0\n</code></pre> <p>Here, parameters like <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If they are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <p></p> <p>The visualized image not saved by default. You can customize the save path through <code>--save_path</code>, and then all results will be saved in the specified path.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#22-python-script-integration","title":"2.2 Python Script Integration","text":"<p>A few lines of code are all you need to quickly perform inference with the pipeline. Taking the General Table Recognition pipeline as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"table_recognition\")\n\noutput = pipeline.predict(\"table_recognition.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_img(\"./output/\")  # Save the results in img format\n    res.save_to_xlsx(\"./output/\")  # Save the results in Excel format\n    res.save_to_html(\"./output/\") # Save results in HTML format\n</code></pre> The results are the same as those obtained through the command line.</p> <p>In the above Python script, the following steps are executed:</p> <p>\uff081\uff09Instantiate the  production line object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the production line or the path to the production line configuration file. If it is the name of the production line, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for production line model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the production line supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the  production line object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters save_to_img Save the results as an img format file <code>- save_path</code>: str, the path to save the file. When it's a directory, the saved file name will be consistent with the input file type; save_to_html Save the results as an html format file <code>- save_path</code>: str, the path to save the file. When it's a directory, the saved file name will be consistent with the input file type; save_to_xlsx Save the results as a spreadsheet format file <code>- save_path</code>: str, the path to save the file. When it's a directory, the saved file name will be consistent with the input file type; <p>Where <code>save_to_img</code> can save visualization results (including OCR result images, layout analysis result images, table structure recognition result images), <code>save_to_html</code> can directly save the table as an html file (including text and table formatting), and <code>save_to_xlsx</code> can save the table as an Excel format file (including text and formatting).</p> <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/table_recognition.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/table_recognition.yaml\")\noutput = pipeline.predict(\"table_recognition.jpg\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_img(\"./output/\")  # Save results in img format\n    res.save_to_xlsx(\"./output/\")  # Save results in Excel format\n    res.save_to_html(\"./output/\") # Save results in HTML format\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that aim to deeply optimize model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Locate and recognize tables in images.</p> <p><code>POST /table-recognition</code></p> <ul> <li>Request body properties:</li> </ul> Name Type Description Required <code>image</code> <code>string</code> The URL of an image file accessible by the service or the Base64 encoded result of the image file content. Yes <code>inferenceParams</code> <code>object</code> Inference parameters. No <p>Properties of <code>inferenceParams</code>:</p> Name Type Description Required <code>maxLongSide</code> <code>integer</code> During inference, if the length of the longer side of the input image for the text detection model is greater than <code>maxLongSide</code>, the image will be scaled so that the length of the longer side equals <code>maxLongSide</code>. No <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>tables</code> <code>array</code> Positions and contents of tables. <code>layoutImage</code> <code>string</code> Layout area detection result image. The image is in JPEG format and encoded using Base64. <code>ocrImage</code> <code>string</code> OCR result image. The image is in JPEG format and encoded using Base64. <p>Each element in <code>tables</code> is an <code>object</code> with the following properties:</p> Name Type Description <code>bbox</code> <code>array</code> Table position. The elements in the array are the x-coordinate of the top-left corner, the y-coordinate of the top-left corner, the x-coordinate of the bottom-right corner, and the y-coordinate of the bottom-right corner of the bounding box, respectively. <code>html</code> <code>string</code> Table recognition result in HTML format. Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/table-recognition\"\nimage_path = \"./demo.jpg\"\nocr_image_path = \"./ocr.jpg\"\nlayout_image_path = \"./layout.jpg\"\n\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(ocr_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"ocrImage\"]))\nprint(f\"Output image saved at {ocr_image_path}\")\nwith open(layout_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"layoutImage\"]))\nprint(f\"Output image saved at {layout_image_path}\")\nprint(\"\\nDetected tables:\")\nprint(result[\"tables\"])\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string imagePath = \"./demo.jpg\";\n    const std::string ocrImagePath = \"./ocr.jpg\";\n    const std::string layoutImagePath = \"./layout.jpg\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(imagePath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedImage = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"image\"] = encodedImage;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/table-recognition\", headers, body, \"application/json\");\n\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedImage = result[\"ocrImage\"];\n        std::string decoded_string = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedOcrImage(decoded_string.begin(), decoded_string.end());\n        std::ofstream outputOcrFile(ocrImagePath, std::ios::binary | std::ios::out);\n        if (outputOcrFile.is_open()) {\n            outputOcrFile.write(reinterpret_cast&lt;char*&gt;(decodedOcrImage.data()), decodedOcrImage.size());\n            outputOcrFile.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; ocrImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; ocrImagePath &lt;&lt; std::endl;\n        }\n\n        encodedImage = result[\"layoutImage\"];\n        decodedString = base64::from_base64(encodedImage);\n        std::vector&lt;unsigned char&gt; decodedLayoutImage(decodedString.begin(), decodedString.end());\n        std::ofstream outputLayoutFile(layoutImagePath, std::ios::binary | std::ios::out);\n        if (outputLayoutFile.is_open()) {\n            outputLayoutFile.write(reinterpret_cast&lt;char*&gt;(decodedLayoutImage.data()), decodedlayoutImage.size());\n            outputLayoutFile.close();\n            std::cout &lt;&lt; \"Output image saved at \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; layoutImagePath &lt;&lt; std::endl;\n        }\n\n        auto tables = result[\"tables\"];\n        std::cout &lt;&lt; \"\\nDetected tables:\" &lt;&lt; std::endl;\n        for (const auto&amp; table : tables) {\n            std::cout &lt;&lt; table &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/table-recognition\";\n        String imagePath = \"./demo.jpg\";\n        String ocrImagePath = \"./ocr.jpg\";\n        String layoutImagePath = \"./layout.jpg\";\n\n        File file = new File(imagePath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String imageData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"image\", imageData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                String ocrBase64Image = result.get(\"ocrImage\").asText();\n                String layoutBase64Image = result.get(\"layoutImage\").asText();\n                JsonNode tables = result.get(\"tables\");\n\n                byte[] imageBytes = Base64.getDecoder().decode(ocrBase64Image);\n                try (FileOutputStream fos = new FileOutputStream(ocrImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + ocrBase64Image);\n\n                imageBytes = Base64.getDecoder().decode(layoutBase64Image);\n                try (FileOutputStream fos = new FileOutputStream(layoutImagePath)) {\n                    fos.write(imageBytes);\n                }\n                System.out.println(\"Output image saved at \" + layoutImagePath);\n\n                System.out.println(\"\\nDetected tables: \" + tables.toString());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/table-recognition\"\n    imagePath := \"./demo.jpg\"\n    ocrImagePath := \"./ocr.jpg\"\n    layoutImagePath := \"./layout.jpg\"\n\n    imageBytes, err := ioutil.ReadFile(imagePath)\n    if err != nil {\n        fmt.Println(\"Error reading image file:\", err)\n        return\n    }\n    imageData := base64.StdEncoding.EncodeToString(imageBytes)\n\n    payload := map[string]string{\"image\": imageData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            OcrImage      string   `json:\"ocrImage\"`\n            LayoutImage      string   `json:\"layoutImage\"`\n            Tables []map[string]interface{} `json:\"tables\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    ocrImageData, err := base64.StdEncoding.DecodeString(respData.Result.OcrImage)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(ocrImagePath, ocrImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", ocrImagePath)\n\n    layoutImageData, err := base64.StdEncoding.DecodeString(respData.Result.LayoutImage)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 image data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(layoutImagePath, layoutImageData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing image to file:\", err)\n        return\n    }\n    fmt.Printf(\"Image saved at %s.jpg\\n\", layoutImagePath)\n\n    fmt.Println(\"\\nDetected tables:\")\n    for _, table := range respData.Result.Tables {\n        fmt.Println(table)\n    }\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/table-recognition\";\n    static readonly string imagePath = \"./demo.jpg\";\n    static readonly string ocrImagePath = \"./ocr.jpg\";\n    static readonly string layoutImagePath = \"./layout.jpg\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] imageBytes = File.ReadAllBytes(imagePath);\n        string image_data = Convert.ToBase64String(imageBytes);\n\n        var payload = new JObject{ { \"image\", image_data } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string ocrBase64Image = jsonResponse[\"result\"][\"ocrImage\"].ToString();\n        byte[] ocrImageBytes = Convert.FromBase64String(ocrBase64Image);\n        File.WriteAllBytes(ocrImagePath, ocrImageBytes);\n        Console.WriteLine($\"Output image saved at {ocrImagePath}\");\n\n        string layoutBase64Image = jsonResponse[\"result\"][\"layoutImage\"].ToString();\n        byte[] layoutImageBytes = Convert.FromBase64String(layoutBase64Image);\n        File.WriteAllBytes(layoutImagePath, layoutImageBytes);\n        Console.WriteLine($\"Output image saved at {layoutImagePath}\");\n\n        Console.WriteLine(\"\\nDetected tables:\");\n        Console.WriteLine(jsonResponse[\"result\"][\"tables\"].ToString());\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/table-recognition'\nconst imagePath = './demo.jpg'\nconst ocrImagePath = \"./ocr.jpg\";\nconst layoutImagePath = \"./layout.jpg\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'image': encodeImageToBase64(imagePath)\n  })\n};\n\nfunction encodeImageToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n\n    const imageBuffer = Buffer.from(result[\"ocrImage\"], 'base64');\n    fs.writeFile(ocrImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${ocrImagePath}`);\n    });\n\n    imageBuffer = Buffer.from(result[\"layoutImage\"], 'base64');\n    fs.writeFile(layoutImagePath, imageBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output image saved at ${layoutImagePath}`);\n    });\n\n    console.log(\"\\nDetected tables:\");\n    console.log(result[\"tables\"]);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/table-recognition\";\n$image_path = \"./demo.jpg\";\n$ocr_image_path = \"./ocr.jpg\";\n$layout_image_path = \"./layout.jpg\";\n\n$image_data = base64_encode(file_get_contents($image_path));\n$payload = array(\"image\" =&gt; $image_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\nfile_put_contents($ocr_image_path, base64_decode($result[\"ocrImage\"]));\necho \"Output image saved at \" . $ocr_image_path . \"\\n\";\n\nfile_put_contents($layout_image_path, base64_decode($result[\"layoutImage\"]));\necho \"Output image saved at \" . $layout_image_path . \"\\n\";\n\necho \"\\nDetected tables:\\n\";\nprint_r($result[\"tables\"]);\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the general table recognition pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the general table recognition pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the general table recognition pipeline consists of four modules, unsatisfactory performance may stem from any of these modules.</p> <p>Analyze images with poor recognition results and follow the rules below for analysis and model fine-tuning:</p> <ul> <li>If the detected table structure is incorrect (e.g., row and column recognition errors, incorrect cell positions), the table structure recognition module may be insufficient. You need to refer to the Customization section in the Table Structure Recognition Module Development Tutorial and use your private dataset to fine-tune the table structure recognition model.</li> <li>If the table area is incorrectly located within the overall layout, the layout detection module may be insufficient. You need to refer to the Customization section in the Layout Detection Module Development Tutorial and use your private dataset to fine-tune the layout detection model.</li> <li>If many texts are undetected (i.e., text miss detection), the text detection model may be insufficient. You need to refer to the Customization section in the Text Detection Module Development Tutorial and use your private dataset to fine-tune the text detection model.</li> <li>If many detected texts contain recognition errors (i.e., the recognized text content does not match the actual text content), the text recognition model requires further improvement. You need to refer to the Customization section.</li> </ul>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning your model with a private dataset, you will obtain local model weights files.</p> <p>To use the fine-tuned model weights, simply modify the production line configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the configuration file:</p> <p><pre><code>......\n Pipeline:\n  layout_model: PicoDet_layout_1x  # Can be modified to the local path of the fine-tuned model\n  table_model: SLANet  # Can be modified to the local path of the fine-tuned model\n  text_det_model: PP-OCRv4_mobile_det  # Can be modified to the local path of the fine-tuned model\n  text_rec_model: PP-OCRv4_mobile_rec  # Can be modified to the local path of the fine-tuned model\n  layout_batch_size: 1\n  text_rec_batch_size: 1\n  table_batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> Then, refer to the command line or Python script method in the local experience to load the modified production line configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html#5-multi-hardware-support","title":"5. Multi-Hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPU, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for table recognition pipeline inference, the Python command is:</p> <p><pre><code>paddlex --pipeline table_recognition --input table_recognition.jpg --device gpu:0\n</code></pre> At this time, if you want to switch the hardware to Ascend NPU, simply modify <code>--device</code> in the Python command to npu:</p> <p><pre><code>paddlex --pipeline table_recognition --input table_recognition.jpg --device npu:0\n</code></pre> If you want to use the general table recognition pipeline on more types of hardware, please refer to the PaddleX Multi-Hardware Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html","title":"Time Series Anomaly Detection Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#1-introduction-to-the-general-time-series-anomaly-detection-pipeline","title":"1. Introduction to the General Time Series Anomaly Detection Pipeline","text":"<p>Time series anomaly detection is a technique for identifying abnormal patterns or behaviors in time series data. It is widely applied in fields such as network security, equipment monitoring, and financial fraud detection. By analyzing normal trends and patterns in historical data, it discovers events that significantly deviate from expected behaviors, such as sudden spikes in network traffic or unusual transaction activities. Time series anomaly detection enable automatic identification of anomalies in data. This technology provides real-time alerts for enterprises and organizations, helping them promptly address potential risks and issues. It plays a crucial role in ensuring system stability and security.</p> <p></p> <p>The General Time Series Anomaly Detection Pipeline includes a time series anomaly detection module. If you prioritize model accuracy, choose a model with higher precision. If you prioritize inference speed, select a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage footprint.</p> Model NameModel Download Link Precision Recall F1-Score Model Storage Size (M) AutoEncoder_adInference Model/Trained Model 99.36 84.36 91.25 52K DLinear_adInference Model/Trained Model 98.98 93.96 96.41 112K Nonstationary_adInference Model/Trained Model 98.55 88.95 93.51 1.8M PatchTST_adInference Model/Trained Model 98.78 90.70 94.57 320K TimesNet_adInference Model/Trained Model 98.37 94.80 96.56 1.3M <p>Note: The above precision metrics are measured on the PSM dataset. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX allow for quick experience of their effects. You can experience the effects of the General Time Series Anomaly Detection Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Time Series Anomaly Detection Pipeline using the official demo for recognition, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model within the pipeline online.</p> <p>Note: Due to the close relationship between time series data and scenarios, the official built-in models for online experience of time series tasks are only model solutions for a specific scenario and are not universal. They are not applicable to other scenarios. Therefore, the experience mode does not support using arbitrary files to experience the effects of the official model solutions. However, after training a model for your own scenario data, you can select your trained model solution and use data from the corresponding scenario for online experience.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Time Series Anomaly Detection Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>A single command is all you need to quickly experience the effects of the time series anomaly detection pipeline:</p> <p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline ts_ad --input ts_ad.csv --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it's the image anomaly detection pipeline.\n--input: The local path or URL of the input image to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). CPU can also be selected (--device cpu).\n</code></pre> <p>When executing the above command, the default image anomaly detection pipeline configuration file is loaded. If you need to customize the configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config ts_ad --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./ts_ad.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./ts_ad.yaml --input ts_ad.csv --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result obtained is:</p> <pre><code>{'input_path': 'ts_ad.csv', 'anomaly':            label\ntimestamp\n220226         0\n220227         0\n220228         0\n220229         0\n220230         0\n...          ...\n220317         1\n220318         1\n220319         1\n220320         1\n220321         0\n\n[96 rows x 1 columns]}\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code can complete the rapid inference of the pipeline. Taking the general time series anomaly detection pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"ts_ad\")\n\noutput = pipeline.predict(\"ts_ad.csv\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_csv(\"./output/\")  # Save the result in CSV format\n</code></pre> <p>The result obtained is the same as that of the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>\uff081\uff09Instantiate the  production line object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the production line or the path to the production line configuration file. If it is the name of the production line, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for production line model inference. Supports: \"gpu\", \"cpu\". <code>str</code> <code>gpu</code> <code>use_hpip</code> Whether to enable high-performance inference, only available if the production line supports it. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the  production line object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters save_to_csv Saves results as a csv file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_xlsx Saves results as table file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/ts_ad.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/ts_ad.yaml\")\noutput = pipeline.predict(\"ts_ad.csv\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_csv(\"./output/\")  # Save results in CSV format\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for the performance metrics of deployment strategies (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX enables users to achieve low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the properties of the response body are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error description. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the properties of the response body are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error description. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs time-series anomaly detection.</p> <p><code>POST /time-series-anomaly-detection</code></p> <ul> <li>Attributes of the request body:</li> </ul> Name Type Description Required <code>csv</code> <code>string</code> The URL of a CSV file accessible by the service or the Base64 encoded result of the CSV file content. The CSV file must be encoded in UTF-8. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>csv</code> <code>string</code> Time-series anomaly detection results in CSV format. Encoded in UTF-8+Base64. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"csv\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/time-series-anomaly-detection\"\ncsv_path = \"./test.csv\"\noutput_csv_path = \"./out.csv\"\n\nwith open(csv_path, \"rb\") as file:\n    csv_bytes = file.read()\n    csv_data = base64.b64encode(csv_bytes).decode(\"ascii\")\n\npayload = {\"csv\": csv_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_csv_path, \"wb\") as f:\n    f.write(base64.b64decode(result[\"csv\"]))\nprint(f\"Output time-series data saved at  {output_csv_path}\")\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string csvPath = \"./test.csv\";\n    const std::string outputCsvPath = \"./out.csv\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(csvPath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedCsv = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"csv\"] = encodedCsv;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/time-series-anomaly-detection\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedCsv = result[\"csv\"];\n        decodedString = base64::from_base64(encodedCsv);\n        std::vector&lt;unsigned char&gt; decodedCsv(decodedString.begin(), decodedString.end());\n        std::ofstream outputCsv(outputCsvPath, std::ios::binary | std::ios::out);\n        if (outputCsv.is_open()) {\n            outputCsv.write(reinterpret_cast&lt;char*&gt;(decodedCsv.data()), decodedCsv.size());\n            outputCsv.close();\n            std::cout &lt;&lt; \"Output time-series data saved at \" &lt;&lt; outputCsvPath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outputCsvPath &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        std::cout &lt;&lt; response-&gt;body &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/time-series-anomaly-detection\";\n        String csvPath = \"./test.csv\";\n        String outputCsvPath = \"./out.csv\";\n\n        File file = new File(csvPath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String csvData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"csv\", csvData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n\n                String base64Csv = result.get(\"csv\").asText();\n                byte[] csvBytes = Base64.getDecoder().decode(base64Csv);\n                try (FileOutputStream fos = new FileOutputStream(outputCsvPath)) {\n                    fos.write(csvBytes);\n                }\n                System.out.println(\"Output time-series data saved at \" + outputCsvPath);\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/time-series-anomaly-detection\"\n    csvPath := \"./test.csv\";\n    outputCsvPath := \"./out.csv\";\n\n    csvBytes, err := ioutil.ReadFile(csvPath)\n    if err != nil {\n        fmt.Println(\"Error reading csv file:\", err)\n        return\n    }\n    csvData := base64.StdEncoding.EncodeToString(csvBytes)\n\n    payload := map[string]string{\"csv\": csvData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Csv string `json:\"csv\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputCsvData, err := base64.StdEncoding.DecodeString(respData.Result.Csv)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 csv data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputCsvPath, outputCsvData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing csv to file:\", err)\n        return\n    }\n    fmt.Printf(\"Output time-series data saved at %s.csv\", outputCsvPath)\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/time-series-anomaly-detection\";\n    static readonly string csvPath = \"./test.csv\";\n    static readonly string outputCsvPath = \"./out.csv\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] csvBytes = File.ReadAllBytes(csvPath);\n        string csvData = Convert.ToBase64String(csvBytes);\n\n        var payload = new JObject{ { \"csv\", csvData } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Csv = jsonResponse[\"result\"][\"csv\"].ToString();\n        byte[] outputCsvBytes = Convert.FromBase64String(base64Csv);\n        File.WriteAllBytes(outputCsvPath, outputCsvBytes);\n        Console.WriteLine($\"Output time-series data saved at {outputCsvPath}\");\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/time-series-anomaly-detection'\nconst csvPath = \"./test.csv\";\nconst outputCsvPath = \"./out.csv\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'csv': encodeFileToBase64(csvPath)\n  })\n};\n\nfunction encodeFileToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n\n    const csvBuffer = Buffer.from(result[\"csv\"], 'base64');\n    fs.writeFile(outputCsvPath, csvBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output time-series data saved at ${outputCsvPath}`);\n    });\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/time-series-anomaly-detection\";\n$csv_path = \"./test.csv\";\n$output_csv_path = \"./out.csv\";\n\n$csv_data = base64_encode(file_get_contents($csv_path));\n$payload = array(\"csv\" =&gt; $csv_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\n\nfile_put_contents($output_csv_path, base64_decode($result[\"csv\"]));\necho \"Output time-series data saved at \" . $output_csv_path . \"\\n\";\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Time Series Anomaly Detection Pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the General Time Series Anomaly Detection Pipeline includes a time series anomaly detection module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Time Series Modules Development Tutorial to fine-tune the time series anomaly detection model using your private dataset.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weights files.</p> <p>To use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <pre><code>......\nPipeline:\n  model: DLinear_ad  # Can be modified to the local path of the fine-tuned model\n  batch_size: 1\n  device: \"gpu:0\"\n......\n</code></pre> <p>Then, refer to the command line method or Python script method in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference of the time series anomaly detection pipeline, the Python command is:</p> <p><pre><code>paddlex --pipeline ts_ad --input ts_ad.csv --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline ts_ad --input ts_ad.csv --device npu:0\n</code></pre> If you want to use the General Time-Series Anomaly Detection Pipeline on more diverse hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html","title":"Time Series Classification Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#1-introduction-to-general-time-series-classification-pipeline","title":"1. Introduction to General Time Series Classification Pipeline","text":"<p>Time series classification is a technique that categorizes time-series data into predefined classes, widely applied in fields such as behavior recognition and financial trend analysis. By analyzing features that vary over time, it identifies different patterns or events, for example, classifying a speech signal as \"greeting\" or \"request,\" or categorizing stock price movements as \"rising\" or \"falling.\" Time series classification typically employs machine learning and deep learning models, effectively capturing temporal dependencies and variation patterns to provide accurate classification labels for data. This technology plays a pivotal role in applications such as intelligent monitoring and market forecasting.</p> <p></p> <p>The General Time Series Classification Pipeline includes a Time Series Classification module.</p> Model NameModel Download Link Acc(%) Model Size (M) TimesNet_clsInference Model/Trained Model 87.5 792K <p>Note: The above accuracy metrics are measured on the UWaveGestureLibrary dataset. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX provides pre-trained model pipelines that can be quickly experienced. You can experience the effects of the General Time Series Classification Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience online the effects of the General Time Series Classification Pipeline using the official demo for recognition, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model in the pipeline online.</p> <p>Note: Due to the close relationship between time series data and scenarios, the official built-in model for online experience of time series tasks is only a model solution for a specific scenario and is not a general solution applicable to other scenarios. Therefore, the experience method does not support using arbitrary files to experience the effect of the official model solution. However, after training a model for your own scenario data, you can select your trained model solution and use data from the corresponding scenario for online experience.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Time Series Classification Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>Experience the effects of the time series classification pipeline quickly with a single command:</p> <p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline ts_cls --input ts_cls.csv --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the time series classification pipeline.\n--input: The local path or URL of the input sequence to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default time series classification pipeline configuration file is loaded. If you need to customize the configuration file, you can execute the following command to obtain it:</p>  \ud83d\udc49Click to Expand <pre><code>paddlex --get_pipeline_yaml ts_cls\n</code></pre> <p>After execution, the time series classification pipeline configuration file will be saved in the current path. If you wish to customize the save location, you can execute the following command (assuming the custom save location is <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config ts_cls --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./ts_ad.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./ts_cls.yaml --input ts_cls.csv --device gpu:0\n</code></pre> <p>In this command, parameters such as <code>--model</code> and <code>--device</code> are not required to be specified, as they will use the parameters defined in the configuration file. If these parameters are specified, the specified values will take precedence.</p> <p>After execution, the result is:</p> <pre><code>{'input_path': 'ts_cls.csv', 'classification':         classid     score\nsample\n0             0  0.617688}\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code can complete rapid inference for production lines. Taking the General Time Series Classification Pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"ts_cls\")\n\noutput = pipeline.predict(\"ts_cls.csv\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_csv(\"./output/\")  # Save results in CSV format\n</code></pre> <p>The results obtained are the same as those from the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the <code>create_pipeline</code> to create a pipeline object: Specific parameter descriptions are as follows:</p> Parameter Description Type Default <code>pipeline</code> The name of the pipeline or the path to the pipeline configuration file. If it's a pipeline name, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for pipeline model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference. Available only if the pipeline supports it. <code>bool</code> <code>False</code> <p>(2) Call the <code>predict</code> method of the pipeline object for inference: The <code>predict</code> method takes <code>x</code> as a parameter, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Description Python Var Supports directly passing Python variables, such as numpy.ndarray representing image data. <code>str</code> Supports passing the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. <code>str</code> Supports passing the URL of the file to be predicted, such as the network URL of an image file: Example. <code>str</code> Supports passing a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. <code>dict</code> Supports passing a dictionary type, where the key needs to correspond to the specific task, e.g., \"img\" for image classification tasks, and the value of the dictionary supports the above data types, e.g., <code>{\"img\": \"/root/data1\"}</code>. <code>list</code> Supports passing a list, where the list elements need to be of the above types, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code>, <code>[{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>(3) Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained by iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/ts_cls.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/ts_cls.yaml\")\noutput = pipeline.predict(\"ts_cls.csv\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_csv(\"./output/\")  # Save results in CSV format\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for deployment performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins that deeply optimize model inference and pre/post-processing to significantly speed up the end-to-end process. Refer to the PaddleX High-Performance Inference Guide for detailed high-performance inference procedures.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX enables users to achieve low-cost service-oriented deployment of pipelines. Refer to the PaddleX Service-Oriented Deployment Guide for detailed service-oriented deployment procedures.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service:</p> <ul> <li><code>infer</code></li> </ul> <p>Classify time-series data.</p> <p><code>POST /time-series-classification</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>csv</code> <code>string</code> The URL of a CSV file accessible by the service or the Base64 encoded result of the CSV file content. The CSV file must be encoded in UTF-8. Yes <ul> <li>When the request is processed successfully, the <code>result</code> in the response body has the following properties:</li> </ul> Name Type Description <code>label</code> <code>string</code> Class label. <code>score</code> <code>number</code> Class score. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"label\": \"running\",\n\"score\": 0.97\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/time-series-classification\"\ncsv_path = \"./test.csv\"\n\nwith open(csv_path, \"rb\") as file:\n    csv_bytes = file.read()\n    csv_data = base64.b64encode(csv_bytes).decode(\"ascii\")\n\npayload = {\"csv\": csv_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nprint(f\"label: {result['label']}, score: {result['score']}\")\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string csvPath = \"./test.csv\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(csvPath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedCsv = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"csv\"] = encodedCsv;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/time-series-classification\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n        std::cout &lt;&lt; \"label: \" &lt;&lt; result[\"label\"] &lt;&lt; \", score: \" &lt;&lt; result[\"score\"] &lt;&lt; std::endl;\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        std::cout &lt;&lt; response-&gt;body &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/time-series-classification\";\n        String csvPath = \"./test.csv\";\n\n        File file = new File(csvPath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String csvData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"csv\", csvData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n                System.out.println(\"label: \" + result.get(\"label\").asText() + \", score: \" + result.get(\"score\").asText());\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/time-series-classification\"\n    csvPath := \"./test.csv\";\n\n    csvBytes, err := ioutil.ReadFile(csvPath)\n    if err != nil {\n        fmt.Println(\"Error reading csv file:\", err)\n        return\n    }\n    csvData := base64.StdEncoding.EncodeToString(csvBytes)\n\n    payload := map[string]string{\"csv\": csvData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Label string `json:\"label\"`\n            Score string `json:\"score\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    fmt.Printf(\"label: %s, score: %s\\n\", respData.Result.Label, respData.Result.Score)\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/time-series-classification\";\n    static readonly string csvPath = \"./test.csv\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] csveBytes = File.ReadAllBytes(csvPath);\n        string csvData = Convert.ToBase64String(csveBytes);\n\n        var payload = new JObject{ { \"csv\", csvData } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string label = jsonResponse[\"result\"][\"label\"].ToString();\n        string score = jsonResponse[\"result\"][\"score\"].ToString();\n        Console.WriteLine($\"label: {label}, score: {score}\");\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/time-series-classification'\nconst csvPath = \"./test.csv\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'csv': encodeFileToBase64(csvPath)\n  })\n};\n\nfunction encodeFileToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n    console.log(`label: ${result[\"label\"]}, score: ${result[\"score\"]}`);\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/time-series-classification\";\n$csv_path = \"./test.csv\";\n\n$csv_data = base64_encode(file_get_contents($csv_path));\n$payload = array(\"csv\" =&gt; $csv_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\necho \"label: \" . $result[\"label\"] . \", score: \" . $result[\"score\"];\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing capabilities on user devices themselves, allowing devices to process data directly without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. Refer to the PaddleX Edge Deployment Guide for detailed edge deployment procedures. Choose the appropriate deployment method based on your needs to proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Time Series Classification Pipeline do not meet your requirements for accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the General Time Series Classification Pipeline includes a time series classification module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Time Series Classification Module Tutorial to fine-tune the time series classification model using your private dataset.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning the model with your private dataset, you will obtain local model weights.</p> <p>To use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: TimesNet_cls  # Replace with the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 1\n......\n</code></pre> Then, refer to the command line or Python script methods in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_classification.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for time series classification pipeline inference, the Python command is:</p> <pre><code>paddlex --pipeline ts_cls --input ts_cls.csv --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` flag in the Python command as follows:\n\n```bash\npaddlex --pipeline ts_cls --input ts_cls.csv --device npu:0\n</code></pre> <p>If you intend to use the General Time Series Classification Pipeline on a wider range of hardware, please refer to the PaddleX Multi-Hardware Usage Guide.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html","title":"Time Series Forecasting Pipeline Tutorial","text":""},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#1-introduction-to-the-general-time-series-forecasting-pipeline","title":"1. Introduction to the General Time Series Forecasting Pipeline","text":"<p>Time series forecasting is a technique that utilizes historical data to predict future trends by analyzing the patterns of change in time series data. It is widely applied in fields such as financial markets, weather forecasting, and sales prediction. Time series forecasting often employs statistical methods or deep learning models (e.g., LSTM, ARIMA), capable of handling temporal dependencies in data to provide accurate predictions, assisting decision-makers in better planning and response. This technology plays a crucial role in various industries, including energy management, supply chain optimization, and market analysis.</p> <p></p> <p>The General Time Series Forecasting Pipeline includes a time series forecasting module. If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, select a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</p> Model NameModel Download Link MSE MAE Model Storage Size (M) DLinearInference Model/Trained Model 0.382 0.394 72K NLinearInference Model/Trained Model 0.386 0.392 40K NonstationaryInference Model/Trained Model 0.600 0.515 55.5 M PatchTSTInference Model/Trained Model 0.385 0.397 2.0M RLinearInference Model/Trained Model 0.384 0.392 40K TiDEInference Model/Trained Model 0.405 0.412 31.7M TimesNetInference Model/Trained Model 0.417 0.431 4.9M <p>Note: The above accuracy metrics are measured on ETTH1. All model GPU inference times are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#2-quick-start","title":"2. Quick Start","text":"<p>The pre-trained model pipelines provided by PaddleX allow for quick experience of their effects. You can experience the effects of the General Time Series Forecasting Pipeline online or locally using command line or Python.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#21-online-experience","title":"2.1 Online Experience","text":"<p>You can experience the General Time Series Forecasting Pipeline online using the demo provided by the official team, for example:</p> <p></p> <p>If you are satisfied with the pipeline's performance, you can directly integrate and deploy it. If not, you can also use your private data to fine-tune the model within the pipeline online.</p> <p>Note: Due to the close relationship between time series data and scenarios, the official built-in models for online time series tasks are scenario-specific and not universal. Therefore, the experience mode does not support using arbitrary files to experience the effects of the official model solutions. However, after training a model with your own scenario data, you can select your trained model solution and use data from the corresponding scenario for online experience.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#22-local-experience","title":"2.2 Local Experience","text":"<p>Before using the General Time Series Forecasting Pipeline locally, ensure you have installed the PaddleX wheel package following the PaddleX Local Installation Tutorial.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#221-command-line-experience","title":"2.2.1 Command Line Experience","text":"<p>Experience the time series forecasting pipeline with a single command:</p> <p>Experience the image anomaly detection pipeline with a single command\uff0cUse the test file, and replace <code>--input</code> with the local path to perform prediction.</p> <p><pre><code>paddlex --pipeline ts_fc --input ts_fc.csv --device gpu:0\n</code></pre> Parameter Explanation:</p> <pre><code>--pipeline: The name of the pipeline, here it is the time series forecasting pipeline.\n--input: The local path or URL of the input sequence to be processed.\n--device: The GPU index to use (e.g., gpu:0 for the first GPU, gpu:1,2 for the second and third GPUs). You can also choose to use CPU (--device cpu).\n</code></pre> <p>When executing the above command, the default image anomaly detection pipeline configuration file is loaded. If you need to customize the configuration file, you can run the following command to obtain it:</p>  \ud83d\udc49Click to expand <pre><code>paddlex --get_pipeline_config ts_fc --save_path ./my_path\n</code></pre> <p>After obtaining the pipeline configuration file, you can replace <code>--pipeline</code> with the configuration file save path to make the configuration file take effect. For example, if the configuration file save path is <code>./ts_fc.yaml</code>, simply execute:</p> <pre><code>paddlex --pipeline ./ts_fc.yaml --input ts_fc.csv --device gpu:0\n</code></pre> <p>Here, parameters such as <code>--model</code> and <code>--device</code> do not need to be specified, as they will use the parameters in the configuration file. If parameters are still specified, the specified parameters will take precedence.</p> <p>After running, the result is:</p> <pre><code>{'input_path': 'ts_fc.csv', 'forecast':                            OT\ndate\n2018-06-26 20:00:00  9.586131\n2018-06-26 21:00:00  9.379762\n2018-06-26 22:00:00  9.252275\n2018-06-26 23:00:00  9.249993\n2018-06-27 00:00:00  9.164998\n...                       ...\n2018-06-30 15:00:00  8.830340\n2018-06-30 16:00:00  9.291553\n2018-06-30 17:00:00  9.097666\n2018-06-30 18:00:00  8.905430\n2018-06-30 19:00:00  8.993793\n\n[96 rows x 1 columns]}\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#222-python-script-integration","title":"2.2.2 Python Script Integration","text":"<p>A few lines of code can complete the quick inference of the production line. Taking the general time series prediction production line as an example:</p> <p><pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(pipeline=\"ts_fc\")\n\noutput = pipeline.predict(\"ts_fc.csv\")\nfor res in output:\n    res.print()  # Print the structured output of the prediction\n    res.save_to_csv(\"./output/\")  # Save the results in CSV format\n</code></pre> The result obtained is the same as that of the command line method.</p> <p>In the above Python script, the following steps are executed:</p> <p>(1) Instantiate the production line object using <code>create_pipeline</code>: Specific parameter descriptions are as follows:</p> Parameter Description Type Default Value <code>pipeline</code> The name of the production line or the path to the production line configuration file. If it is the name of the production line, it must be supported by PaddleX. <code>str</code> None <code>device</code> The device for production line model inference. Supports: \"gpu\", \"cpu\". <code>str</code> \"gpu\" <code>use_hpip</code> Whether to enable high-performance inference, only available when the production line supports high-performance inference. <code>bool</code> <code>False</code> <p>\uff082\uff09Invoke the <code>predict</code> method of the  production line object for inference prediction: The <code>predict</code> method parameter is <code>x</code>, which is used to input data to be predicted, supporting multiple input methods, as shown in the following examples:</p> Parameter Type Parameter Description Python Var Supports directly passing in Python variables, such as numpy.ndarray representing image data. str Supports passing in the path of the file to be predicted, such as the local path of an image file: <code>/root/data/img.jpg</code>. str Supports passing in the URL of the file to be predicted, such as the network URL of an image file: Example. str Supports passing in a local directory, which should contain files to be predicted, such as the local path: <code>/root/data/</code>. dict Supports passing in a dictionary type, where the key needs to correspond to a specific task, such as \"img\" for image classification tasks. The value of the dictionary supports the above types of data, for example: <code>{\"img\": \"/root/data1\"}</code>. list Supports passing in a list, where the list elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray], [\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"], [\"/root/data1\", \"/root/data2\"], [{\"img\": \"/root/data1\"}, {\"img\": \"/root/data2/img.jpg\"}]</code>. <p>\uff083\uff09Obtain the prediction results by calling the <code>predict</code> method: The <code>predict</code> method is a <code>generator</code>, so prediction results need to be obtained through iteration. The <code>predict</code> method predicts data in batches, so the prediction results are in the form of a list.</p> <p>\uff084\uff09Process the prediction results: The prediction result for each sample is of <code>dict</code> type and supports printing or saving to files, with the supported file types depending on the specific pipeline. For example:</p> Method Description Method Parameters print Prints results to the terminal <code>- format_json</code>: bool, whether to format the output content with json indentation, default is True;<code>- indent</code>: int, json formatting setting, only valid when format_json is True, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, only valid when format_json is True, default is False; save_to_json Saves results as a json file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type;<code>- indent</code>: int, json formatting setting, default is 4;<code>- ensure_ascii</code>: bool, json formatting setting, default is False; save_to_img Saves results as an image file <code>- save_path</code>: str, the path to save the file, when it's a directory, the saved file name is consistent with the input file type; <p>If you have a configuration file, you can customize the configurations of the image anomaly detection pipeline by simply modifying the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the pipeline configuration file.</p> <p>For example, if your configuration file is saved at <code>./my_path/ts_fc.yaml</code>, you only need to execute:</p> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"./my_path/ts_fc.yaml\")\noutput = pipeline.predict(\"ts_fc.csv\")\nfor res in output:\n    res.print()  # Print the structured output of prediction\n    res.save_to_csv(\"./output/\")  # Save results in CSV format\n</code></pre>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#3-development-integrationdeployment","title":"3. Development Integration/Deployment","text":"<p>If the pipeline meets your requirements for inference speed and accuracy in production, you can proceed with development integration/deployment.</p> <p>If you need to directly apply the pipeline in your Python project, refer to the example code in 2.2.2 Python Script Integration.</p> <p>Additionally, PaddleX provides three other deployment methods, detailed as follows:</p> <p>\ud83d\ude80 High-Performance Inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, refer to the PaddleX High-Performance Inference Guide.</p> <p>\u2601\ufe0f Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment of pipelines. For detailed service-oriented deployment procedures, refer to the PaddleX Service-Oriented Deployment Guide.</p> <p>Below are the API references and multi-language service invocation examples:</p> API Reference <p>For main operations provided by the service:</p> <ul> <li>The HTTP request method is POST.</li> <li>The request body and the response body are both JSON data (JSON objects).</li> <li>When the request is processed successfully, the response status code is <code>200</code>, and the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Fixed as <code>0</code>. <code>errorMsg</code> <code>string</code> Error message. Fixed as <code>\"Success\"</code>. <p>The response body may also have a <code>result</code> property of type <code>object</code>, which stores the operation result information.</p> <ul> <li>When the request is not processed successfully, the response body properties are as follows:</li> </ul> Name Type Description <code>errorCode</code> <code>integer</code> Error code. Same as the response status code. <code>errorMsg</code> <code>string</code> Error message. <p>Main operations provided by the service are as follows:</p> <ul> <li><code>infer</code></li> </ul> <p>Performs time-series forecasting.</p> <p><code>POST /time-series-forecasting</code></p> <ul> <li>The request body properties are as follows:</li> </ul> Name Type Description Required <code>csv</code> <code>string</code> The URL of a CSV file accessible by the service or the Base64 encoded result of the CSV file content. The CSV file must be encoded in UTF-8. Yes <ul> <li>When the request is processed successfully, the <code>result</code> of the response body has the following properties:</li> </ul> Name Type Description <code>csv</code> <code>string</code> The time-series forecasting result in CSV format. Encoded in UTF-8+Base64. <p>An example of <code>result</code> is as follows:</p> <pre><code>{\n\"csv\": \"xxxxxx\"\n}\n</code></pre> Multi-Language Service Invocation Examples Python <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/time-series-forecasting\"\ncsv_path = \"./test.csv\"\noutput_csv_path = \"./out.csv\"\n\nwith open(csv_path, \"rb\") as file:\n    csv_bytes = file.read()\n    csv_data = base64.b64encode(csv_bytes).decode(\"ascii\")\n\npayload = {\"csv\": csv_data}\n\nresponse = requests.post(API_URL, json=payload)\n\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_csv_path, \"wb\") as f:\n    f.write(base64.b64decode(result[\"csv\"]))\nprint(f\"Output time-series data saved at  {output_csv_path}\")\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n#include \"cpp-httplib/httplib.h\" // https://github.com/Huiyicc/cpp-httplib\n#include \"nlohmann/json.hpp\" // https://github.com/nlohmann/json\n#include \"base64.hpp\" // https://github.com/tobiaslocker/base64\n\nint main() {\n    httplib::Client client(\"localhost:8080\");\n    const std::string csvPath = \"./test.csv\";\n    const std::string outputCsvPath = \"./out.csv\";\n\n    httplib::Headers headers = {\n        {\"Content-Type\", \"application/json\"}\n    };\n\n    std::ifstream file(csvPath, std::ios::binary | std::ios::ate);\n    std::streamsize size = file.tellg();\n    file.seekg(0, std::ios::beg);\n\n    std::vector&lt;char&gt; buffer(size);\n    if (!file.read(buffer.data(), size)) {\n        std::cerr &lt;&lt; \"Error reading file.\" &lt;&lt; std::endl;\n        return 1;\n    }\n    std::string bufferStr(reinterpret_cast&lt;const char*&gt;(buffer.data()), buffer.size());\n    std::string encodedCsv = base64::to_base64(bufferStr);\n\n    nlohmann::json jsonObj;\n    jsonObj[\"csv\"] = encodedCsv;\n    std::string body = jsonObj.dump();\n\n    auto response = client.Post(\"/time-series-forecasting\", headers, body, \"application/json\");\n    if (response &amp;&amp; response-&gt;status == 200) {\n        nlohmann::json jsonResponse = nlohmann::json::parse(response-&gt;body);\n        auto result = jsonResponse[\"result\"];\n\n        encodedCsv = result[\"csv\"];\n        decodedString = base64::from_base64(encodedCsv);\n        std::vector&lt;unsigned char&gt; decodedCsv(decodedString.begin(), decodedString.end());\n        std::ofstream outputCsv(outputCsvPath, std::ios::binary | std::ios::out);\n        if (outputCsv.is_open()) {\n            outputCsv.write(reinterpret_cast&lt;char*&gt;(decodedCsv.data()), decodedCsv.size());\n            outputCsv.close();\n            std::cout &lt;&lt; \"Output time-series data saved at \" &lt;&lt; outputCsvPath &lt;&lt; std::endl;\n        } else {\n            std::cerr &lt;&lt; \"Unable to open file for writing: \" &lt;&lt; outputCsvPath &lt;&lt; std::endl;\n        }\n    } else {\n        std::cout &lt;&lt; \"Failed to send HTTP request.\" &lt;&lt; std::endl;\n        std::cout &lt;&lt; response-&gt;body &lt;&lt; std::endl;\n        return 1;\n    }\n\n    return 0;\n}\n</code></pre> Java <pre><code>import okhttp3.*;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.node.ObjectNode;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Base64;\n\npublic class Main {\n    public static void main(String[] args) throws IOException {\n        String API_URL = \"http://localhost:8080/time-series-forecasting\";\n        String csvPath = \"./test.csv\";\n        String outputCsvPath = \"./out.csv\";\n\n        File file = new File(csvPath);\n        byte[] fileContent = java.nio.file.Files.readAllBytes(file.toPath());\n        String csvData = Base64.getEncoder().encodeToString(fileContent);\n\n        ObjectMapper objectMapper = new ObjectMapper();\n        ObjectNode params = objectMapper.createObjectNode();\n        params.put(\"csv\", csvData);\n\n        OkHttpClient client = new OkHttpClient();\n        MediaType JSON = MediaType.Companion.get(\"application/json; charset=utf-8\");\n        RequestBody body = RequestBody.Companion.create(params.toString(), JSON);\n        Request request = new Request.Builder()\n                .url(API_URL)\n                .post(body)\n                .build();\n\n        try (Response response = client.newCall(request).execute()) {\n            if (response.isSuccessful()) {\n                String responseBody = response.body().string();\n                JsonNode resultNode = objectMapper.readTree(responseBody);\n                JsonNode result = resultNode.get(\"result\");\n\n                String base64Csv = result.get(\"csv\").asText();\n                byte[] csvBytes = Base64.getDecoder().decode(base64Csv);\n                try (FileOutputStream fos = new FileOutputStream(outputCsvPath)) {\n                    fos.write(csvBytes);\n                }\n                System.out.println(\"Output time-series data saved at \" + outputCsvPath);\n            } else {\n                System.err.println(\"Request failed with code: \" + response.code());\n            }\n        }\n    }\n}\n</code></pre> Go <pre><code>package main\n\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"net/http\"\n)\n\nfunc main() {\n    API_URL := \"http://localhost:8080/time-series-forecasting\"\n    csvPath := \"./test.csv\";\n    outputCsvPath := \"./out.csv\";\n\n    csvBytes, err := ioutil.ReadFile(csvPath)\n    if err != nil {\n        fmt.Println(\"Error reading csv file:\", err)\n        return\n    }\n    csvData := base64.StdEncoding.EncodeToString(csvBytes)\n\n    payload := map[string]string{\"csv\": csvData}\n    payloadBytes, err := json.Marshal(payload)\n    if err != nil {\n        fmt.Println(\"Error marshaling payload:\", err)\n        return\n    }\n\n    client := &amp;http.Client{}\n    req, err := http.NewRequest(\"POST\", API_URL, bytes.NewBuffer(payloadBytes))\n    if err != nil {\n        fmt.Println(\"Error creating request:\", err)\n        return\n    }\n\n    res, err := client.Do(req)\n    if err != nil {\n        fmt.Println(\"Error sending request:\", err)\n        return\n    }\n    defer res.Body.Close()\n\n    body, err := ioutil.ReadAll(res.Body)\n    if err != nil {\n        fmt.Println(\"Error reading response body:\", err)\n        return\n    }\n    type Response struct {\n        Result struct {\n            Csv string `json:\"csv\"`\n        } `json:\"result\"`\n    }\n    var respData Response\n    err = json.Unmarshal([]byte(string(body)), &amp;respData)\n    if err != nil {\n        fmt.Println(\"Error unmarshaling response body:\", err)\n        return\n    }\n\n    outputCsvData, err := base64.StdEncoding.DecodeString(respData.Result.Csv)\n    if err != nil {\n        fmt.Println(\"Error decoding base64 csv data:\", err)\n        return\n    }\n    err = ioutil.WriteFile(outputCsvPath, outputCsvData, 0644)\n    if err != nil {\n        fmt.Println(\"Error writing csv to file:\", err)\n        return\n    }\n    fmt.Printf(\"Output time-series data saved at %s.csv\", outputCsvPath)\n}\n</code></pre> C# <pre><code>using System;\nusing System.IO;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing Newtonsoft.Json.Linq;\n\nclass Program\n{\n    static readonly string API_URL = \"http://localhost:8080/time-series-forecasting\";\n    static readonly string csvPath = \"./test.csv\";\n    static readonly string outputCsvPath = \"./out.csv\";\n\n    static async Task Main(string[] args)\n    {\n        var httpClient = new HttpClient();\n\n        byte[] csvBytes = File.ReadAllBytes(csvPath);\n        string csvData = Convert.ToBase64String(csvBytes);\n\n        var payload = new JObject{ { \"csv\", csvData } };\n        var content = new StringContent(payload.ToString(), Encoding.UTF8, \"application/json\");\n\n        HttpResponseMessage response = await httpClient.PostAsync(API_URL, content);\n        response.EnsureSuccessStatusCode();\n\n        string responseBody = await response.Content.ReadAsStringAsync();\n        JObject jsonResponse = JObject.Parse(responseBody);\n\n        string base64Csv = jsonResponse[\"result\"][\"csv\"].ToString();\n        byte[] outputCsvBytes = Convert.FromBase64String(base64Csv);\n        File.WriteAllBytes(outputCsvPath, outputCsvBytes);\n        Console.WriteLine($\"Output time-series data saved at {outputCsvPath}\");\n    }\n}\n</code></pre> Node.js <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nconst API_URL = 'http://localhost:8080/time-series-forecasting'\nconst csvPath = \"./test.csv\";\nconst outputCsvPath = \"./out.csv\";\n\nlet config = {\n   method: 'POST',\n   maxBodyLength: Infinity,\n   url: API_URL,\n   data: JSON.stringify({\n    'csv': encodeFileToBase64(csvPath)\n  })\n};\n\nfunction encodeFileToBase64(filePath) {\n  const bitmap = fs.readFileSync(filePath);\n  return Buffer.from(bitmap).toString('base64');\n}\n\naxios.request(config)\n.then((response) =&gt; {\n    const result = response.data[\"result\"];\n\n    const csvBuffer = Buffer.from(result[\"csv\"], 'base64');\n    fs.writeFile(outputCsvPath, csvBuffer, (err) =&gt; {\n      if (err) throw err;\n      console.log(`Output time-series data saved at ${outputCsvPath}`);\n    });\n})\n.catch((error) =&gt; {\n  console.log(error);\n});\n</code></pre> PHP <pre><code>&lt;?php\n\n$API_URL = \"http://localhost:8080/time-series-forecasting\";\n$csv_path = \"./test.csv\";\n$output_csv_path = \"./out.csv\";\n\n$csv_data = base64_encode(file_get_contents($csv_path));\n$payload = array(\"csv\" =&gt; $csv_data);\n\n$ch = curl_init($API_URL);\ncurl_setopt($ch, CURLOPT_POST, true);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($payload));\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application/json'));\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$result = json_decode($response, true)[\"result\"];\n\nfile_put_contents($output_csv_path, base64_decode($result[\"csv\"]));\necho \"Output time-series data saved at \" . $output_csv_path . \"\\n\";\n\n?&gt;\n</code></pre> <p></p> <p>\ud83d\udcf1 Edge Deployment: Edge deployment is a method that places computing and data processing functions on user devices themselves, enabling devices to directly process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, refer to the PaddleX Edge Deployment Guide. Choose the appropriate deployment method for your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#4-custom-development","title":"4. Custom Development","text":"<p>If the default model weights provided by the General Time Series Forecasting Pipeline do not meet your requirements in terms of accuracy or speed in your specific scenario, you can try to further fine-tune the existing model using your own domain-specific or application-specific data to improve the recognition performance of the pipeline in your scenario.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#41-model-fine-tuning","title":"4.1 Model Fine-tuning","text":"<p>Since the General Time Series Forecasting Pipeline includes a time series forecasting module, if the performance of the pipeline does not meet expectations, you need to refer to the Customization section in the Time Series Forecasting Module Development Tutorial and use your private dataset to fine-tune the time series forecasting model.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#42-model-application","title":"4.2 Model Application","text":"<p>After fine-tuning with your private dataset, you will obtain local model weight files.</p> <p>To use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local path of the fine-tuned model weights to the corresponding location in the pipeline configuration file:</p> <p><pre><code>......\nPipeline:\n  model: DLinear  # Replace with the local path of the fine-tuned model\n  device: \"gpu\"\n  batch_size: 0\n......\n</code></pre> Then, refer to the command line or Python script methods in the local experience section to load the modified pipeline configuration file.</p>"},{"location":"en/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html#5-multi-hardware-support","title":"5. Multi-hardware Support","text":"<p>PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. Simply modify the <code>--device</code> parameter to seamlessly switch between different hardware.</p> <p>For example, if you use an NVIDIA GPU for inference with the time series forecasting pipeline, the Python command would be:</p> <p><pre><code>paddlex --pipeline ts_fc --input ts_fc.csv --device gpu:0\n``````\nAt this point, if you wish to switch the hardware to Ascend NPU, simply modify the `--device` in the Python command to `npu:0`:\n\n```bash\npaddlex --pipeline ts_fc --input ts_fc.csv --device npu:0\n</code></pre> If you want to use the General Time Series Forecasting Pipeline on a wider range of hardware, please refer to the PaddleX Multi-Device Usage Guide.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html","title":"Anomaly detection tutorial","text":"<p>\u200b\u7b80\u4f53\u4e2d\u6587\u200b | English</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#paddlex-30","title":"PaddleX 3.0 \u200b\u56fe\u50cf\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u4ea7\u7ebf\u200b\u2014\u2014\u2014\u200b\u98df\u54c1\u200b\u5916\u89c2\u200b\u8d28\u68c0\u200b\u6559\u7a0b","text":"<p>PaddleX \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e30\u5bcc\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\uff0c\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\u7531\u200b\u4e00\u4e2a\u200b\u6216\u200b\u591a\u4e2a\u200b\u6a21\u578b\u200b\u7ec4\u5408\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\u90fd\u200b\u80fd\u591f\u200b\u89e3\u51b3\u200b\u7279\u5b9a\u200b\u7684\u200b\u573a\u666f\u200b\u4efb\u52a1\u200b\u95ee\u9898\u200b\u3002PaddleX \u200b\u6240\u200b\u63d0\u4f9b\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\u5747\u200b\u652f\u6301\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\uff0c\u200b\u5982\u679c\u200b\u6548\u679c\u200b\u4e0d\u53ca\u200b\u9884\u671f\u200b\uff0c\u200b\u4e5f\u200b\u540c\u6837\u200b\u652f\u6301\u200b\u4f7f\u7528\u200b\u79c1\u6709\u200b\u6570\u636e\u200b\u5fae\u8c03\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b PaddleX \u200b\u63d0\u4f9b\u200b\u4e86\u200b Python API\uff0c\u200b\u65b9\u4fbf\u200b\u5c06\u4ea7\u7ebf\u200b\u96c6\u6210\u200b\u5230\u200b\u4e2a\u4eba\u200b\u9879\u76ee\u200b\u4e2d\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\uff0c\u200b\u60a8\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b PaddleX\uff0c \u200b\u5b89\u88c5\u200b\u65b9\u5f0f\u200b\u8bf7\u200b\u53c2\u8003\u200b PaddleX \u200b\u5b89\u88c5\u200b\u3002\u200b\u6b64\u5904\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\u98df\u54c1\u200b\u5916\u89c2\u200b\u8d28\u68c0\u200b\u7684\u200b\u4efb\u52a1\u200b\u4e3a\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u4ecb\u7ecd\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\u5de5\u5177\u200b\u7684\u200b\u4f7f\u7528\u200b\u6d41\u7a0b\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#1","title":"1. \u200b\u9009\u62e9\u200b\u4ea7\u7ebf","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u60a8\u200b\u7684\u200b\u4efb\u52a1\u200b\u573a\u666f\u200b\uff0c\u200b\u9009\u62e9\u200b\u5bf9\u5e94\u200b\u7684\u200b PaddleX \u200b\u4ea7\u7ebf\u200b\uff0c\u200b\u6b64\u5904\u200b\u4e3a\u200b\u98df\u54c1\u200b\u5916\u89c2\u200b\u8d28\u68c0\u200b\uff0c\u200b\u9700\u8981\u200b\u4e86\u89e3\u200b\u5230\u200b\u8fd9\u4e2a\u200b\u4efb\u52a1\u200b\u5c5e\u4e8e\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u4efb\u52a1\u200b\uff0c\u200b\u5bf9\u5e94\u200b PaddleX \u200b\u7684\u200b\u901a\u7528\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u4ea7\u7ebf\u200b\u3002\u200b\u5982\u679c\u200b\u65e0\u6cd5\u200b\u786e\u5b9a\u200b\u4efb\u52a1\u200b\u548c\u200b\u4ea7\u7ebf\u200b\u7684\u200b\u5bf9\u5e94\u200b\u5173\u7cfb\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b PaddleX \u200b\u652f\u6301\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\u5217\u8868\u200b\u4e2d\u200b\u4e86\u89e3\u200b\u76f8\u5173\u200b\u4ea7\u7ebf\u200b\u7684\u200b\u80fd\u529b\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#2","title":"2. \u200b\u5feb\u901f\u200b\u4f53\u9a8c","text":"<p>PaddleX \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b PaddleX wheel \u200b\u5305\u200b\u5728\u200b\u672c\u5730\u200b\u4f53\u9a8c\u200b\u3002</p> <ul> <li>\u200b\u672c\u5730\u200b\u4f53\u9a8c\u200b\u65b9\u5f0f\u200b\uff1a     <pre><code>paddlex --pipeline anomaly_detection \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png \\\n    --save_path output\n</code></pre></li> </ul> <p>\u200b\u5feb\u901f\u200b\u4f53\u9a8c\u200b\u4ea7\u51fa\u200b\u63a8\u7406\u200b\u7ed3\u679c\u200b\u793a\u4f8b\u200b\uff1a    <p></p> <p></p> <p>\u200b\u5f53\u200b\u4f53\u9a8c\u200b\u5b8c\u8be5\u200b\u4ea7\u7ebf\u200b\u4e4b\u540e\u200b\uff0c\u200b\u9700\u8981\u200b\u786e\u5b9a\u200b\u4ea7\u7ebf\u200b\u662f\u5426\u200b\u7b26\u5408\u200b\u9884\u671f\u200b\uff08\u200b\u5305\u542b\u200b\u7cbe\u5ea6\u200b\u3001\u200b\u901f\u5ea6\u200b\u7b49\u200b\uff09\uff0c\u200b\u4ea7\u7ebf\u200b\u5305\u542b\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u7ee7\u7eed\u200b\u5fae\u8c03\u200b\uff0c\u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u7684\u200b\u901f\u5ea6\u200b\u6216\u8005\u200b\u7cbe\u5ea6\u200b\u4e0d\u200b\u7b26\u5408\u200b\u9884\u671f\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u6a21\u578b\u200b\u9009\u62e9\u200b\u9009\u62e9\u200b\u53ef\u200b\u66ff\u6362\u200b\u7684\u200b\u6a21\u578b\u200b\u7ee7\u7eed\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u786e\u5b9a\u200b\u6548\u679c\u200b\u662f\u5426\u200b\u6ee1\u610f\u200b\u3002\u200b\u5982\u679c\u200b\u6700\u7ec8\u200b\u6548\u679c\u200b\u5747\u200b\u4e0d\u200b\u6ee1\u610f\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u5fae\u8c03\u200b\u6a21\u578b\u200b\u3002\u200b\u672c\u200b\u6559\u7a0b\u200b\u5e0c\u671b\u200b\u4ea7\u51fa\u200b\u68c0\u6d4b\u200b\u51fa\u200b\u98df\u54c1\u200b\u699b\u5b50\u200b\u5916\u89c2\u200b\u5f02\u5e38\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u663e\u7136\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u6743\u91cd\u200b\uff08\u200b\u94c1\u4e1d\u7f51\u200b \u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u4ea7\u51fa\u200b\u7684\u200b\u6743\u91cd\u200b\uff09\u200b\u65e0\u6cd5\u200b\u6ee1\u8db3\u8981\u6c42\u200b\uff0c\u200b\u9700\u8981\u200b\u91c7\u96c6\u200b\u548c\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u5fae\u8c03\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#3","title":"3. \u200b\u9009\u62e9\u200b\u6a21\u578b","text":"<p>PaddleX \u200b\u63d0\u4f9b\u200b\u4e86\u200b 1 \u200b\u4e2a\u7aef\u200b\u5230\u200b\u7aef\u7684\u200b\u9ad8\u7cbe\u5ea6\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c\u200b\u5177\u4f53\u200b\u53ef\u200b\u53c2\u8003\u200b \u200b\u6a21\u578b\u200b\u5217\u8868\u200b\uff0c\u200b\u5176\u4e2d\u200b\u90e8\u5206\u200b\u6a21\u578b\u200b\u7684\u200bbenchmark\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u6a21\u578b\u200b\u540d\u79f0\u200b Avg\uff08%\uff09 GPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09 CPU\u200b\u63a8\u7406\u200b\u8017\u65f6\u200b\uff08ms\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b yaml \u200b\u6587\u4ef6\u200b STFPM 96.2 - - 21.5 M STFPM.yaml <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b **MVTec AD \u200b\u9a8c\u8bc1\u200b\u96c6\u200b \u200b\u5e73\u5747\u200b\u5f02\u5e38\u200b\u5206\u6570\u200b\u3002**</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#4","title":"4. \u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u548c\u6821\u9a8c","text":""},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#41","title":"4.1 \u200b\u6570\u636e\u200b\u51c6\u5907","text":"<p>\u200b\u672c\u200b\u6559\u7a0b\u200b\u91c7\u7528\u200b <code>\u200b\u98df\u54c1\u200b\u5916\u89c2\u200b\u8d28\u68c0\u200b\u6570\u636e\u200b\u96c6\u200b</code> \u200b\u4f5c\u4e3a\u200b\u793a\u4f8b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u53ef\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u83b7\u53d6\u200b\u793a\u4f8b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u4f7f\u7528\u200b\u81ea\u5907\u200b\u7684\u200b\u5df2\u200b\u6807\u6ce8\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u9700\u8981\u200b\u6309\u7167\u200b PaddleX \u200b\u7684\u200b\u683c\u5f0f\u200b\u8981\u6c42\u200b\u5bf9\u200b\u81ea\u5907\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\uff0c\u200b\u4ee5\u200b\u6ee1\u8db3\u200b PaddleX \u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\u8981\u6c42\u200b\u3002\u200b\u5173\u4e8e\u200b\u6570\u636e\u683c\u5f0f\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b PaddleX \u200b\u5206\u5272\u200b\u4efb\u52a1\u200b\u6a21\u5757\u200b\u6570\u636e\u200b\u6807\u6ce8\u200b\u6559\u7a0b\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u83b7\u53d6\u200b\u547d\u4ee4\u200b\uff1a <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/anomaly_detection_hazelnut.tar -P ./dataset\ntar -xf ./dataset/anomaly_detection_hazelnut.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#42","title":"4.2 \u200b\u6570\u636e\u200b\u96c6\u200b\u6821\u9a8c","text":"<p>\u200b\u5728\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u6821\u9a8c\u200b\u65f6\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u4e00\u884c\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut\n</code></pre> <p>\u200b\u6267\u884c\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u540e\u200b\uff0cPaddleX \u200b\u4f1a\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u6821\u9a8c\u200b\uff0c\u200b\u5e76\u200b\u7edf\u8ba1\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u57fa\u672c\u200b\u4fe1\u606f\u200b\u3002\u200b\u547d\u4ee4\u200b\u8fd0\u884c\u200b\u6210\u529f\u200b\u540e\u4f1a\u200b\u5728\u200b log \u200b\u4e2d\u200b\u6253\u5370\u200b\u51fa\u200b <code>Check dataset passed !</code> \u200b\u4fe1\u606f\u200b\uff0c\u200b\u540c\u65f6\u200b\u76f8\u5173\u200b\u4ea7\u51fa\u200b\u4f1a\u200b\u4fdd\u5b58\u200b\u5728\u200b\u5f53\u524d\u76ee\u5f55\u200b\u7684\u200b <code>./output/check_dataset</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b\uff0c\u200b\u4ea7\u51fa\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u5305\u62ec\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u793a\u4f8b\u200b\u6837\u672c\u200b\u56fe\u7247\u200b\u548c\u200b\u6837\u672c\u5206\u5e03\u200b\u76f4\u65b9\u56fe\u200b\u3002\u200b\u6821\u9a8c\u200b\u7ed3\u679c\u200b\u6587\u4ef6\u200b\u4fdd\u5b58\u200b\u5728\u200b <code>./output/check_dataset_result.json</code>\uff0c\u200b\u6821\u9a8c\u200b\u7ed3\u679c\u200b\u6587\u4ef6\u200b\u5177\u4f53\u5185\u5bb9\u200b\u4e3a\u200b <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_sample_paths\": [\n      \"check_dataset\\/demo_img\\/294.png\",\n      \"check_dataset\\/demo_img\\/260.png\",\n      \"check_dataset\\/demo_img\\/297.png\",\n      \"check_dataset\\/demo_img\\/170.png\",\n      \"check_dataset\\/demo_img\\/068.png\",\n      \"check_dataset\\/demo_img\\/212.png\",\n      \"check_dataset\\/demo_img\\/204.png\",\n      \"check_dataset\\/demo_img\\/233.png\",\n      \"check_dataset\\/demo_img\\/367.png\",\n      \"check_dataset\\/demo_img\\/383.png\"\n    ],\n    \"train_samples\": 391,\n    \"val_sample_paths\": [\n      \"check_dataset\\/demo_img\\/012.png\",\n      \"check_dataset\\/demo_img\\/017.png\",\n      \"check_dataset\\/demo_img\\/006.png\",\n      \"check_dataset\\/demo_img\\/013.png\",\n      \"check_dataset\\/demo_img\\/014.png\",\n      \"check_dataset\\/demo_img\\/010.png\",\n      \"check_dataset\\/demo_img\\/007.png\",\n      \"check_dataset\\/demo_img\\/001.png\",\n      \"check_dataset\\/demo_img\\/002.png\",\n      \"check_dataset\\/demo_img\\/009.png\"\n    ],\n    \"val_samples\": 70,\n    \"num_classes\": 1\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/dataset\\/hazelnut\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"SegDataset\"\n}\n</code></pre> \u200b\u4e0a\u8ff0\u200b\u6821\u9a8c\u200b\u7ed3\u679c\u200b\u4e2d\u200b\uff0ccheck_pass \u200b\u4e3a\u200b True \u200b\u8868\u793a\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u7b26\u5408\u8981\u6c42\u200b\uff0c\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\u6307\u6807\u200b\u7684\u200b\u8bf4\u660e\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>attributes.num_classes\uff1a\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u7c7b\u522b\u200b\u6570\u4e3a\u200b 1\uff0c\u200b\u6b64\u5904\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u4e3a\u200b\u540e\u7eed\u200b\u8bad\u7ec3\u200b\u9700\u8981\u200b\u4f20\u5165\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\uff1b</li> <li>attributes.train_samples\uff1a\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\u4e3a\u200b 391\uff1b</li> <li>attributes.val_samples\uff1a\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\u4e3a\u200b 70\uff1b</li> <li>attributes.train_sample_paths\uff1a\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6837\u672c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b\u76f8\u5bf9\u8def\u5f84\u200b\u5217\u8868\u200b\uff1b</li> <li>attributes.val_sample_paths\uff1a\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6837\u672c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u7247\u200b\u76f8\u5bf9\u8def\u5f84\u200b\u5217\u8868\u200b\uff1b</li> </ul> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u6821\u9a8c\u200b\u8fd8\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\u7684\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\u5206\u5e03\u200b\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5206\u6790\u200b\uff0c\u200b\u5e76\u200b\u7ed8\u5236\u200b\u4e86\u200b\u5206\u5e03\u200b\u76f4\u65b9\u56fe\u200b\uff08histogram.png\uff09\uff1a  <p></p> <p></p> <p>\u200b\u6ce8\u200b\uff1a\u200b\u53ea\u6709\u200b\u901a\u8fc7\u200b\u6570\u636e\u200b\u6821\u9a8c\u200b\u7684\u200b\u6570\u636e\u200b\u624d\u200b\u53ef\u4ee5\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#43","title":"4.3 \u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8f6c\u6362\u200b/\u200b\u6570\u636e\u200b\u96c6\u200b\u5212\u5206\u200b\uff08\u200b\u975e\u200b\u5fc5\u9009\u200b\uff09","text":"<p>\u200b\u5982\u9700\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\u6216\u662f\u200b\u91cd\u65b0\u200b\u5212\u5206\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u53ef\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u6216\u662f\u200b\u8ffd\u52a0\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u6821\u9a8c\u200b\u76f8\u5173\u200b\u7684\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b <code>CheckDataset</code> \u200b\u4e0b\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u90e8\u5206\u200b\u53c2\u6570\u200b\u7684\u200b\u793a\u4f8b\u200b\u8bf4\u660e\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: \u200b\u662f\u5426\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8f6c\u6362\u200b\uff0c\u200b\u4e3a\u200b <code>True</code> \u200b\u65f6\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8f6c\u6362\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b <code>False</code>;</li> <li><code>src_dataset_type</code>: \u200b\u5982\u679c\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8f6c\u6362\u200b\uff0c\u200b\u5219\u200b\u9700\u200b\u8bbe\u7f6e\u200b\u6e90\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\uff0c\u200b\u6570\u636e\u200b\u53ef\u9009\u6e90\u200b\u683c\u5f0f\u200b\u4e3a\u200b <code>LabelMe</code> \u200b\u548c\u200b <code>VOC</code>\uff1b</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: \u200b\u662f\u5426\u200b\u8fdb\u884c\u200b\u91cd\u65b0\u200b\u5212\u5206\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e3a\u200b <code>True</code> \u200b\u65f6\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u96c6\u200b\u683c\u5f0f\u200b\u8f6c\u6362\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b <code>False</code>\uff1b</li> <li><code>train_percent</code>: \u200b\u5982\u679c\u200b\u91cd\u65b0\u200b\u5212\u5206\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u767e\u5206\u6bd4\u200b\uff0c\u200b\u7c7b\u578b\u200b\u4e3a\u200b 0-100 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u4efb\u610f\u200b\u6574\u6570\u200b\uff0c\u200b\u9700\u8981\u200b\u4fdd\u8bc1\u200b\u548c\u200b <code>val_percent</code> \u200b\u503c\u52a0\u200b\u548c\u200b\u4e3a\u200b 100\uff1b</li> <li><code>val_percent</code>: \u200b\u5982\u679c\u200b\u91cd\u65b0\u200b\u5212\u5206\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5219\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u767e\u5206\u6bd4\u200b\uff0c\u200b\u7c7b\u578b\u200b\u4e3a\u200b 0-100 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u4efb\u610f\u200b\u6574\u6570\u200b\uff0c\u200b\u9700\u8981\u200b\u4fdd\u8bc1\u200b\u548c\u200b <code>train_percent</code> \u200b\u503c\u52a0\u200b\u548c\u200b\u4e3a\u200b 100\uff1b</li> </ul> </li> </ul> </li> </ul> <p>\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u548c\u200b\u6570\u636e\u200b\u5212\u5206\u200b\u652f\u6301\u200b\u540c\u65f6\u200b\u5f00\u542f\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6570\u636e\u200b\u5212\u5206\u200b\u539f\u6709\u200b\u6807\u6ce8\u200b\u6587\u4ef6\u200b\u4f1a\u200b\u88ab\u200b\u5728\u200b\u539f\u200b\u8def\u5f84\u200b\u4e0b\u200b\u91cd\u547d\u540d\u200b\u4e3a\u200b <code>xxx.bak</code>\uff0c\u200b\u4ee5\u4e0a\u200b\u53c2\u6570\u200b\u540c\u6837\u200b\u652f\u6301\u200b\u901a\u8fc7\u200b\u8ffd\u52a0\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4f8b\u5982\u200b\u91cd\u65b0\u200b\u5212\u5206\u200b\u6570\u636e\u200b\u96c6\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u4e0e\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6bd4\u4f8b\u200b\uff1a<code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#5","title":"5. \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30","text":""},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#51","title":"5.1 \u200b\u6a21\u578b\u200b\u8bad\u7ec3","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u5df2\u7ecf\u200b\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6821\u9a8c\u200b\u3002\u200b\u5b8c\u6210\u200b PaddleX \u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u5982\u4e0b\u200b\u4e00\u6761\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut \\\n    -o Train.epochs_iters=4000\n</code></pre> <p>\u200b\u5728\u200b PaddleX \u200b\u4e2d\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u652f\u6301\u200b\uff1a\u200b\u4fee\u6539\u200b\u8bad\u7ec3\u200b\u8d85\u200b\u53c2\u6570\u200b\u3001\u200b\u5355\u673a\u200b\u5355\u5361\u200b/\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u7b49\u200b\u529f\u80fd\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u6216\u200b\u8ffd\u52a0\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u3002</p> <p>PaddleX \u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u90fd\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6a21\u578b\u200b\u5f00\u53d1\u200b\u7684\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8bbe\u7f6e\u200b\u76f8\u5173\u200b\u53c2\u6570\u200b\u3002\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u76f8\u5173\u200b\u7684\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b <code>Train</code> \u200b\u4e0b\u200b\u7684\u200b\u5b57\u200b\u6bb5\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u90e8\u5206\u200b\u53c2\u6570\u200b\u7684\u200b\u793a\u4f8b\u200b\u8bf4\u660e\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>Global</code>\uff1a<ul> <li><code>mode</code>\uff1a\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u652f\u6301\u200b\u6570\u636e\u200b\u6821\u9a8c\u200b\uff08<code>check_dataset</code>\uff09\u3001\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff08<code>train</code>\uff09\u3001\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\uff08<code>evaluate</code>\uff09\uff1b</li> <li><code>device</code>\uff1a\u200b\u8bad\u7ec3\u200b\u8bbe\u5907\u200b\uff0c\u200b\u53ef\u200b\u9009\u200b<code>cpu</code>\u3001<code>gpu</code>\u3001<code>xpu</code>\u3001<code>npu</code>\u3001<code>mlu</code>\uff0c\u200b\u9664\u200b cpu \u200b\u5916\u200b\uff0c\u200b\u591a\u5361\u200b\u8bad\u7ec3\u200b\u53ef\u200b\u6307\u5b9a\u200b\u5361\u53f7\u200b\uff0c\u200b\u5982\u200b\uff1a<code>gpu:0,1,2,3</code>\uff1b</li> </ul> </li> <li><code>Train</code>\uff1a\u200b\u8bad\u7ec3\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\uff1b<ul> <li><code>epochs_iters</code>\uff1a\u200b\u8bad\u7ec3\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\u6570\u200b\u8bbe\u7f6e\u200b\uff1b</li> <li><code>learning_rate</code>\uff1a\u200b\u8bad\u7ec3\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\uff1b</li> </ul> </li> </ul> <p>\u200b\u66f4\u591a\u8d85\u200b\u53c2\u6570\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u8003\u200b PaddleX \u200b\u901a\u7528\u200b\u6a21\u578b\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u53c2\u6570\u200b\u8bf4\u660e\u200b\u3002</p> <p>\u200b\u6ce8\u200b\uff1a - \u200b\u4ee5\u4e0a\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8ffd\u52a0\u200b\u4ee4\u884c\u200b\u53c2\u6570\u200b\u7684\u200b\u5f62\u5f0f\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5982\u200b\u6307\u5b9a\u200b\u6a21\u5f0f\u200b\u4e3a\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff1a<code>-o Global.mode=train</code>\uff1b\u200b\u6307\u5b9a\u200b\u524d\u200b 2 \u200b\u5361\u200b gpu \u200b\u8bad\u7ec3\u200b\uff1a<code>-o Global.device=gpu:0,1</code>\uff1b\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\u4e3a\u200b 5000\uff1a<code>-o Train.epochs_iters=5000</code>\u3002 - \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0cPaddleX \u200b\u4f1a\u200b\u81ea\u52a8\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u6587\u4ef6\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b<code>output</code>\uff0c\u200b\u5982\u9700\u200b\u6307\u5b9a\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\uff0c\u200b\u53ef\u200b\u901a\u8fc7\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b <code>-o Global.output</code> \u200b\u5b57\u200b\u6bb5\u200b - PaddleX \u200b\u5bf9\u200b\u60a8\u200b\u5c4f\u853d\u200b\u4e86\u200b\u52a8\u6001\u56fe\u200b\u6743\u91cd\u200b\u548c\u200b\u9759\u6001\u200b\u56fe\u200b\u6743\u91cd\u200b\u7684\u200b\u6982\u5ff5\u200b\u3002\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f1a\u200b\u540c\u65f6\u200b\u4ea7\u51fa\u200b\u52a8\u6001\u56fe\u200b\u548c\u200b\u9759\u6001\u200b\u56fe\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u5728\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\u65f6\u200b\uff0c\u200b\u9ed8\u8ba4\u200b\u9009\u62e9\u200b\u9759\u6001\u200b\u56fe\u200b\u6743\u91cd\u200b\u63a8\u7406\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u4ea7\u51fa\u200b\u89e3\u91ca\u200b:</p> <p>\u200b\u5728\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u6240\u6709\u200b\u4ea7\u51fa\u200b\u4fdd\u5b58\u200b\u5728\u200b\u6307\u5b9a\u200b\u7684\u200b\u8f93\u51fa\u200b\u76ee\u5f55\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b<code>./output/</code>\uff09\u200b\u4e0b\u200b\uff0c\u200b\u901a\u5e38\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u4ea7\u51fa\u200b\uff1a</p> <ul> <li>train_result.json\uff1a\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u8bb0\u5f55\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bb0\u5f55\u200b\u4e86\u200b\u8bad\u7ec3\u4efb\u52a1\u200b\u662f\u5426\u200b\u6b63\u5e38\u200b\u5b8c\u6210\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4ea7\u51fa\u200b\u7684\u200b\u6743\u91cd\u200b\u6307\u6807\u200b\u3001\u200b\u76f8\u5173\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u7b49\u200b\uff1b</li> <li>train.log\uff1a\u200b\u8bad\u7ec3\u200b\u65e5\u5fd7\u200b\u6587\u4ef6\u200b\uff0c\u200b\u8bb0\u5f55\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\u6307\u6807\u200b\u53d8\u5316\u200b\u3001loss \u200b\u53d8\u5316\u200b\u7b49\u200b\uff1b</li> <li>config.yaml\uff1a\u200b\u8bad\u7ec3\u200b\u914d\u7f6e\u6587\u4ef6\u200b\uff0c\u200b\u8bb0\u5f55\u200b\u4e86\u200b\u672c\u6b21\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u914d\u7f6e\u200b\uff1b</li> <li>.pdparams\u3001.pdema\u3001.pdopt.pdstate\u3001.pdiparams\u3001.pdmodel\uff1a\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u76f8\u5173\u200b\u6587\u4ef6\u200b\uff0c\u200b\u5305\u62ec\u200b\u7f51\u7edc\u200b\u53c2\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u3001EMA\u3001\u200b\u9759\u6001\u200b\u56fe\u200b\u7f51\u7edc\u200b\u53c2\u6570\u200b\u3001\u200b\u9759\u6001\u200b\u56fe\u200b\u7f51\u7edc\u7ed3\u6784\u200b\u7b49\u200b\uff1b</li> </ul>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#52","title":"5.2 \u200b\u6a21\u578b\u200b\u8bc4\u4f30","text":"<p>\u200b\u5728\u200b\u5b8c\u6210\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6307\u5b9a\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u6587\u4ef6\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u6a21\u578b\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u4f7f\u7528\u200b PaddleX \u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u4e00\u884c\u200b\u547d\u4ee4\u200b\uff1a</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut\n</code></pre> <p>\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u652f\u6301\u200b\u4fee\u6539\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u6216\u200b\u8ffd\u52a0\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u6ce8\u200b\uff1a \u200b\u5728\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u65f6\u200b\uff0c\u200b\u9700\u8981\u200b\u6307\u5b9a\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u90fd\u200b\u5185\u7f6e\u200b\u4e86\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u6743\u91cd\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\uff0c\u200b\u5982\u200b\u9700\u8981\u200b\u6539\u53d8\u200b\uff0c\u200b\u53ea\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u8ffd\u52a0\u200b\u547d\u4ee4\u884c\u200b\u53c2\u6570\u200b\u7684\u200b\u5f62\u5f0f\u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\u5373\u53ef\u200b\uff0c\u200b\u5982\u200b<code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#53","title":"5.3 \u200b\u6a21\u578b\u200b\u8c03\u4f18","text":"<p>\u200b\u5728\u200b\u5b66\u4e60\u200b\u4e86\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u6765\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u901a\u8fc7\u200b\u5408\u7406\u200b\u8c03\u6574\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u63a7\u5236\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6df1\u5ea6\u200b\uff0c\u200b\u907f\u514d\u200b\u8fc7\u200b\u62df\u5408\u200b\u6216\u200b\u6b20\u200b\u62df\u5408\u200b\uff1b\u200b\u800c\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u5219\u200b\u5173\u4e4e\u200b\u6a21\u578b\u200b\u6536\u655b\u200b\u7684\u200b\u901f\u5ea6\u200b\u548c\u200b\u7a33\u5b9a\u6027\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u4f18\u5316\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u65f6\u200b\uff0c\u200b\u52a1\u5fc5\u200b\u5ba1\u614e\u8003\u8651\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u53d6\u503c\u200b\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u5b9e\u9645\u200b\u60c5\u51b5\u200b\u8fdb\u884c\u200b\u7075\u6d3b\u200b\u8c03\u6574\u200b\uff0c\u200b\u4ee5\u200b\u83b7\u5f97\u6700\u4f73\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u63a8\u8350\u200b\u5728\u200b\u8c03\u8bd5\u200b\u53c2\u6570\u200b\u65f6\u200b\u9075\u5faa\u200b\u63a7\u5236\u53d8\u91cf\u200b\u6cd5\u200b\uff1a</p> <ol> <li>\u200b\u9996\u5148\u200b\u56fa\u5b9a\u200b\u8bad\u7ec3\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\u4e3a\u200b 4000\uff0c\u200b\u6279\u200b\u5927\u5c0f\u200b\u4e3a\u200b 1\u3002</li> <li>\u200b\u57fa\u4e8e\u200b STFPM \u200b\u6a21\u578b\u200b\u542f\u52a8\u200b\u4e09\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u5206\u522b\u200b\u4e3a\u200b\uff1a0.01\uff0c0.1\uff0c0.4\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u5b9e\u9a8c\u200b\u4e09\u200b\u7cbe\u5ea6\u200b\u6700\u9ad8\u200b\u7684\u200b\u914d\u7f6e\u200b\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b 0.4\uff0c\u200b\u5728\u200b\u8be5\u200b\u8bad\u7ec3\u200b\u8d85\u200b\u53c2\u6570\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff0c\u200b\u589e\u52a0\u200b\u8bad\u7ec3\u200b\u8bba\u200b\u6b21\u6570\u200b\u5230\u200b5000\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fbe\u5230\u200b\u66f4\u4f18\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u3002</li> </ol> <p>\u200b\u5b66\u4e60\u200b\u7387\u200b\u63a2\u5bfb\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff1a  \u200b\u5b9e\u9a8c\u200b \u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b \u200b\u5b66\u4e60\u200b\u7387\u200b batch_size \u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b mIoU \u200b\u5b9e\u9a8c\u200b\u4e00\u200b 4000 0.01 1 4\u200b\u5361\u200b 0.9646 \u200b\u5b9e\u9a8c\u200b\u4e8c\u200b 4000 0.1 1 4\u200b\u5361\u200b 0.9707 \u200b\u5b9e\u9a8c\u200b\u4e09\u200b 4000 0.4 1 4\u200b\u5361\u200b 0.9797 <p></p> <p>\u200b\u6539\u53d8\u200b epoch \u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\uff1a  \u200b\u5b9e\u9a8c\u200b \u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b \u200b\u5b66\u4e60\u200b\u7387\u200b batch_size \u200b\u8bad\u7ec3\u200b\u73af\u5883\u200b mIoU \u200b\u5b9e\u9a8c\u200b\u4e09\u200b 4000 0.4 1 4\u200b\u5361\u200b 0.9797 \u200b\u5b9e\u9a8c\u200b\u4e09\u200b\u589e\u5927\u200b\u8bad\u7ec3\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b 5000 0.4 1 4\u200b\u5361\u200b 0.9826 <p>\u200b\u6ce8\u200b\uff1a\u200b\u672c\u200b\u6559\u7a0b\u200b\u4e3a\u200b4\u200b\u5361\u200b\u6559\u7a0b\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u53ea\u6709\u200b1\u200b\u5f20\u200bGPU\uff0c\u200b\u53ef\u200b\u901a\u8fc7\u200b\u8c03\u6574\u200b\u8bad\u7ec3\u200b\u5361\u6570\u200b\u5b8c\u6210\u200b\u672c\u6b21\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4f46\u200b\u6700\u7ec8\u200b\u6307\u6807\u200b\u672a\u5fc5\u200b\u548c\u200b\u4e0a\u8ff0\u200b\u6307\u6807\u200b\u5bf9\u9f50\u200b\uff0c\u200b\u5c5e\u200b\u6b63\u5e38\u200b\u60c5\u51b5\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#6","title":"6. \u200b\u4ea7\u7ebf\u200b\u6d4b\u8bd5","text":"<p>\u200b\u5c06\u4ea7\u7ebf\u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\u66ff\u6362\u200b\u4e3a\u200b\u5fae\u8c03\u200b\u540e\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u5982\u200b\uff1a</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png\"\n</code></pre> <p>\u200b\u901a\u8fc7\u200b\u4e0a\u8ff0\u200b\u53ef\u200b\u5728\u200b<code>./output</code>\u200b\u4e0b\u200b\u751f\u6210\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5176\u4e2d\u200b<code>uad_hazelnut.png</code>\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5982\u4e0b\u200b\uff1a  <p></p> <p></p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial.html#7","title":"7. \u200b\u5f00\u53d1\u200b\u96c6\u6210\u200b/\u200b\u90e8\u7f72","text":"<p>\u200b\u5982\u679c\u200b\u901a\u7528\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u4ea7\u7ebf\u200b\u53ef\u4ee5\u200b\u8fbe\u5230\u200b\u60a8\u200b\u5bf9\u200b\u4ea7\u7ebf\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u548c\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u8981\u6c42\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8fdb\u884c\u200b\u5f00\u53d1\u200b\u96c6\u6210\u200b/\u200b\u90e8\u7f72\u200b\u3002 1. \u200b\u76f4\u63a5\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5e94\u7528\u200b\u5728\u200b\u60a8\u200b\u7684\u200b Python \u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u5982\u4e0b\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b<code>paddlex/pipelines/anomaly_detection.yaml</code>\u200b\u914d\u7f6e\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>Pipeline.model</code>\u200b\u4fee\u6539\u200b\u4e3a\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u8def\u5f84\u200b<code>output/best_model/inference</code>\uff1a <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/anomaly_detection.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png\")\nfor res in output:\n    res.print() # \u200b\u6253\u5370\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u8f93\u51fa\u200b\n    res.save_to_img(\"./output/\") # \u200b\u4fdd\u5b58\u200b\u7ed3\u679c\u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\n    res.save_to_json(\"./output/\") # \u200b\u4fdd\u5b58\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u8f93\u51fa\u200b\n</code></pre> \u200b\u66f4\u200b\u591a\u200b\u53c2\u6570\u200b\u8bf7\u200b\u53c2\u8003\u200b \u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u4ea7\u7ebf\u200b\u4f7f\u7528\u200b\u6559\u7a0b\u200b\u3002</p> <ol> <li> <p>\u200b\u6b64\u5916\u200b\uff0cPaddleX \u200b\u4e5f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5176\u4ed6\u200b\u4e09\u79cd\u200b\u90e8\u7f72\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u8bf4\u660e\u200b\u5982\u4e0b\u200b\uff1a</p> </li> <li> <p>\u200b\u9ad8\u6027\u80fd\u200b\u90e8\u7f72\u200b\uff1a\u200b\u5728\u200b\u5b9e\u9645\u200b\u751f\u4ea7\u200b\u73af\u5883\u200b\u4e2d\u200b\uff0c\u200b\u8bb8\u591a\u200b\u5e94\u7528\u200b\u5bf9\u200b\u90e8\u7f72\u200b\u7b56\u7565\u200b\u7684\u200b\u6027\u80fd\u6307\u6807\u200b\uff08\u200b\u5c24\u5176\u200b\u662f\u200b\u54cd\u5e94\u901f\u5ea6\u200b\uff09\u200b\u6709\u7740\u200b\u8f83\u200b\u4e25\u82db\u200b\u7684\u200b\u6807\u51c6\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u7cfb\u7edf\u200b\u7684\u200b\u9ad8\u6548\u200b\u8fd0\u884c\u200b\u4e0e\u200b\u7528\u6237\u200b\u4f53\u9a8c\u200b\u7684\u200b\u6d41\u7545\u6027\u200b\u3002\u200b\u4e3a\u6b64\u200b\uff0cPaddleX \u200b\u63d0\u4f9b\u200b\u9ad8\u6027\u80fd\u200b\u63a8\u7406\u200b\u63d2\u4ef6\u200b\uff0c\u200b\u65e8\u5728\u200b\u5bf9\u6a21\u578b\u200b\u63a8\u7406\u200b\u53ca\u200b\u524d\u540e\u200b\u5904\u7406\u200b\u8fdb\u884c\u200b\u6df1\u5ea6\u200b\u6027\u80fd\u200b\u4f18\u5316\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u7aef\u5230\u200b\u7aef\u200b\u6d41\u7a0b\u200b\u7684\u200b\u663e\u8457\u200b\u63d0\u901f\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u7684\u200b\u9ad8\u6027\u80fd\u200b\u90e8\u7f72\u200b\u6d41\u7a0b\u200b\u8bf7\u200b\u53c2\u8003\u200b PaddleX \u200b\u9ad8\u6027\u80fd\u200b\u63a8\u7406\u200b\u6307\u5357\u200b\u3002</p> </li> <li>\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\uff1a\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u662f\u200b\u5b9e\u9645\u200b\u751f\u4ea7\u200b\u73af\u5883\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e00\u79cd\u200b\u90e8\u7f72\u200b\u5f62\u5f0f\u200b\u3002\u200b\u901a\u8fc7\u200b\u5c06\u200b\u63a8\u7406\u200b\u529f\u80fd\u200b\u5c01\u88c5\u200b\u4e3a\u200b\u670d\u52a1\u200b\uff0c\u200b\u5ba2\u6237\u7aef\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7f51\u7edc\u200b\u8bf7\u6c42\u200b\u6765\u200b\u8bbf\u95ee\u200b\u8fd9\u4e9b\u200b\u670d\u52a1\u200b\uff0c\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u63a8\u7406\u200b\u7ed3\u679c\u200b\u3002PaddleX \u200b\u652f\u6301\u200b\u7528\u6237\u200b\u4ee5\u200b\u4f4e\u6210\u672c\u200b\u5b9e\u73b0\u200b\u4ea7\u7ebf\u200b\u7684\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u7684\u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u6d41\u7a0b\u200b\u8bf7\u200b\u53c2\u8003\u200b PaddleX \u200b\u670d\u52a1\u5316\u200b\u90e8\u7f72\u200b\u6307\u5357\u200b\u3002</li> <li>\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\uff1a\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\u662f\u200b\u4e00\u79cd\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u548c\u200b\u6570\u636e\u5904\u7406\u200b\u529f\u80fd\u200b\u653e\u5728\u200b\u7528\u6237\u200b\u8bbe\u5907\u200b\u672c\u8eab\u200b\u4e0a\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8bbe\u5907\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5904\u7406\u200b\u6570\u636e\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u4f9d\u8d56\u200b\u8fdc\u7a0b\u200b\u7684\u200b\u670d\u52a1\u5668\u200b\u3002PaddleX \u200b\u652f\u6301\u200b\u5c06\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5728\u200b Android \u200b\u7b49\u200b\u7aef\u4fa7\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u7684\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\u6d41\u7a0b\u200b\u8bf7\u200b\u53c2\u8003\u200b PaddleX\u200b\u7aef\u4fa7\u200b\u90e8\u7f72\u200b\u6307\u5357\u200b\u3002</li> </ol> <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u9009\u62e9\u200b\u5408\u9002\u200b\u7684\u200b\u65b9\u5f0f\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\u4ea7\u7ebf\u200b\uff0c\u200b\u8fdb\u800c\u200b\u8fdb\u884c\u200b\u540e\u7eed\u200b\u7684\u200b AI \u200b\u5e94\u7528\u200b\u96c6\u6210\u200b\u3002</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html","title":"Anomaly detection tutorial en","text":"<p>\u200b\u7b80\u4f53\u4e2d\u6587\u200b | English</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#paddlex-30-image-anomaly-detection-pipeline-food-appearance-quality-inspection-tutorial","title":"PaddleX 3.0 Image Anomaly Detection Pipeline \u2014 Food Appearance Quality Inspection Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, fine-tuning the models with private data is also supported. PaddleX provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of a Food Appearance Quality Inspection task.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For Food Appearance Quality Inspection, this falls under the category of anomaly detection tasks, corresponding to PaddleX's Image Anomaly Detection Pipeline. If you are unsure about the correspondence between tasks and pipelines, you can refer to the Pipeline List for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers 1 ways to experience the pipeline: one is through the PaddleX wheel package locally.</p> <ul> <li>Local Experience:   <pre><code>paddlex --pipeline anomaly_detection \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png \\\n    --save_path output\n</code></pre></li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After experiencing the pipeline, determine if it meets your expectations (including accuracy, speed, etc.). If the model's speed or accuracy does not meet your requirements, you can select alternative models for further testing. If the final results are unsatisfactory, you may need to fine-tune the model. This tutorial aims to produce a model that segments lane lines, and the default weights (trained on the Cityscapes dataset) cannot meet this requirement. Therefore, you need to collect and annotate data for training and fine-tuning.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides 1 end-to-end anomaly detection models. For details, refer to the Model List. Some model benchmarks are as follows:</p> Model List Avg (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) yaml file STFPM 96.2 - - 21.5 M STFPM.yaml <p>Note: The above accuracy metrics are measured on the MVTec AD dataset.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Food Appearance Quality Inspection Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to PaddleX anomaly detection Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/anomaly_detection_hazelnut.tar -P ./dataset\ntar -xf ./dataset/anomaly_detection_hazelnut.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Upon successful execution, the log will print \"Check dataset passed !\" information, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory, including visualized sample images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_sample_paths\": [\n      \"check_dataset\\/demo_img\\/294.png\",\n      \"check_dataset\\/demo_img\\/260.png\",\n      \"check_dataset\\/demo_img\\/297.png\",\n      \"check_dataset\\/demo_img\\/170.png\",\n      \"check_dataset\\/demo_img\\/068.png\",\n      \"check_dataset\\/demo_img\\/212.png\",\n      \"check_dataset\\/demo_img\\/204.png\",\n      \"check_dataset\\/demo_img\\/233.png\",\n      \"check_dataset\\/demo_img\\/367.png\",\n      \"check_dataset\\/demo_img\\/383.png\"\n    ],\n    \"train_samples\": 391,\n    \"val_sample_paths\": [\n      \"check_dataset\\/demo_img\\/012.png\",\n      \"check_dataset\\/demo_img\\/017.png\",\n      \"check_dataset\\/demo_img\\/006.png\",\n      \"check_dataset\\/demo_img\\/013.png\",\n      \"check_dataset\\/demo_img\\/014.png\",\n      \"check_dataset\\/demo_img\\/010.png\",\n      \"check_dataset\\/demo_img\\/007.png\",\n      \"check_dataset\\/demo_img\\/001.png\",\n      \"check_dataset\\/demo_img\\/002.png\",\n      \"check_dataset\\/demo_img\\/009.png\"\n    ],\n    \"val_samples\": 70,\n    \"num_classes\": 1\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/dataset\\/hazelnut\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"SegDataset\"\n}\n</code></pre> <p>In the verification results above, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 1, which is the number of classes that need to be passed for subsequent training;</li> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 391;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 70;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the sample distribution across all classes and plots a histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#43-dataset-format-conversion-dataset-splitting-optional","title":"4.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: Whether to convert the dataset format. Set to <code>True</code> to enable dataset format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, the source dataset format must be set. Available source formats are <code>LabelMe</code> and <code>VOC</code>;</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If dataset splitting is enabled, the percentage of the training set must be set. The type is any integer between 0-100, and the sum with <code>val_percent</code> must be 100;</li> <li><code>val_percent</code>: If dataset splitting is enabled, the percentage of the validation set must be set. The type is any integer between 0-100, and the sum with <code>train_percent</code> must be 100;</li> </ul> </li> </ul> </li> </ul> <p>Data conversion and splitting can be enabled simultaneously. For data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths. These parameters also support being set by appending command-line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated your dataset. To complete the training of a PaddleX model, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut \\\n    -o Train.epochs_iters=4000\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, and more, simply by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development, which is used to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supports dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training iterations;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training iterations to 5000: <code>-o Train.epochs_iters=5000</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Training Outputs Explanation:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task completed normally, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, recording changes in model metrics, loss, etc. during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdema, .pdopt.pdstate, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/anomaly_detection_hazelnut\n</code></pre> <p>Similar to model training, model evaluation supports setting parameters by modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating a model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the method of controlled variables when debugging parameters:</p> <ol> <li>First, fix the number of training iterations at 4000 and the batch size at 1.</li> <li>Initiate three experiments based on the STFPM model, with learning rates of: 0.01, 0.1, 0.4.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 3 is a learning rate of 0.4. Based on this training hyperparameter, change the number of training epochs and observe the accuracy results of different iterations, finding that the optimal accuracy is basically achieved at 5000 iterations.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Iterations Learning Rate batch_size Training Environment mIoU Experiment 1 4000 0.01 1 4 0.9646 Experiment 2 4000 0.1 1 4 0.9707 Experiment 3 4000 0.4 1 4 0.9797 <p></p> <p>Changing Epoch Results:  Experiment Iterations Learning Rate batch_size Training Environment mIoU Experiment 3 4000 0.4 1 4 0.9797 Experiment 3 with more epochs 5000 0.4 1 4 0.9826 <p></p> <p>Note: This tutorial is designed for 4 GPUs. If you have only 1 GPU, you can adjust the number of training GPUs to complete the experiment, but the final metrics may not align with the above indicators, which is normal.</p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, for example:</p> <pre><code>python main.py -c paddlex/configs/anomaly_detection/STFPM.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, where the prediction result for <code>uad_hazelnut.png</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/anomaly_detection_tutorial_en.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the anomaly detection pipeline meets your requirements for inference speed and accuracy in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/anomaly_detection.yaml</code> configuration file to your own model path <code>output/best_model/inference</code>: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/anomaly_detection.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/uad_hazelnut.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to Anomaly Detection Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/deployment_tutorial.html","title":"PaddleX 3.0 Pipeline Deployment Tutorial","text":"<p>Before using this tutorial, you need to install PaddleX. For installation instructions, please refer to the PaddleX Installation guide.</p> <p>The three deployment methods of PaddleX are detailed below:</p> <ul> <li>High-Performance Inference: In actual production environments, many applications have stringent performance standards for deployment strategies, especially in terms of response speed, to ensure efficient system operation and smooth user experience. To this end, PaddleX provides a high-performance inference plugin aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end speedups. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving low-cost service-oriented deployment in pipelines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method where computing and data processing functions are placed on the user's device itself, allowing the device to directly process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ul> <p>This tutorial will introduce the three deployment methods of PaddleX through three practical application examples.</p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#1-high-performance-inference-example","title":"1 High-Performance Inference Example","text":""},{"location":"en/practical_tutorials/deployment_tutorial.html#11-obtain-serial-number-and-activation","title":"1.1 Obtain Serial Number and Activation","text":"<p>On the Baidu AIStudio Community - AI Learning and Training Community page, select \"Get Now\" in the \"Consultation and Acquisition of Serial Numbers for Open Source Pipeline Deployment\" section, as shown in the figure below:</p> <p></p> <p>Select the pipeline you need to deploy and click \"Get\". Afterward, you can find the obtained serial number in the \"Serial Number Management for Open Source Pipeline Deployment SDK\" section below:</p> <p></p> <p>After using the serial number to complete activation, you can use the high-performance inference plugin. PaddleX provides both offline and online activation methods (both only support Linux systems):</p> <ul> <li>Online Activation: When using the inference API or CLI, specify the serial number and activate online, allowing the program to complete activation automatically.</li> <li>Offline Activation: Follow the instructions on the serial number management interface (click \"Offline Activation\" in \"Operations\") to obtain the device fingerprint of the machine and bind the serial number with the device fingerprint to obtain a certificate for activation. To use this activation method, you need to manually store the certificate in the <code>${HOME}/.baidu/paddlex/licenses</code> directory on the machine (create the directory if it does not exist) and specify the serial number when using the inference API or CLI. Please note: Each serial number can only be bound to a unique device fingerprint and can only be bound once. This means that if users deploy models on different machines, they must prepare separate serial numbers for each machine.</li> </ul>"},{"location":"en/practical_tutorials/deployment_tutorial.html#12-install-high-performance-inference-plugin","title":"1.2 Install High-Performance Inference Plugin","text":"<p>Find the corresponding installation command in the table below based on processor architecture, operating system, device type, Python version, and other information, and execute it in the deployment environment:</p> Processor Architecture Operating System Device Type Python Version Installation Command x86-64 Linux CPU 3.8 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.8 - --arch x86_64 --os linux --device cpu --py 38 3.9 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.9 - --arch x86_64 --os linux --device cpu --py 39 3.10 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.10 - --arch x86_64 --os linux --device cpu --py 310 GPU (CUDA 11.8 + cuDNN 8.6) 3.8 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.8 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 38 3.9 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.9 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 39 3.10 curl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.10 - --arch x86_64 --os linux --device gpu_cuda118_cudnn86 --py 310 <ul> <li>When the device type is GPU, please use the installation command corresponding to the CUDA and cuDNN versions that match your environment; otherwise, the high-performance inference plugin will not function properly.</li> <li>For Linux systems, use Bash to execute the installation command.</li> <li>When the device type is CPU, the installed high-performance inference plugin only supports inference using the CPU; for other device types, the installed high-performance inference plugin supports inference using the CPU or other devices.</li> </ul>"},{"location":"en/practical_tutorials/deployment_tutorial.html#13-enabling-high-performance-inference-plugins","title":"1.3 Enabling High-Performance Inference Plugins","text":"<p>Before enabling high-performance plugins, ensure that the <code>LD_LIBRARY_PATH</code> in the current environment does not specify the shared library directory of TensorRT, as the plugins already integrate TensorRT to avoid conflicts caused by different TensorRT versions that may prevent the plugins from functioning properly.</p> <p>For PaddleX CLI, specify <code>--use_hpip</code> and set the serial number to enable the high-performance inference plugin. If you wish to activate online, specify <code>--update_license</code> when using the serial number for the first time, taking the General OCR pipeline as an example:</p> <pre><code>paddlex \\\n    --pipeline OCR \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png \\\n    --device gpu:0 \\\n    --use_hpip \\\n    --serial_number {serial_number}\n\n# If you wish to activate online\npaddlex \\\n    --pipeline OCR \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png \\\n    --device gpu:0 \\\n    --use_hpip \\\n    --update_license \\\n    --serial_number {serial_number}\n</code></pre> <p>For the PaddleX Python API, the method to enable the high-performance inference plugin is similar. Again, taking the General OCR pipeline as an example:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"OCR\",\n    use_hpip=True,\n    hpi_params={\"serial_number\": xxx}\n)\n\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png \")\n</code></pre> <p>The inference results obtained with the high-performance inference plugin are consistent with those without the plugin enabled. For some models, enabling the high-performance inference plugin for the first time may take a longer time to complete the construction of the inference engine. PaddleX will cache relevant information in the model directory after the first construction of the inference engine and reuse the cached content in subsequent initializations to improve speed.</p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#14-inference-steps","title":"1.4 Inference Steps","text":"<p>This inference process is based on PaddleX CLI, online activation of serial numbers, Python 3.10.0, and using a CPU device with the high-performance inference plugin. For other usage methods (such as different Python versions, device types, or PaddleX Python API), refer to the PaddleX High-Performance Inference Guide to replace the corresponding commands.</p> <pre><code># Install the high-performance inference plugin\ncurl -s https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/deploy/paddlex_hpi/install_script/latest/install_paddlex_hpi.py | python3.10 - --arch x86_64 --os linux --device cpu --py 310\n# Ensure that the `LD_LIBRARY_PATH` in the current environment does not specify the shared library directory of TensorRT. You can use the following command to remove it or manually remove it.\nexport LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\\n' | grep -v TensorRT | tr '\\n' ':' | sed 's/:*$//')\n# Perform inference\npaddlex --pipeline OCR --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --device gpu:0 --use_hpip --serial_number {serial_number} --update_license True --save_path ./output\n</code></pre> <p>Output:</p> <p> </p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#15-changing-pipelines-or-models","title":"1.5 Changing Pipelines or Models","text":"<ul> <li>Changing Pipelines:</li> </ul> <p>If you want to use a different pipeline with the high-performance inference plugin, simply replace the value passed to <code>--pipeline</code>. Here is an example using the General Object Detection pipeline:</p> <pre><code>paddlex --pipeline object_detection --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_object_detection_002.png --device gpu:0 --use_hpip --serial_number {serial_number} --update_license True --save_path ./output\n</code></pre> <ul> <li>Changing Models:</li> </ul> <p>The OCR pipeline defaults to using the PP-OCRv4_mobile_det and PP-OCRv4_mobile_rec models. If you want to use other models, such as PP-OCRv4_server_det and PP-OCRv4_server_rec, refer to the General OCR Pipeline Tutorial. The specific operations are as follows:</p> <pre><code># 1. Obtain the OCR pipeline configuration file and save it to ./OCR.yaml\npaddlex --get_pipeline_config OCR --save_path ./OCR.yaml\n\n# 2. Modify the ./OCR.yaml configuration file\n#    Change the value of Pipeline.text_det_model to the path of the PP-OCRv4_server_det model\n#    Change the value of Pipeline.text_rec_model to the path of the PP-OCRv4_server_rec model\n\n# 3. Use the modified configuration file when performing inference\npaddlex --pipeline ./OCR.yaml --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --device gpu:0 --use_hpip --serial_number {serial_number} --update_license True --save_path ./output\n</code></pre> <p>The General Object Detection pipeline defaults to using the PicoDet-S model. If you want to use another model, such as RT-DETR, refer to the General Object Detection Pipeline Tutorial. The specific operations are as follows:</p> <pre><code># 1. Obtain the OCR pipeline configuration file and save it to ./object_detection.yaml\npaddlex --get_pipeline_config object_detection --save_path ./object_detection.yaml\n\n# 2. Modify the ./object_detection.yaml configuration file\n#    Change the value of Pipeline.model to the path of the RT-DETR model\n\n# 3. Use the modified configuration file when performing inference\npaddlex --pipeline ./object_detection.yaml --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --device gpu:0 --use_hpip --serial_number {serial_number} --update_license True --save_path ./output\n</code></pre> <p>The operations for other pipelines are similar to those for the above two pipelines. For more details, refer to the pipeline usage tutorials.</p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#2-service-deployment-example","title":"2 Service Deployment Example","text":""},{"location":"en/practical_tutorials/deployment_tutorial.html#21-installing-the-service-deployment-plugin","title":"2.1 Installing the Service Deployment Plugin","text":"<p>Execute the following command to install the service deployment plugin:</p> <pre><code>paddlex --install serving\n</code></pre>"},{"location":"en/practical_tutorials/deployment_tutorial.html#22-starting-the-service","title":"2.2 Starting the Service","text":"<p>Start the service through the PaddleX CLI with the command format:</p> <pre><code>paddlex --serve --pipeline {Pipeline name or pipeline configuration file path} [{Other command-line options}]\n</code></pre> <p>Taking the General OCR pipeline as an example:</p> <pre><code>paddlex --serve --pipeline OCR\n</code></pre> <p>After the service starts successfully, you will see information similar to the following:</p> <pre><code>INFO:     Started server process [63108]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n</code></pre> <p><code>--pipeline</code> can be specified as an official pipeline name or a local pipeline configuration file path. PaddleX constructs the pipeline based on this and deploys it as a service. To adjust configurations (such as model path, batch_size, deployment device), please refer to the \"Model Application\" section in the General OCR Pipeline Tutorial.</p> <p>The command-line options related to service deployment are as follows:</p> Name Description <code>--pipeline</code> Pipeline name or pipeline configuration file path. <code>--device</code> Deployment device for the pipeline. Default is <code>cpu</code> (if GPU is not available) or <code>gpu</code> (if GPU is available). <code>--host</code> Hostname or IP address bound to the server. Default is 0.0.0.0. <code>--port</code> Port number listened to by the server. Default is 8080. <code>--use_hpip</code> Enables the high-performance inference plugin if specified. <code>--serial_number</code> Serial number used by the high-performance inference plugin. Only valid when the high-performance inference plugin is enabled. Please note that not all pipelines and models support the use of the high-performance inference plugin. For detailed support, please refer to the PaddleX High-Performance Inference Guide. <code>--update_license</code> Performs online activation if specified. Only valid when the high-performance inference plugin is enabled."},{"location":"en/practical_tutorials/deployment_tutorial.html#23-calling-the-service","title":"2.3 Calling the Service","text":"<p>Here, only the Python calling example is shown. For API references and service calling examples in other languages, please refer to the \"Calling the Service\" section in the \"Development Integration/Deployment\" part of each pipeline usage tutorial in the PaddleX Serving Deployment Guide.</p> <pre><code>import base64\nimport requests\n\nAPI_URL = \"http://localhost:8080/ocr\" # Service URL\nimage_path = \"./demo.jpg\"\noutput_image_path = \"./out.jpg\"\n\n# Encode the local image in Base64\nwith open(image_path, \"rb\") as file:\n    image_bytes = file.read()\n    image_data = base64.b64encode(image_bytes).decode(\"ascii\")\n\npayload = {\"image\": image_data}  # Base64-encoded file content or image URL\n\n# Call the API\nresponse = requests.post(API_URL, json=payload)\n\n# Process the response data\nassert response.status_code == 200\nresult = response.json()[\"result\"]\nwith open(output_image_path, \"wb\") as file:\n    file.write(base64.b64decode(result[\"image\"]))\nprint(f\"Output image saved at {output_image_path}\")\nprint(\"\\nDetected texts:\")\nprint(result[\"texts\"])\n</code></pre>"},{"location":"en/practical_tutorials/deployment_tutorial.html#24-deployment-steps","title":"2.4 Deployment Steps","text":"<pre><code># Install the service deployment plugin\npaddlex --install serving\n# Start the service\npaddlex --serve --pipeline OCR\n# Call the service | The code in fast_test.py is a Python calling example from the previous section\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png -O demo.jpg\npython fast_test.py\n</code></pre> <p>Running Results:</p> <p> </p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#25-change-pipelines-or-models","title":"2.5 Change Pipelines or Models","text":"<ul> <li>Change Pipelines:</li> </ul> <p>If you want to deploy another pipeline for service, simply replace the value passed to <code>--pipeline</code>. The following example uses the General Object Detection pipeline:</p> <pre><code>paddlex --serve --pipeline object_detection\n</code></pre> <ul> <li>Change Models:</li> </ul> <p>The OCR pipeline defaults to using the PP-OCRv4_mobile_det and PP-OCRv4_mobile_rec models. If you want to switch to other models, such as PP-OCRv4_server_det and PP-OCRv4_server_rec, refer to the General OCR Pipeline Tutorial. The specific steps are as follows:</p> <pre><code># 1. Obtain the OCR pipeline configuration file and save it as ./OCR.yaml\npaddlex --get_pipeline_config OCR --save_path ./OCR.yaml\n\n# 2. Modify the ./OCR.yaml configuration file\n#    Change the value of Pipeline.text_det_model to the path of the PP-OCRv4_server_det model\n#    Change the value of Pipeline.text_rec_model to the path of the PP-OCRv4_server_rec model\n\n# 3. Start the service using the modified configuration file\npaddlex --serve --pipeline ./OCR.yaml\n# 4. Call the service\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png -O demo.jpg\npython fast_test.py\n</code></pre> <p>The General Object Detection pipeline defaults to using the PicoDet-S model. If you want to switch to another model, such as RT-DETR, refer to the General Object Detection Pipeline Tutorial. The specific steps are as follows:</p> <pre><code># 1. Obtain the object detection pipeline configuration file and save it as ./object_detection.yaml\npaddlex --get_pipeline_config object_detection --save_path ./object_detection.yaml\n\n# 2. Modify the ./object_detection.yaml configuration file\n#    Change the value of Pipeline.model to the path of the RT-DETR model\n\n# 3. Start the service using the modified configuration file\npaddlex --serve --pipeline ./object_detection.yaml\n# 4. Call the service | fast_test.py needs to be replaced with the Python calling example from the General Object Detection Pipeline Tutorial\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png -O demo.jpg\npython fast_test.py\n</code></pre> <p>The operations for other pipelines are similar to the above two. For more details, refer to the pipeline usage tutorials.</p>"},{"location":"en/practical_tutorials/deployment_tutorial.html#3-edge-deployment-example","title":"3 Edge Deployment Example","text":""},{"location":"en/practical_tutorials/deployment_tutorial.html#31-environment-preparation","title":"3.1 Environment Preparation","text":"<ol> <li> <p>Install the CMake compilation tool locally and download the required version of the NDK software package from the Android NDK official website. For example, if developing on a Mac, download the NDK software package for the Mac platform from the Android NDK website.</p> <p>Environment Requirements - <code>CMake &gt;= 3.10</code> (the minimum version is not verified, but 3.20 and above are recommended) - <code>Android NDK &gt;= r17c</code> (the minimum version is not verified, but r20b and above are recommended)</p> <p>Testing Environment Used in This Guide: - <code>cmake == 3.20.0</code> - <code>android-ndk == r20b</code></p> </li> <li> <p>Prepare an Android phone and enable USB debugging mode. Enable method: <code>Phone Settings -&gt; Find Developer Options -&gt; Turn on Developer Options and USB Debugging Mode</code>.</p> </li> <li> <p>Install the ADB tool on your computer for debugging. The ADB installation methods are as follows:</p> <p>3.1. Install ADB on Mac</p> <pre><code> brew cask install android-platform-tools\n</code></pre> <p>3.2. Install ADB on Linux</p> <pre><code> # Installation method for Debian-based Linux distributions\n sudo apt update\n sudo apt install -y wget adb\n\n # Installation method for Red Hat-based Linux distributions\n sudo yum install adb\n</code></pre> <p>3.3. Install ADB on Windows</p> <p>To install on Windows, go to Google's Android platform to download the ADB software package for installation: Link</p> <p>Open the terminal, connect the phone to the computer, and enter in the terminal</p> <pre><code> adb devices\n</code></pre> <p>If there is a device output, it indicates successful installation.</p> <pre><code> List of devices attached\n 744be294    device\n</code></pre> </li> </ol>"},{"location":"en/practical_tutorials/deployment_tutorial.html#32-material-preparation","title":"3.2 Material Preparation","text":"<ol> <li> <p>Clone the <code>feature/paddle-x</code> branch of the <code>Paddle-Lite-Demo</code> repository into the <code>PaddleX-Lite-Deploy</code> directory.</p> <pre><code>git clone -b feature/paddle-x https://github.com/PaddlePaddle/Paddle-Lite-Demo.git PaddleX-Lite-Deploy\n</code></pre> </li> <li> <p>Fill out the survey to download the compressed package, place the compressed package in the specified extraction directory, switch to the specified extraction directory, and execute the extraction command.</p> <pre><code># 1. Switch to the specified extraction directory\ncd PaddleX-Lite-Deploy/ocr/android/shell/ppocr_demo\n\n# 2. Execute the extraction command\nunzip ocr.zip\n</code></pre> </li> </ol>"},{"location":"en/practical_tutorials/deployment_tutorial.html#33-deployment-steps","title":"3.3 Deployment Steps","text":"<ol> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/libs</code>, run the <code>download.sh</code> script to download the required Paddle Lite prediction library. This step only needs to be executed once to support each demo.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/assets</code>, run the <code>download.sh</code> script to download the model files optimized by the paddle_lite_opt tool.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/android/shell/cxx/ppocr_demo</code>, run the <code>build.sh</code> script to complete the compilation of the executable file.</p> </li> <li> <p>Switch the working directory to <code>PaddleX-Lite-Deploy/ocr/android/shell/cxx/ppocr_demo</code>, run the <code>run.sh</code> script to complete the inference on the edge side.</p> </li> </ol> <p>Notes:   - Before running the <code>build.sh</code> script, you need to change the path specified by <code>NDK_ROOT</code> to the actual installed NDK path.   - You can execute the deployment steps using Git Bash on a Windows system.   - If compiling on a Windows system, set <code>CMAKE_SYSTEM_NAME</code> in <code>CMakeLists.txt</code> to <code>windows</code>.   - If compiling on a Mac system, set <code>CMAKE_SYSTEM_NAME</code> in <code>CMakeLists.txt</code> to <code>darwin</code>.   - Maintain an ADB connection when running the <code>run.sh</code> script.   - The <code>download.sh</code> and <code>run.sh</code> scripts support passing parameters to specify models. If not specified, the <code>PP-OCRv4_mobile</code> model is used by default. Currently, two models are supported:     - <code>PP-OCRv3_mobile</code>     - <code>PP-OCRv4_mobile</code></p> <p>Here are examples of actual operations:</p> <pre><code># 1. Download the required Paddle Lite prediction library\ncd PaddleX-Lite-Deploy/libs\nsh download.sh\n\n# 2. Download the model files optimized by the paddle_lite_opt tool\ncd ../ocr/assets\nsh download.sh PP-OCRv4_mobile\n\n# 3. Complete the compilation of the executable file\ncd ../android/shell/ppocr_demo\nsh build.sh\n\n# 4. Inference\nsh run.sh PP-OCRv4_mobile\n</code></pre> <p>Detection Results:</p> <p></p> <p>Recognition Results:</p> <pre><code>The detection visualized image saved in ./test_img_result.jpg\n0       Pure Nutrition Hair Conditioner  0.993706\n1       Product Information/Parameters   0.991224\n2       (45 yuan/kg, minimum order 100 kg)    0.938893\n3       Each bottle 22 yuan, minimum order 1000 bottles)  0.988353\n4       [Brand]: OEM/ODM Manufacturing   0.97557\n5       [Product Name]: Pure Nutrition Hair Conditioner  0.986914\n6       OEM/ODM  0.929891\n7       [Product Number]: YM-X-3011 0.964156\n8       [Net Content]: 220ml      0.976404\n9       [Suitable for]: All skin types  0.987942\n10      [Main Ingredients]: Cetyl Stearyl Alcohol, Oat \u03b2-Glucan,  0.968315\n11      Cocoamide Propyl Betaine, Panthenol    0.941537\n12      (Finished Product Packaging)    0.974796\n13      [Main Function]: Can tighten the hair\u200b\u9cde\u7247\u200b, achieving  0.988799\n14      immediate and lasting improvement in hair gloss, providing sufficient nourishment to dry hair  0.989547\n15      [Main Function Continued]: Nourishment    0.998413\n</code></pre>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html","title":"PaddleX 3.0 Document Scene Information Extraction v3 (PP-ChatOCRv3_doc) \u2014\u2014 Tutorial on Paper and Literature Information Extraction","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can tackle specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can fine-tune the models with your private data. PaddleX also provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, please refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with a garbage classification task as an example.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>Document information extraction is a part of document processing with widespread applications in numerous scenarios, such as academic research, library management, scientific and technological intelligence analysis, and literature review writing. Through document information extraction technology, we can automatically extract key information from academic papers, including titles, authors, abstracts, keywords, publication years, journal names, citation information, and more, and store this information in a structured format for easy subsequent retrieval, analysis, and application. This not only enhances the work efficiency of researchers but also provides strong support for the in-depth development of academic research.</p> <p>Firstly, it is necessary to select the corresponding PaddleX pipeline based on the task scenario. This section takes information extraction from academic papers as an example to introduce how to conduct secondary development for tasks related to the Document Scene Information Extraction v3 pipeline, which corresponds to the Document Scene Information Extraction v3 in PaddleX. If you are unsure about the correspondence between tasks and pipelines, you can refer to the capability introductions of related pipelines in the  PaddleX Supported Pipelines List.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience its capabilities. You can try out the Document Scene Information Extraction v3 pipeline online, or you can use Python locally to experience the effects of the Document Scene Information Extraction v3 pipeline.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#21-local-experience","title":"2.1 Local Experience","text":"<p>Before using the Document Scene Information Extraction v3 pipeline locally, please ensure that you have completed the installation of the PaddleX wheel package according to the PaddleX Local Installation Tutorial. With just a few lines of code, you can quickly perform inference using the pipeline:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"PP-ChatOCRv3-doc\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # To use the Qianfan API, please fill in your Access Key (ak) and Secret Key (sk), as you will not be able to invoke large models without them.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Or, to use the AIStudio API, please fill in your access_token, as you will not be able to invoke large models without it.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_layout/test.jpg\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"header\", \"table caption\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>Note: Currently, the large language model only supports Ernie. You can obtain the relevant ak/sk (access_token) on the Baidu Cloud Qianfan Platform or Baidu AIStudio Community. If you use the Baidu Cloud Qianfan Platform, you can refer to the AK and SK Authentication API Calling Process to obtain ak/sk. If you use Baidu AIStudio Community, you can obtain the access_token from the Baidu AIStudio Community Access Token.</p> <p>The printed output results are as follows:</p> <pre><code>The result has been saved in output/tmpfnss9sq9_layout.jpg.\nThe result has been saved in output/tmpfnss9sq9_ocr.jpg.\nThe result has been saved in output/tmpfnss9sq9_table.jpg.\nThe result has been saved in output/tmpfnss9sq9_table.jpg.\nThe result has been saved in output/tmpfnss9sq9/tmpfnss9sq9.html.\nThe result has been saved in output/tmpfnss9sq9/tmpfnss9sq9.html.\nThe result has been saved in output/tmpfnss9sq9/tmpfnss9sq9.xlsx.\nThe result has been saved in output/tmpfnss9sq9/tmpfnss9sq9.xlsx.\n\n{'chat_res': {'\u200b\u9875\u7709\u200b': '\u200b\u672a\u77e5\u200b', '\u200b\u56fe\u8868\u200b\u6807\u9898\u200b': '\u200b\u672a\u77e5\u200b'}, 'prompt': ''}\n</code></pre> <p>In the <code>output</code> directory, the visualization results of layout area detection, OCR, table recognition, as well as the table results in HTML and XLSX formats, are saved.</p> <p>Among them, the visualization of the layout area detection results is as follows:</p> <p></p> <p>Through the online experience of the document scene information extraction, a Badcase analysis can be conducted to identify issues with the official model of the document scene information extraction pipeline. Due to the current limitation of the official model, which only distinguishes among figures, tables, and seals, it is currently unable to accurately locate and extract other information such as headers and table titles. The results for these in <code>{'chat_res': {'header': 'unknown', 'table caption': 'unknown'}, 'prompt': ''}</code> are unknown. Therefore, this section focuses on the scenario of academic papers. Utilizing a dataset of academic paper documents, with the extraction of header and chart title information as examples, the layout analysis model within the document scene information extraction pipeline is fine-tuned to achieve the ability to accurately extract header and table title information from the document.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#22-online-experience","title":"2.2 Online Experience","text":"<p>You can experience the effectiveness of the Document Scene Information Extraction v3 pipeline on the AIStudio Community. Click the link to download the Test Paper Document File, and then upload it to the official Document Scene Information Extraction v3 application to experience the extraction results. The process is as follows:</p> <p></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#3-choosing-a-model","title":"3. Choosing a Model","text":"<p>PaddleX provides 4 end-to-end layout detection models, which can be referenced in the Model List. Some of the benchmarks for these models are as follows:</p> Model mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description PicoDet_layout_1x 86.8 13.0 91.3 7.4 An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate five types of areas, including text, titles, tables, images, and lists. PicoDet_layout_1x_table 95.7 12.623 90.8934 7.4 M An efficient layout area localization model trained on the PubLayNet dataset based on PicoDet-1x can locate one type of tables. PicoDet-S_layout_3cls 87.1 13.5 45.8 4.8 An high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-S_layout_17cls 70.3 13.6 46.2 4.8 A high-efficient layout area localization model trained on a self-constructed dataset based on PicoDet-S_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. PicoDet-L_layout_3cls 89.3 15.7 159.8 22.6 An efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. PicoDet-L_layout_17cls 79.9 17.2 160.2 22.6 A efficient layout area localization model trained on a self-constructed dataset based on PicoDet-L_layout_17cls for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. RT-DETR-H_layout_3cls 95.9 114.6 3832.6 470.1 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes three categories: tables, images, and seals. RT-DETR-H_layout_17cls 92.6 115.1 3827.2 470.2 A high-precision layout area localization model trained on a self-constructed dataset based on RT-DETR-H for scenarios such as Chinese and English papers, magazines, and research reports includes 17 common layout categories, namely: paragraph titles, images, text, numbers, abstracts, content, chart titles, formulas, tables, table titles, references, document titles, footnotes, headers, algorithms, footers, and seals. <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout region analysis dataset, containing 10,000 images of common document types, including English and Chinese papers, magazines, research reports, etc. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the <code>Academic paper literature Dataset</code> as an example dataset. You can obtain the example dataset using the following commands. If you are using your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to the PaddleX Object Detection Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/paperlayout.tar -P ./dataset\ntar -xf ./dataset/paperlayout.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply run the following command:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/RT-DETR-H_layout_3cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/paperlayout/\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and count the basic information of the dataset. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log, and the relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory. The output directory includes visualized example images and a histogram of sample distribution. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is as follows:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 4,\n    \"train_samples\": 4734,\n    \"train_sample_paths\": [\n      \"check_dataset\\/demo_img\\/train_4612.jpg\",\n      \"check_dataset\\/demo_img\\/train_4844.jpg\",\n      \"check_dataset\\/demo_img\\/train_0084.jpg\",\n      \"check_dataset\\/demo_img\\/train_0448.jpg\",\n      \"check_dataset\\/demo_img\\/train_4703.jpg\",\n      \"check_dataset\\/demo_img\\/train_3572.jpg\",\n      \"check_dataset\\/demo_img\\/train_4516.jpg\",\n      \"check_dataset\\/demo_img\\/train_2836.jpg\",\n      \"check_dataset\\/demo_img\\/train_1353.jpg\",\n      \"check_dataset\\/demo_img\\/train_0225.jpg\"\n    ],\n    \"val_samples\": 928,\n    \"val_sample_paths\": [\n      \"check_dataset\\/demo_img\\/val_0982.jpg\",\n      \"check_dataset\\/demo_img\\/val_0607.jpg\",\n      \"check_dataset\\/demo_img\\/val_0623.jpg\",\n      \"check_dataset\\/demo_img\\/val_0890.jpg\",\n      \"check_dataset\\/demo_img\\/val_0036.jpg\",\n      \"check_dataset\\/demo_img\\/val_0654.jpg\",\n      \"check_dataset\\/demo_img\\/val_0895.jpg\",\n      \"check_dataset\\/demo_img\\/val_0059.jpg\",\n      \"check_dataset\\/demo_img\\/val_0142.jpg\",\n      \"check_dataset\\/demo_img\\/val_0088.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/dataset\\/paperlayout\\/\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 4, which is the number of classes that need to be passed in for subsequent training;</li> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 4734;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 928;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized images of the training set samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized images of the validation set samples in this dataset;</li> </ul> <p>In addition, the dataset verification also analyzes the sample number distribution of all categories in the dataset and draws a distribution histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#43-dataset-splitting-optional","title":"4.3 Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>The parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example explanations of the parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, the dataset format will be converted. The default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is to be re-split, you need to set the percentage of the training set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If the dataset is to be re-split, you need to set the percentage of the validation set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>train_percent</code> is 100;</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#data-splitting","title":"Data Splitting","text":"<p>When splitting data, the original annotation files will be renamed as <code>xxx.bak</code> in the original path. The above parameters can also be set by appending command line arguments, for example, to re-split the dataset and set the ratio of training set to validation set: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, please ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/RT-DETR-H_layout_3cls.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/paperlayout \\\n    -o Train.num_classes=4\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, etc., by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations of parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify the card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. If you need to specify a save path, you can use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Explanation of Training Outputs:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics and related file paths;</li> <li>train.log: Training log file, recording changes in model metrics and loss during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdopt, .pdstates, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/RT-DETR-H_layout_3cls.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/paperlayout\n</code></pre> <p>Similar to model training, model evaluation supports setting by modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 30, and set the batch size to 4 due to the small size of the training dataset.</li> <li>Initiate four experiments based on the <code>RT-DETR-H_layout_3cls</code> model, with learning rates of: 0.001\uff0c0.0005\uff0c0.0001\uff0c0.00001.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 2 is a learning rate of 0.0001. Based on this training hyperparameter, change the number of epochs and observe the accuracy results at different epochs, finding that the best accuracy is generally achieved at 50 and 100 epochs.</li> </ol> <p>Learning Rate Exploration Results:</p> <p> Experiment ID Learning Rate mAP@@0.50:0.95 1 0.00001 88.90 2 0.0001 92.41 3 0.0005 92.27 4 0.001 90.66 <p></p> <p>Next, we can increase the number of training epochs based on a learning rate set to 0.001. Comparing experiments [2, 4, 5] below, it can be seen that as the number of training epochs increases, the model's accuracy further improves.</p> <p> Experiment ID Learning Rate mAP@@0.50:0.95 2 30 92.41 4 50 92.63 5 100 92.88 <p></p> <p>Note: This tutorial is designed for a 4-GPU setup. If you only have 1 GPU, you can complete the experiment by adjusting the number of training GPUs, but the final metrics may not align perfectly with the above indicators, which is normal.</p> <p>When selecting a training environment, it is important to consider the relationship between the number of training GPUs, the total batch_size, and the learning rate. Firstly, the total batch_size is equal to the number of training GPUs multiplied by the batch_size per GPU. Secondly, the total batch_size and the learning rate are related, and the learning rate should be adjusted in synchronization with the total batch_size. The default learning rate corresponds to a total batch_size based on 4 GPUs. If you plan to train in a single-GPU environment, you need to divide the learning rate by 4 accordingly. If you plan to train in an 8-GPU environment, you need to multiply the learning rate by 2 accordingly.</p> <p>For reference, the command to execute training with different parameter adjustments can be:</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/RT-DETR-H_layout_3cls.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/paperlayout \\\n    -o Train.num_classes=4 \\\n    -o Train.learning_rate=0.0001 \\\n    -o Train.epochs_iters=30 \\\n    -o Train.batch_size=4\n</code></pre>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#54-model-inference","title":"5.4 Model Inference","text":"<p>You can test the fine-tuned single model using test file</p> <pre><code>python main.py -c paddlex/configs/structure_analysis/RT-DETR-H_layout_3cls.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_layout/test.jpg\"\n</code></pre> <p>By following the above steps, prediction results can be generated under the ./output directory. The layout prediction result for test.jpg is as follows:</p> <p></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#6-pipeline-inference","title":"6. Pipeline Inference","text":"<p>Replace the model in the production line with the fine-tuned model for testing, and use the academic paper literature test file to perform predictions.</p> <p>First, obtain and update the configuration file for the Document Information Extraction v3. Execute the following command to retrieve the configuration file (assuming a custom save location of <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config PP-ChatOCRv3-doc --save_path ./my_path\n</code></pre> <p>Modify the <code>Pipeline.layout_model</code> field in <code>PP-ChatOCRv3-doc.yaml</code> to the path of the fine-tuned model mentioned above. The modified configuration is as follows:</p> <pre><code>Pipeline:\n  layout_model: ./output/best_model/inference\n  table_model: SLANet_plus\n  text_det_model: PP-OCRv4_server_det\n  text_rec_model: PP-OCRv4_server_rec\n  seal_text_det_model: PP-OCRv4_server_seal_det\n  doc_image_ori_cls_model: null\n  doc_image_unwarp_model: null\n  llm_name: \"ernie-3.5\"\n  llm_params:\n    api_type: qianfan\n    ak:\n    sk:\n</code></pre> <p>After making the modifications, you only need to change the value of the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the production line configuration file to apply the configuration.</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"./my_path/PP-ChatOCRv3-doc.yaml\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # To use the Qianfan API, please fill in your Access Key (ak) and Secret Key (sk), as you will not be able to invoke large models without them.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Or, to use the AIStudio API, please fill in your access_token, as you will not be able to invoke large models without it.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_layout/test.jpg\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output_ft\")\n    res.save_to_html('./output_ft')\n    res.save_to_xlsx('./output_ft')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"header\", \"table caption\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>By following the above steps, prediction results can be generated under <code>./output_ft</code>, and the printed key information extraction results are as follows:</p> <pre><code>{'chat_res': {'header': '\u200b\u7b2c\u200b43\u200b\u5377\u200b\\n \u200b\u822a\u7a7a\u200b\u53d1\u52a8\u673a\u200b\\n 44', 'table caption': '\u200b\u8868\u200b1\u200b\u6a21\u62df\u200b\u6765\u6d41\u200bMa=5\u200b\u98de\u884c\u200b\u7684\u200b\u7a7a\u6c14\u200b\u52a0\u70ed\u5668\u200b\u5de5\u4f5c\u200b\u53c2\u6570\u200b'}, 'prompt': ''}\n</code></pre> <p>It can be observed that after fine-tuning the model, the key information has been correctly extracted.</p> <p>The visualization result of the layout is as follows, with the correctly added ability to locate the areas of headers and table titles:</p> <p></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the Document Scene Information Extraction v3 production line meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <ol> <li>Directly apply the trained model production line in your Python project, as shown in the following code:</li> </ol> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"./my_path/PP-ChatOCRv3-doc.yaml\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # To use the Qianfan API, please fill in your Access Key (ak) and Secret Key (sk), as you will not be able to invoke large models without them.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Or, to use the AIStudio API, please fill in your access_token, as you will not be able to invoke large models without it.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_layout/test.jpg\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"header\", \"table caption\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>For more parameters, please refer to the Document Scene Information Extraction Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugin aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html","title":"PaddleX 3.0 Document Scene Information Extraction v3 (PP-ChatOCRv3_doc) \u2014\u2014 Tutorial on Seal Information Extraction","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can tackle specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can fine-tune the models with your private data. PaddleX also provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, please refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with a garbage classification task as an example.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>Seal information extraction is a part of document processing and has applications in many scenarios, such as contract comparison, inventory review, and invoice reimbursement review. Using artificial intelligence technology for automatic seal recognition can effectively save labor costs and improve efficiency for enterprises.</p> <p>Firstly, it is necessary to select the corresponding PaddleX production line based on the task scenario. This section is about the seal information extraction task, and it is not difficult to find that the seal information extraction task is closely related to the document scenario information extraction task. Document scene information extraction, which extracts text information from documents or images, is a classic problem in the field of computer vision. Corresponding to PaddleX's document scene information extraction v3 production line. If you are unable to determine the correspondence between tasks and production lines, you can learn about the capabilities of the relevant production lines in PaddleX Supported Pipelines List.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience its capabilities. You can try out the Document Scene Information Extraction v3 pipeline online, or you can use Python locally to experience the effects of the Document Scene Information Extraction v3 pipeline.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#21-local-experience","title":"2.1 Local Experience","text":"<p>Before using the Document Scene Information Extraction v3 pipeline locally, please ensure that you have completed the installation of the PaddleX wheel package according to the PaddleX Local Installation Tutorial. With just a few lines of code, you can quickly perform inference using the pipeline:</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"PP-ChatOCRv3-doc\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # To use the Qianfan API, please fill in your Access Key (ak) and Secret Key (sk), as you will not be able to invoke large models without them.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Or, to use the AIStudio API, please fill in your access_token, as you will not be able to invoke large models without it.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_seal/test.png\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"\u200b\u5370\u7ae0\u200b\u540d\u79f0\u200b\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>Note: Currently, the large language model only supports Ernie. You can obtain the relevant ak/sk (access_token) on the Baidu Cloud Qianfan Platform or Baidu AIStudio Community. If you use the Baidu Cloud Qianfan Platform, you can refer to the AK and SK Authentication API Calling Process to obtain ak/sk. If you use Baidu AIStudio Community, you can obtain the access_token from the Baidu AIStudio Community Access Token.</p> <p>The printed output results are as follows:</p> <pre><code>The result has been saved in output/tmpx5tmgsem_curve_0.jpg.\nThe result has been saved in output/tmpx5tmgsem_layout.jpg.\nThe result has been saved in output/tmpx5tmgsem_ocr.jpg.\nThe retrieved vectorstore is not for PaddleX and will return vectorstore directly\n\n{'chat_res': {'\u200b\u5370\u7ae0\u200b\u540d\u79f0\u200b': '\u200b\u672a\u77e5\u200b'}, 'prompt': ''}\n</code></pre> <p>In the <code>output</code> directory, the visualization results of seal_recognition and OCR (if table exits, table recognition, as well as the table results in HTML and XLSX formats), are saved.</p> <p>Among them, the visualization of the seal_recognition results is as follows:</p> <p></p> <p>Through the online experience of the document scene information extraction, a Badcase analysis can be conducted to identify issues with the official model of the document scene information extraction pipeline. The official model of the document scene information extraction production line has been found to have the following issues in the current demand scenario: in the visualization of OCR recognition, there is a deviation in the text bending detection box of the seal, resulting in incorrect seal text recognition; The information on the seal was not correctly extracted. The results for these in <code>{'chat_res': {'seal': 'unknown'}, 'prompt': ''}</code> are unknown. Therefore, this section focuses on the scenario of Seal Text Recognitions. Utilizing a dataset of Seal Text Recognition, with the extraction of seal information as examples, the seal text detection model within the document scene information extraction pipeline is fine-tuned to achieve the ability to accurately extract seal information from the document.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#22-online-experience","title":"2.2 Online Experience","text":"<p>You can experience the effectiveness of the Document Scene Information Extraction v3 pipeline on the AIStudio Community. Click the link to download the Test Seal File, and then upload it to the official Document Scene Information Extraction v3 application to experience the extraction results. The process is as follows:</p> <p></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#3-choosing-a-model","title":"3. Choosing a Model","text":"<p>PaddleX provides 2 end-to-end seal text detection models, which can be referenced in the Model List. Some of the benchmarks for these models are as follows:</p> Model mAP(0.5) (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) Description - - - - - - PP-OCRv4_mobile_seal_det 96.47 10.5878 131.813 4.7M PP-OCRv4_mobile_seal_det.yaml PP-OCRv4_server_seal_det 98.21 84.341 2425.06 108.3 M PP-OCRv4_server_seal_det.yaml <p>Note: The evaluation set for the above accuracy metrics is our self-built seal text detection dataset, containing 500 images of circle seal types. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the <code>Seal Text Detection Dataset</code> as an example dataset. You can obtain the example dataset using the following commands. If you are using your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to the PaddleX Object Detection Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/practical_seal.tar -P ./dataset\ntar -xf ./dataset/practical_seal.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply run the following command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/practical_seal/\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and count the basic information of the dataset. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log, and the relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory. The output directory includes visualized example images and a histogram of sample distribution. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is as follows:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 793,\n    \"train_sample_paths\": [\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC4055390_00006_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC3712248_00008_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC4227328_00001_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC3745965_00007_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC3980931_00001_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC5896212_00003_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC3838814_00003_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC4677212_00002_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC4058803_00001_seal_0_crop.png\",\n      \"..\\/dataset\\/practical_seal\\/images\\/PMC4925966_00001_seal_0_crop.png\"\n    ],\n    \"val_samples\": 277,\n    \"val_sample_paths\": [\n      \"..\\/dataset\\/practical_seal\\/images\\/15.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/16.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/17.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/18.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/19.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/20.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/21.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/22.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/23.jpg\",\n      \"..\\/dataset\\/practical_seal\\/images\\/24.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \".\\/dataset\\/practical_seal\\/\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"TextDetDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 739;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 277;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized images of the training set samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized images of the validation set samples in this dataset;</li> </ul> <p>In addition, the dataset verification also analyzes the sample number distribution of all boxes's width and height in the dataset and draws a distribution histogram (<code>histogram.png</code>):</p> <p> <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#43-dataset-splitting-optional","title":"4.3 Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>The parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example explanations of the parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, the dataset format will be converted. The default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is to be re-split, you need to set the percentage of the training set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If the dataset is to be re-split, you need to set the percentage of the validation set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>train_percent</code> is 100;</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#data-splitting","title":"Data Splitting","text":"<p>When splitting data, the original annotation files will be renamed as <code>xxx.bak</code> in the original path. The above parameters can also be set by appending command line arguments, for example, to re-split the dataset and set the ratio of training set to validation set: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, please ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/practical_seal \\\n    -o Train.epochs_iters=30 \\\n    -o Train.batch_size=4 \\\n    -o Train.learning_rate=0.0001\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, etc., by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations of parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify the card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. If you need to specify a save path, you can use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Explanation of Training Outputs:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics and related file paths;</li> <li>train.log: Training log file, recording changes in model metrics and loss during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdopt, .pdstates, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/practical_seal\n</code></pre> <p>Similar to model training, model evaluation supports setting by modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 30, and set the batch size to 4 due to the small size of the training dataset.</li> <li>Initiate 3 experiments based on the <code>PP-OCRv4_server_seal_det</code> model, with learning rates of: 0.001\uff0c0.0001\uff0c0.00001.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 1 is a learning rate of 0.001. Based on this training hyperparameter, change the number of epochs and observe the accuracy results at different epochs, finding that the best accuracy is generally achieved at 100 epochs.</li> </ol> <p>Learning Rate Exploration Results:</p> <p> Experiment ID Learning Rate Hmean(%) 1 0.001 97.35 2 0.0001 93.32 3 0.00001 87.63 <p></p> <p>Next, we can increase the number of training epochs based on a learning rate set to 0.001. Comparing experiments [1, 4] below, it can be seen that as the number of training epochs increases, the model's accuracy further improves.</p> <p> Experiment ID Learning Rate Hmean(%) 1 30 97.35 4 100 98.13 <p></p> <p>Note: This tutorial is designed for a 4-GPU setup. If you only have 1 GPU, you can complete the experiment by adjusting the number of training GPUs, but the final metrics may not align perfectly with the above indicators, which is normal.</p> <p>When selecting a training environment, it is important to consider the relationship between the number of training GPUs, the total batch_size, and the learning rate. Firstly, the total batch_size is equal to the number of training GPUs multiplied by the batch_size per GPU. Secondly, the total batch_size and the learning rate are related, and the learning rate should be adjusted in synchronization with the total batch_size. The default learning rate corresponds to a total batch_size based on 4 GPUs. If you plan to train in a single-GPU environment, you need to divide the learning rate by 4 accordingly. If you plan to train in an 8-GPU environment, you need to multiply the learning rate by 2 accordingly.</p> <p>For reference, the command to execute training with different parameter adjustments can be:</p> <pre><code>python main.py -c paddlex/configs/text_detection_seal/PP-OCRv4_server_seal_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/practical_seal \\\n    -o Train.learning_rate=0.0001 \\\n    -o Train.epochs_iters=30 \\\n    -o Train.batch_size=4\n</code></pre>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#6-pipeline-inference","title":"6. Pipeline Inference","text":"<p>Replace the model in the production line with the fine-tuned model for testing, and use the test file to perform predictions.</p> <p>First, obtain and update the configuration file for the Document Information Extraction v3. Execute the following command to retrieve the configuration file (assuming a custom save location of <code>./my_path</code>):</p> <pre><code>paddlex --get_pipeline_config PP-ChatOCRv3-doc --save_path ./my_path\n</code></pre> <p>Modify the <code>Pipeline.seal_text_det_model</code> field in <code>PP-ChatOCRv3-doc.yaml</code> to the path of the fine-tuned model mentioned above. The modified configuration is as follows:</p> <pre><code>Pipeline:\n  layout_model: RT-DETR-H_layout_3cls\n  table_model: SLANet_plus\n  text_det_model: PP-OCRv4_server_det\n  text_rec_model: PP-OCRv4_server_rec\n  seal_text_det_model: ./output/best_accuracy/inference\n  doc_image_ori_cls_model: null\n  doc_image_unwarp_model: null\n  llm_name: \"ernie-3.5\"\n  llm_params:\n    api_type: qianfan\n    ak:\n    sk:\n</code></pre> <p>After making the modifications, you only need to change the value of the <code>pipeline</code> parameter in the <code>create_pipeline</code> method to the path of the production line configuration file to apply the configuration.</p> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"./my_path/PP-ChatOCRv3-doc.yaml\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # \u200b\u8bf7\u200b\u586b\u5165\u200b\u60a8\u200b\u7684\u200bak\u200b\u4e0e\u200bsk\uff0c\u200b\u5426\u5219\u200b\u65e0\u6cd5\u200b\u8c03\u7528\u200b\u5927\u200b\u6a21\u578b\u200b\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # \u200b\u6216\u8005\u200b\u4f7f\u7528\u200bAIStudio\u200b\u63a5\u53e3\u200b\uff0c\u200b\u8bf7\u200b\u586b\u5165\u200b\u60a8\u200b\u7684\u200baccess_token\uff0c\u200b\u5426\u5219\u200b\u65e0\u6cd5\u200b\u8c03\u7528\u200b\u5927\u200b\u6a21\u578b\u200b\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_seal/test.png\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output_ft\")\n    res.save_to_html('./output_ft')\n    res.save_to_xlsx('./output_ft')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"\u200b\u5370\u7ae0\u200b\u540d\u79f0\u200b\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>By following the above steps, prediction results can be generated under <code>./output_ft</code>, and the printed key information extraction results are as follows:</p> <pre><code>{'chat_res': {'\u200b\u5370\u7ae0\u200b\u540d\u79f0\u200b': '\u200b\u5e7f\u5173\u5e02\u200b\u56fd\u58eb\u200b\u8d44\u6e90\u200b\u5c40\u200b'}, 'prompt': ''}\n</code></pre> <p>It can be observed that after fine-tuning the model, the key information has been correctly extracted.</p> <p>The visualization result of the seal is as follows, with the correctly added ability to extract the seal text information:</p> <p></p>"},{"location":"en/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the Document Scene Information Extraction v3 production line meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <ol> <li>Directly apply the trained model production line in your Python project, as shown in the following code:</li> </ol> <pre><code>from paddlex import create_pipeline\n\npipeline = create_pipeline(\n    pipeline=\"./my_path/PP-ChatOCRv3-doc.yaml\",\n    llm_name=\"ernie-3.5\",\n    llm_params={\"api_type\": \"qianfan\", \"ak\": \"\", \"sk\": \"\"} # To use the Qianfan API, please fill in your Access Key (ak) and Secret Key (sk), as you will not be able to invoke large models without them.\n    # llm_params={\"api_type\": \"aistudio\", \"access_token\": \"\"} # Or, to use the AIStudio API, please fill in your access_token, as you will not be able to invoke large models without it.\n    )\n\nvisual_result, visual_info = pipeline.visual_predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/PP-ChatOCRv3_doc_seal/test.png\")\n\nfor res in visual_result:\n    res.save_to_img(\"./output\")\n    res.save_to_html('./output')\n    res.save_to_xlsx('./output')\n\nvector = pipeline.build_vector(visual_info=visual_info)\nchat_result = pipeline.chat(\n    key_list=[\"seal\"],\n    visual_info=visual_info,\n    vector=vector,\n    )\nchat_result.print()\n</code></pre> <p>For more parameters, please refer to the Document Scene Information Extraction Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugin aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html","title":"PaddleX 3.0 General Image Classification Pipeline \u2014 Garbage Classification Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can tackle specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can fine-tune the models with your private data. PaddleX also provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, please refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with a garbage classification task as an example.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For garbage classification, it falls under the general image classification task, corresponding to PaddleX's universal image classification pipeline. If you're unsure about the task-pipeline correspondence, you can check the capabilities of relevant pipelines in the PaddleX Supported Pipelines List.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipelines: one is through the PaddleX wheel package locally, and the other is on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:     <pre><code>paddlex --pipeline image_classification \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/garbage_demo.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Go to Baidu AIStudio Community, click \"Create Pipeline\", and create a General Image Classification pipeline for a quick trial.</p> </li> </ul> <p>Quick Trial Output Example:  <p></p> <p></p> <p>After trying the pipeline, determine if it meets your expectations (including accuracy, speed, etc.). If the model's speed or accuracy is not satisfactory, you can test alternative models and decide if further fine-tuning is needed. Since this tutorial aims to classify specific garbage types, the default weights (trained on the ImageNet-1k dataset) are insufficient. You need to collect and annotate data for training and fine-tuning.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#3-choosing-a-model","title":"3. Choosing a Model","text":"<p>PaddleX provides 80 end-to-end image classification models, which can be referenced in the Model List. Some of the benchmarks for these models are as follows:</p> Model List Top-1 Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) PP-HGNetV2_B6 86.30 10.46 240.18 288 CLIP_vit_base_patch16_224 85.39 12.03 234.85 331 PP-HGNetV2_B4 83.57 2.45 38.10 76 SwinTransformer_base_patch4_window7_224 83.37 12.35 - 342 PP-HGNet_small 81.51 4.24 108.21 94 PP-HGNetV2_B0 77.77 0.68 6.41 23 ResNet50 76.50 3.12 50.90 98 PP-LCNet_x1_0 71.32 1.01 3.39 7 MobileNetV3_small_x1_0 68.24 1.09 3.65 12 <p>Note: The above accuracy metrics are Top-1 Accuracy on the ImageNet-1k validation set. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In short, the models listed from top to bottom have faster inference speeds, while those from bottom to top have higher accuracy. This tutorial will use the <code>PP-LCNet_x1_0</code> model as an example to complete the full model development process. You can select an appropriate model for training based on your actual usage scenarios. After training, you can evaluate the suitable model weights within your pipeline and ultimately use them in real-world scenarios.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the <code>Garbage Classification Dataset</code> as an example dataset. You can obtain the example dataset using the following commands. If you are using your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to the PaddleX Image Classification Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/trash40.tar -P ./dataset\ntar -xf ./dataset/trash40.tar -C ./dataset/\n</code></pre>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply run the following command:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/trash40/\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and count the basic information of the dataset. If the command runs successfully, it will print <code>Check dataset passed !</code> in the log, and the relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory. The output directory includes visualized example images and a histogram of sample distribution. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is as follows:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"label_file\": ../../dataset/trash40/label.txt\",\n    \"num_classes\": 40,\n    \"train_samples\": 1605,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/img_14950.jpg\",\n      \"check_dataset/demo_img/img_12045.jpg\",\n    ],\n    \"val_samples\": 3558,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/img_198.jpg\",\n      \"check_dataset/demo_img/img_19627.jpg\",\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/trash40/\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"ClsDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 40, which is the number of classes that need to be passed in for subsequent training;</li> <li><code>attributes.train_samples</code>: The number of training set samples in this dataset is 1605;</li> <li><code>attributes.val_samples</code>: The number of validation set samples in this dataset is 3558;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized images of the training set samples in this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized images of the validation set samples in this dataset;</li> </ul> <p>In addition, the dataset verification also analyzes the sample number distribution of all categories in the dataset and draws a distribution histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#43-dataset-splitting-optional","title":"4.3 Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>The parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example explanations of the parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. When set to <code>True</code>, the dataset format will be converted. The default is <code>False</code>;</li> <li><code>train_percent</code>: If the dataset is to be re-split, you need to set the percentage of the training set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If the dataset is to be re-split, you need to set the percentage of the validation set. The type is any integer between 0-100, and it needs to ensure that the sum with <code>train_percent</code> is 100;</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#data-splitting","title":"Data Splitting","text":"<p>When splitting data, the original annotation files will be renamed as <code>xxx.bak</code> in the original path. The above parameters can also be set by appending command line arguments, for example, to re-split the dataset and set the ratio of training set to validation set: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, please ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/trash40 \\\n    -o Train.num_classes=40\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, etc., by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations of parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify the card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. If you need to specify a save path, you can use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Explanation of Training Outputs:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics and related file paths;</li> <li>train.log: Training log file, recording changes in model metrics and loss during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdopt, .pdstates, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/trash40\n</code></pre> <p>Similar to model training, model evaluation supports setting by modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 20, and set the batch size to 64 due to the small size of the training dataset.</li> <li>Initiate three experiments based on the <code>PP-LCNet_x1_0</code> model, with learning rates of: 0.01, 0.001, 0.1.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 1 is a learning rate of 0.01. Based on this training hyperparameter, change the number of epochs and observe the accuracy results at different epochs, finding that the best accuracy is generally achieved at 100 epochs.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Epochs Learning Rate batch_size Training Environment Top-1 Acc Experiment 1 20 0.01 64 4 GPUs 73.83% Experiment 2 20 0.001 64 4 GPUs 30.64% Experiment 3 20 0.1 64 4 GPUs 71.53% <p></p> <p>Changing Epochs Experiment Results:  Experiment Epochs Learning Rate batch_size Training Environment Top-1 Acc Experiment 1 20 0.01 64 4 GPUs 73.83% Experiment 1 (Increased Epochs) 50 0.01 64 4 GPUs 77.32% Experiment 1 (Increased Epochs) 80 0.01 64 4 GPUs 77.60% Experiment 1 (Increased Epochs) 100 0.01 64 4 GPUs 77.80% <p></p> <p>Note: The above accuracy metrics are Top-1 Accuracy on the ImageNet-1k validation set. GPU inference time is based on an NVIDIA Tesla T4 machine, with FP32 precision. CPU inference speed is based on an Intel\u00ae Xeon\u00ae Gold 5117 CPU @ 2.00GHz, with 8 threads and FP32 precision.</p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing. Use the test file to perform predictions:</p> <pre><code>python main.py -c paddlex/configs/image_classification/PP-LCNet_x1_0.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/garbage_demo.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, and the prediction result for <code>garbage_demo.png</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/image_classification_garbage_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the General Image Classification Pipeline meets your requirements for inference speed and accuracy in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/image_classification.yaml</code> configuration file to your own model path: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/image_classification.yaml\")\noutput = pipeline.predict(\"./dataset/trash40/images/test/0/img_154.jpg\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized result image\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to the General Image Classification Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugin aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html","title":"PaddleX 3.0 General Instance Segmentation Pipeline \u2014 Tutorial for Remote Sensing Image Instance Segmentation","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can tackle specific scenario tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, fine-tuning with private data is also supported. PaddleX provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of remote sensing image segmentation.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For remote sensing image segmentation, this falls under the category of instance segmentation, corresponding to PaddleX's Universal Instance Segmentation Pipeline. If unsure about the task-pipeline correspondence, refer to the Pipeline List for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipeline: locally through the PaddleX wheel package or on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:     <pre><code>paddlex --pipeline instance_segmentation \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/remotesensing_demo.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Go to Baidu AIStudio Community, click \"Create Pipeline\", and create a Universal Instance Segmentation pipeline for a quick trial.</p> </li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After experiencing the pipeline, determine if it meets your expectations (including accuracy, speed, etc.). If the included model needs further fine-tuning due to unsatisfactory speed or accuracy, select alternative models for testing until satisfied. If the final results are unsatisfactory, fine-tuning the model is necessary. This tutorial aims to produce a model that segments geospatial objects, and the default weights (trained on the COCO dataset) cannot meet this requirement. Data collection and annotation are required for training and fine-tuning.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#3-select-a-model","title":"3. Select a Model","text":"<p>PaddleX provides 15 end-to-end instance segmentation models. Refer to the Model List for details. Benchmarks for some models are as follows:</p> Model List mAP(%) GPU Inference Time(ms) Model Size(M) Mask-RT-DETR-H 48.8 61.40 486 Mask-RT-DETR-X 47.5 45.70 257 Mask-RT-DETR-L 45.7 37.40 123 Mask-RT-DETR-S 40.9 32.40 57 <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the COCO2017 validation set. GPU inference time is based on an NVIDIA V100 machine with FP32 precision.</p> <p>In summary, models listed from top to bottom offer faster inference speeds, while those from bottom to top offer higher accuracy. This tutorial uses the <code>Mask-RT-DETR-H</code> model as an example to complete the full model development process. Choose a suitable model based on your actual usage scenario, train it, evaluate the model weights within the pipeline, and finally apply them in real-world scenarios.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#4-data-preparation-and-validation","title":"4. Data Preparation and Validation","text":""},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Remote Sensing Image Instance Segmentation Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you are using your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to PaddleX Instance Segmentation Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/intseg_remote_sense_coco.tar -P ./dataset\ntar -xf ./dataset/intseg_remote_sense_coco.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>When verifying the dataset, you only need one command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-H.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/intseg_remote_sense_coco\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Upon successful execution, the log will print <code>Check dataset passed !</code>, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> folder. The output directory includes visualized sample images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is as follows:</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 10,\n    \"train_samples\": 2989,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/524.jpg\",\n      \"check_dataset/demo_img/024.jpg\",\n    ],\n    \"val_samples\": 932,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/326.jpg\",\n      \"check_dataset/demo_img/596.jpg\",\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/intseg_remote_sense_coco/\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCOInstSegDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>true</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 10, which is the number of classes that need to be passed in for subsequent training.</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 2989.</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 932.</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualized training samples.</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualized validation samples.</li> </ul> <p>Additionally, the dataset verification also analyzes the sample number distribution of all categories in the dataset and generates a distribution histogram (<code>histogram.png</code>):</p> <p> <p></p> <p></p> <p>Note: Only data that passes verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#43-dataset-format-conversion-dataset-splitting-optional","title":"4.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can modify the configuration file or append hyperparameters for settings.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. The following are example explanations of some parameters in the configuration file:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: Whether to convert the dataset format. Set to <code>True</code> to enable dataset format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, the source dataset format needs to be set. Available source formats are <code>LabelMe</code> and <code>VOC</code>;</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, the percentage of the training set needs to be set, which is an integer between 0-100, and the sum with <code>val_percent</code> must be 100;</li> <li><code>val_percent</code>: If re-splitting the dataset, the percentage of the validation set needs to be set, which is an integer between 0-100, and the sum with <code>train_percent</code> must be 100;</li> </ul> </li> </ul> </li> </ul> <p>Data conversion and data splitting can be enabled simultaneously. For data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in the original path. The above parameters also support setting through appending command line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have verified the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-H.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/intseg_remote_sense_coco \\\n    -o Train.num_classes=10\n</code></pre> <p>PaddleX model training supports modifying training hyperparameters, single/multi-GPU training, etc., simply by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Parameters related to model training can be set by modifying the fields under <code>Train</code> in the configuration file. The following are example explanations of some parameters in the configuration file:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset verification (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first 2 GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. If you need to specify a save path, you can use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced. During model inference, static graph weights are selected by default.</p> <p>Explanation of Training Outputs:</p> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, which logs whether the training task completed successfully, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, which records changes in model metrics, loss variations, etc. during the training process;</li> <li>config.yaml: Training configuration file, which records the hyperparameter configurations for this training session;</li> <li>.pdparams, .pdema, .pdopt.pdstate, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer states, EMA (Exponential Moving Average), static graph network parameters, and static graph network structures;</li> </ul>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight files on the validation set to verify the model's accuracy. To perform model evaluation using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-H.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/intseg_remote_sense_coco\n</code></pre> <p>Similar to model training, model evaluation supports setting configurations by modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating the model, you need to specify the model weight file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command-line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#53-model-tuning","title":"5.3 Model Tuning","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 80 and the batch size at 2.</li> <li>Launch three experiments based on the <code>Mask-RT-DETR-H</code> model with learning rates of: 0.0005, 0.005, 0.0001</li> <li>You may find that the configuration with the highest accuracy in Experiment 2 is a learning rate of 0.0001. Based on this training hyperparameter, change the number of epochs and observe the accuracy results for different epochs.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 1 80 0.0005 2 4 GPUs 0.695 Experiment 2 80 0.0001 2 4 GPUs 0.825 Experiment 3 80 0.00005 2 4 GPUs 0.706 <p></p> <p>Epoch Variation Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 2 80 0.0001 2 4 GPUs 0.825 Reduced Epochs in Experiment 2 30 0.0001 2 4 GPUs 0.287 Reduced Epochs in Experiment 2 50 0.0001 2 4 GPUs 0.545 Increased Epochs in Experiment 2 100 0.0001 2 4 GPUs 0.813 <p></p> <p>Note: This tutorial is designed for 4 GPUs. If you only have 1 GPU, you can adjust the number of training GPUs to complete the experiments, but the final metrics may not align with the above, which is normal.</p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, e.g.:</p> <pre><code>python main.py -c paddlex/configs/instance_segmentation/Mask-RT-DETR-H.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/remotesensing_demo.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, and the prediction result for <code>remotesensing_demo.png</code> is as follows:  <p></p> <p></p>"},{"location":"en/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the general instance segmentation pipeline meets your requirements for inference speed and accuracy, you can proceed with development integration/deployment.</p> <ol> <li>Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/instance_segmentation.yaml</code> configuration file to your own model path:</li> </ol> <p><pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/instance_segmentation.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/remotesensing_demo.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the result visualization image\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to the General Instance Segmentation Pipline User Guide\u3002</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html","title":"PaddleX 3.0 General Object Detection Pipeline \u2014 Tutorial on Pedestrian Fall Detection","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models tailored to solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can fine-tune the models with your private data. PaddleX also provides Python APIs for easy integration into personal projects. Before proceeding, ensure you have installed PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of pedestrian fall detection.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the appropriate PaddleX pipeline based on your task scenario. For pedestrian fall detection, this falls under the General Object Detection pipeline in PaddleX. If unsure about the task-pipeline correspondence, consult the Pipeline List for capabilities of each pipeline.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipelines: locally through the PaddleX wheel package or on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:     <pre><code>paddlex --pipeline object_detection \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/fall.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Navigate to Baidu AIStudio Community, click \"Create Pipeline,\" and create a General Object Detection pipeline for a quick trial.</p> </li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After the trial, determine if the pipeline meets your expectations (including accuracy, speed, etc.). If the model's speed or accuracy is unsatisfactory, test alternative models or proceed with fine-tuning. Since the default weights (trained on the COCO dataset) are unlikely to meet the requirements for detecting pedestrian falls, you'll need to collect and annotate data for training and fine-tuning.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides 37 end-to-end object detection models. Refer to the Model List for details. Here's a benchmark of some models:</p> Model List mAP(%) GPU Inference Time(ms) CPU Inference Time(ms) Model Size(M) RT-DETR-H 56.3 100.65 8451.92 471 RT-DETR-L 53.0 27.89 841.00 125 PP-YOLOE_plus-L 52.9 29.67 700.97 200 PP-YOLOE_plus-S 43.7 8.11 137.23 31 PicoDet-L 42.6 10.09 129.32 23 PicoDet-S 29.1 3.17 13.36 5 <p>Note: The above accuracy metrics are based on the mAP(0.5:0.95) of the COCO2017 validation set. GPU inference time is measured on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In summary, models listed from top to bottom offer faster inference speeds, while those from bottom to top offer higher accuracy. This tutorial uses the PP-YOLOE_plus-S model as an example to complete the full model development process. Choose a suitable model based on your actual usage scenario, train it, evaluate the model weights within the pipeline, and finally deploy</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Pedestrian Fall Detection Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to the PaddleX format requirements to meet PaddleX's data format specifications. For data format introductions, you can refer to the PaddleX Object Detection Task Module Data Preparation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/fall_det.tar -P ./dataset\ntar -xf ./dataset/fall_det.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PP-YOLOE_plus-S.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/fall_det\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and count its basic information. Upon successful execution, the log will print out <code>Check dataset passed !</code> information, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> folder. The output directory includes visualized example images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is:</p> <p><pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 1,\n    \"train_samples\": 1224,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/fall_1168.jpg\",\n      \"check_dataset/demo_img/fall_1113.jpg\"\n    ],\n    \"val_samples\": 216,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/fall_349.jpg\",\n      \"check_dataset/demo_img/fall_394.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/fall_det\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> The above verification results indicate that the <code>check_pass</code> being <code>True</code> means the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 1, which is the number of classes that need to be passed in for subsequent training.</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 1224.</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 216.</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of training samples in this dataset.</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of validation samples in this dataset.</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all classes and generates a histogram (<code>histogram.png</code>) for visualization:  <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#43-dataset-format-conversiondataset-splitting-optional","title":"4.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can modify the configuration file or append hyperparameters for settings.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: Whether to convert the dataset format. Set to <code>True</code> to enable dataset format conversion, default is <code>False</code>.</li> <li><code>src_dataset_type</code>: If dataset format conversion is performed, the source dataset format must be specified. Available source formats are <code>LabelMe</code> and <code>VOC</code>.</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>.</li> <li><code>train_percent</code>: If dataset splitting is performed, the percentage of the training set must be set. The value should be an integer between 0 and 100, and the sum with <code>val_percent</code> must be 100.</li> <li><code>val_percent</code>: If dataset splitting is performed, the percentage of the validation set must be set. The value should be an integer between 0 and 100, and the sum with <code>train_percent</code> must be 100.</li> </ul> </li> </ul> </li> </ul> <p>Data conversion and data splitting can be enabled simultaneously. For data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths. The above parameters also support setting through appending command-line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated your dataset. To complete the training of a PaddleX model, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PP-YOLOE_plus-S.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/fall_det \\\n    -o Train.num_classes=1\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, and more, simply by modifying the configuration file or appending command-line parameters.</p> <p>Each model in PaddleX provides a configuration file for model development, which is used to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations of parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command-line parameters, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Explanation of Training Outputs:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task completed normally, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, recording changes in model metrics, loss, etc., during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdema, .pdopt.pdstate, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PP-YOLOE_plus-S.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/fall_det\n</code></pre> <p>Similar to model training, model evaluation supports setting parameters by modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating a model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command-line parameter, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#53-model-tuning","title":"5.3 Model Tuning","text":"<p>After learning about model training and evaluation, we can improve model accuracy by adjusting hyperparameters. By reasonably adjusting the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. The learning rate setting affects the speed and stability of model convergence. Therefore, when optimizing model performance, carefully consider the values of these two parameters and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the control variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs to 10 and the batch size to 8.</li> <li>Start three experiments based on the PP-YOLOE_plus-S model with learning rates of: 0.00002, 0.0001, 0.0005.</li> <li>You may find that the configuration with the highest accuracy in Experiment 2 is a learning rate of 0.0001. Based on this training hyperparameter, change the number of epochs and observe the accuracy results of different epochs. It is found that the best accuracy is basically achieved at 100 epochs.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 1 10 0.00002 8 4 GPUs 0.880 Experiment 2 10 0.0001 8 4 GPUs 0.910 Experiment 3 10 0.0005 8 4 GPUs 0.888 <p></p> <p>Changing Epochs Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 2 10 0.0001 8 4 GPUs 0.910 Experiment 2 (Increased Epochs) 50 0.0001 8 4 GPUs 0.944 Experiment 2 (Increased Epochs) 100 0.0001 8 4 GPUs 0.947 <p></p> <p>Note: The above accuracy metrics are based on the mAP(0.5:0.95) of the COCO2017 validation set. GPU inference time is measured on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, for example:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PP-YOLOE_plus-S.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/fall.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, and the prediction result for <code>fall.png</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/object_detection_fall_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the General Object Detection Pipeline meets your requirements for inference speed and precision in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/object_detection.yaml</code> configuration file to your own model path: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/object_detection.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/fall.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to General Object Detection Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html","title":"PaddleX 3.0 General Object Detection Pipeline \u2014 Tutorial for Fashion Element Detection","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models tailored to solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can fine-tune the models with your private data. PaddleX also provides Python APIs for easy integration into personal projects. Before proceeding, ensure you have installed PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of fashion element detection in clothing.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For fashion element detection, this falls under the General Object Detection pipeline in PaddleX. If unsure about the task-pipeline correspondence, consult the Pipeline List for capabilities of each pipeline.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipeline: locally through the PaddleX wheel package or on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:   <pre><code>paddlex --pipeline object_detection \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/object_detection/FashionPedia_demo.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Visit Baidu AIStudio Community, click \"Create Pipeline,\" and select the General Object Detection pipeline for a quick trial.</p> </li> </ul> <p>Quick Trial Output Example:  <p></p> <p></p> <p>After the trial, determine if the pipeline meets your expectations (including accuracy, speed, etc.). If the model's speed or accuracy is unsatisfactory, test alternative models or proceed with fine-tuning. For this tutorial, aiming to detect fashion elements in clothing, the default weights (trained on the COCO dataset) are insufficient. Data collection and annotation, followed by training and fine-tuning, are necessary.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides 37 end-to-end object detection models. Refer to the Model List for details. Below are benchmarks for some models:</p> Model List mAP(%) GPU Inference Time(ms) CPU Inference Time(ms) Model Size(M) RT-DETR-H 56.3 100.65 8451.92 471 RT-DETR-L 53.0 27.89 841.00 125 PP-YOLOE_plus-L 52.9 29.67 700.97 200 PP-YOLOE_plus-S 43.7 8.11 137.23 31 PicoDet-L 42.6 10.09 129.32 23 PicoDet-S 29.1 3.17 13.36 5 <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the COCO2017 validation set. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In summary, models with faster inference speed are placed higher in the table, while models with higher accuracy are lower. This tutorial takes the PicoDet-L model as an example to complete a full model development process. You can judge and select an appropriate model for training based on your actual usage scenarios. After training, you can evaluate the suitable model weights within the pipeline and ultimately use them in practical scenarios.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Fashion Element Detection Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For data format introductions, you can refer to the PaddleX Object Detection Task Module Data Preparation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/det_mini_fashion_pedia_coco.tar -P ./dataset\ntar -xf ./dataset/det_mini_fashion_pedia_coco.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-L.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/det_mini_fashion_pedia_coco\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Upon successful command execution, \"Check dataset passed !\" will be printed in the log, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory. The output directory includes visualized sample images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is</p> <p><pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 15,\n    \"train_samples\": 4000,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/297ea597f7dfa6d710b2e8176cb3b913.jpg\",\n      \"check_dataset/demo_img/2d8b75ce472dbebd41ca8527f0a292f3.jpg\"\n    ],\n    \"val_samples\": 800,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/40e13ebcfa281567c92fc9842510abea.jpg\",\n      \"check_dataset/demo_img/87808e379034ac2344f5132d3dccc6e6.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/det_mini_fashion_pedia_coco\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> The above verification results indicate that the dataset format meets the requirements as <code>check_pass</code> is True. The explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 15, which is the number of classes to be passed for subsequent training.</li> <li><code>attributes.train_samples</code>: The number of training samples in this dataset is 4000.</li> <li><code>attributes.val_samples</code>: The number of validation samples in this dataset is 800.</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of training samples in this dataset.</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of validation samples in this dataset.</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all classes and generates a histogram (<code>histogram.png</code>) for visualization:  <p></p> <p></p> <p>Note: Only datasets that pass the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#43-dataset-format-conversiondataset-splitting-optional","title":"4.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can modify the configuration file or append hyperparameters.</p> <p>The parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Some example explanations for the parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: Whether to convert the dataset format. Set to <code>True</code> to enable dataset format conversion. The default is <code>False</code>.</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, you need to set the source dataset format. Available source formats are <code>LabelMe</code> and <code>VOC</code>.</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting. The default is <code>False</code>.</li> <li><code>train_percent</code>: If dataset splitting is enabled, you need to set the percentage of the training set. The value should be an integer between 0 and 100, and the sum with <code>val_percent</code> should be 100.</li> <li><code>val_percent</code>: If dataset splitting is enabled, you need to set the percentage of the validation set. The value should be an integer between 0 and 100, and the sum with <code>train_percent</code> should be 100.</li> </ul> </li> </ul> </li> </ul> <p>Data conversion and data splitting can be enabled simultaneously. The original annotation files for data splitting will be renamed as <code>xxx.bak</code> in the original path. The above parameters can also be set by appending command line arguments, for example, to re-split the dataset and set the training set and validation set ratio: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, please ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-L.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/det_mini_fashion_pedia_coco \\\n    -o Train.num_classes=15\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, etc., simply by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations of parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify the card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 50: <code>-o Train.epochs_iters=50</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Training Output Explanation:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task was completed normally, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, recording model metric changes, loss changes, etc., during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdema, .pdopt.pdstate, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-L.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/det_mini_fashion_pedia_coco\n</code></pre> <p>Similar to model training, model evaluation supports modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating the model, you need to specify the model weight file path. Each configuration file has a built-in default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_model/best_model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#53-model-tuning","title":"5.3 Model Tuning","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By reasonably tuning the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to carefully consider the values of these two parameters and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 50 and the batch size at 16.</li> <li>Initiate three experiments based on the PicoDet-L model with learning rates of: 0.02, 0.04, 0.08.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 2 is a learning rate of 0.04. Based on this training hyperparameter, change the number of epochs and observe the accuracy results at different epochs, finding that the best accuracy is generally achieved at 80 epochs.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 1 50 0.02 16 4 GPUs 0.428 Experiment 2 50 0.04 16 4 GPUs 0.471 Experiment 3 50 0.08 16 4 GPUs 0.440 <p></p> <p>Epoch Variation Results:  Experiment Epochs Learning Rate batch_size Training Environment mAP@0.5 Experiment 2 50 0.04 16 4 GPUs 0.471 Reduced Epochs in Exp. 2 30 0.04 16 4 GPUs 0.425 Increased Epochs in Exp. 2 80 0.04 16 4 GPUs 0.491 Further Increased Epochs 100 0.04 16 4 GPUs 0.459 <p></p> <p>Note: This tutorial is designed for a 4-GPU setup. If you have only 1 GPU, you can adjust the number of training GPUs to complete the experiments, but the final metrics may not align with the above figures, which is normal.</p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#6-pipeline-testing","title":"6. Pipeline Testing","text":"<p>Replace the model in the pipeline with the fine-tuned model for testing, e.g.:</p> <pre><code>python main.py -c paddlex/configs/object_detection/PicoDet-L.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/object_detection/FashionPedia_demo.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, and the prediction result for <code>FashionPedia_demo.png</code> is as follows:  <p></p> <p></p>"},{"location":"en/practical_tutorials/object_detection_fashion_pedia_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the General Object Detection Pipeline meets your requirements for inference speed and precision in your production line, you can proceed directly with development integration/deployment.</p> <ol> <li>Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/object_detection.yaml</code> configuration file to your own model path:</li> </ol> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/object_detection.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/object_detection/FashionPedia_demo.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized result image\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> <p>For more parameters, please refer to General Object Detection Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html","title":"PaddleX 3.0 General OCR Pipeline \u2014 License Plate Recognition Tutorial","text":"<p>PaddleX provides a rich set of pipelines, each consisting of one or more models that work together to solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, you can also fine-tune the models with private data. PaddleX provides Python APIs to easily integrate pipelines into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with a license plate recognition task as an example.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For license plate recognition, this task falls under text detection, corresponding to PaddleX's Universal OCR pipeline. If you are unsure about the correspondence between tasks and pipelines, you can refer to the Pipeline List supported by PaddleX to understand the capabilities of relevant pipelines.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipeline: one is through the PaddleX wheel package locally, and the other is on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:     <pre><code>paddlex --pipeline OCR \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR/case1.jpg\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Go to Baidu AIStudio Community, click \"Create Pipeline\", and create a Universal OCR pipeline for a quick trial.</p> </li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After experiencing the pipeline, determine if it meets your expectations (including accuracy, speed, etc.), and whether the models included in the pipeline need further fine-tuning. If the speed or accuracy of the models does not meet expectations, select replaceable models for continued testing to determine satisfaction. If the final results are unsatisfactory, fine-tune the models.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#3-select-a-model","title":"3. Select a Model","text":"<p>PaddleX provides two end-to-end text detection models. For details, refer to the Model List. The benchmarks of the models are as follows:</p> Model List Detection Hmean(%) Recognition Avg Accuracy(%) GPU Inference Time(ms) CPU Inference Time(ms) Model Size(M) PP-OCRv4_server 82.69 79.20 22.20346 2662.158 198 PP-OCRv4_mobile 77.79 78.20 2.719474 79.1097 15 <p>Note: The above accuracy metrics are for the Detection Hmean and Recognition Avg Accuracy on PaddleOCR's self-built Chinese dataset validation set. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In short, the models listed from top to bottom have faster inference speeds, while from bottom to top, they have higher accuracy. This tutorial uses the <code>PP-OCRv4_server</code> model as an example to complete a full model development process. Depending on your actual usage scenario, choose a suitable model for training. After training, evaluate the appropriate model weights within the pipeline and use them in practical scenarios.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#4-data-preparation-and-validation","title":"4. Data Preparation and Validation","text":""},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"License Plate Recognition Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to the PaddleX format requirements to meet PaddleX's data format specifications. For information on data format, you can refer to PaddleX Text Detection/Text Recognition Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ccpd_text_det.tar -P ./dataset\ntar -xf ./dataset/ccpd_text_det.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#42-dataset-validation","title":"4.2 Dataset Validation","text":"<p>To validate the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_server_det.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ccpd_text_det\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset and collect basic information about it. Upon successful execution, the log will print \"Check dataset passed !\" information, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory, including visualized sample images and sample distribution histograms. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the validation result file is</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 5769,\n    \"train_sample_paths\": [\n      \"..\\/..\\/ccpd_text_det\\/images\\/0274305555556-90_266-204&amp;460_520&amp;548-516&amp;548_209&amp;547_204&amp;464_520&amp;460-0_0_3_25_24_24_24_26-63-89.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0126171875-90_267-294&amp;424_498&amp;486-498&amp;486_296&amp;485_294&amp;425_496&amp;424-0_0_3_24_33_32_30_31-157-29.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0371516927083-89_254-178&amp;423_517&amp;534-517&amp;534_204&amp;525_178&amp;431_496&amp;423-1_0_3_24_33_31_29_31-117-667.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/03349609375-90_268-211&amp;469_526&amp;576-526&amp;567_214&amp;576_211&amp;473_520&amp;469-0_0_3_27_31_32_29_32-174-48.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0388454861111-90_269-138&amp;409_496&amp;518-496&amp;518_138&amp;517_139&amp;410_491&amp;409-0_0_3_24_27_26_26_30-174-148.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0198741319444-89_112-208&amp;517_449&amp;600-423&amp;593_208&amp;600_233&amp;517_449&amp;518-0_0_3_24_28_26_26_26-87-268.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/3027782118055555555-91_92-186&amp;493_532&amp;574-529&amp;574_199&amp;565_186&amp;497_532&amp;493-0_0_3_27_26_30_33_32-73-336.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/034375-90_258-168&amp;449_528&amp;546-528&amp;542_186&amp;546_168&amp;449_525&amp;449-0_0_3_26_30_30_26_33-94-221.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0286501736111-89_92-290&amp;486_577&amp;587-576&amp;577_290&amp;587_292&amp;491_577&amp;486-0_0_3_17_25_28_30_33-134-122.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/02001953125-92_103-212&amp;486_458&amp;569-458&amp;569_224&amp;555_212&amp;486_446&amp;494-0_0_3_24_24_25_24_24-88-24.jpg\"\n    ],\n    \"val_samples\": 1001,\n    \"val_sample_paths\": [\n      \"..\\/..\\/ccpd_text_det\\/images\\/3056141493055555554-88_93-205&amp;455_603&amp;597-603&amp;575_207&amp;597_205&amp;468_595&amp;455-0_0_3_24_32_27_31_33-90-213.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0680295138889-88_94-120&amp;474_581&amp;623-577&amp;605_126&amp;623_120&amp;483_581&amp;474-0_0_5_24_31_24_24_24-116-518.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0482421875-87_265-154&amp;388_496&amp;530-490&amp;495_154&amp;530_156&amp;411_496&amp;388-0_0_5_25_33_33_33_33-84-104.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0347504340278-105_106-235&amp;443_474&amp;589-474&amp;589_240&amp;518_235&amp;443_473&amp;503-0_0_3_25_30_33_27_30-162-4.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0205338541667-93_262-182&amp;428_410&amp;519-410&amp;519_187&amp;499_182&amp;428_402&amp;442-0_0_3_24_26_29_32_24-83-63.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0380913628472-97_250-234&amp;403_529&amp;534-529&amp;534_250&amp;480_234&amp;403_528&amp;446-0_0_3_25_25_24_25_25-185-85.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/020598958333333334-93_267-256&amp;471_482&amp;563-478&amp;563_256&amp;546_262&amp;471_482&amp;484-0_0_3_26_24_25_32_24-102-115.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/3030323350694444445-86_131-170&amp;495_484&amp;593-434&amp;569_170&amp;593_226&amp;511_484&amp;495-11_0_5_30_30_31_33_24-118-59.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/3016158854166666667-86_97-243&amp;471_462&amp;546-462&amp;527_245&amp;546_243&amp;479_453&amp;471-0_0_3_24_30_27_24_29-98-40.jpg\",\n      \"..\\/..\\/ccpd_text_det\\/images\\/0340831163194-89_264-177&amp;412_488&amp;523-477&amp;506_177&amp;523_185&amp;420_488&amp;412-0_0_3_24_30_29_31_31-109-46.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \"\\/mnt\\/liujiaxuan01\\/new\\/new2\\/ccpd_text_det\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"TextDetDataset\"\n}\n</code></pre> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 5769;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 1001;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all categories in the dataset and plots a histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#43-dataset-splitting-optional","title":"4.3 Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to perform dataset format conversion, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, you need to set the percentage of the training set, which is an integer between 0-100, ensuring the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If re-splitting the dataset, you need to set the percentage of the validation set, which is an integer between 0-100, ensuring the sum with <code>train_percent</code> is 100;</li> </ul> </li> </ul> </li> </ul> <p>During data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths. The above parameters also support being set by appending command-line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure you have verified the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_server_det.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/ccpd_text_det\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single-machine single/multi-GPU training, etc., by modifying the configuration file or appending command line parameters.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Parameters related to model training can be set by modifying the fields under <code>Train</code> in the configuration file. Some example explanations of the parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset verification (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, please refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first 2 GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, defaulting to <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced. During model inference, static graph weights are selected by default.</p> <p>Training Output Explanation:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task completed normally, as well as the output weight metrics, related file paths, etc.;</li> <li>train.log: Training log file, recording model metric changes, loss changes, etc., during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdopt, .pdstates, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation requires only one command:</p> <pre><code>python main.py -c paddlex/configs/text_detection/PP-OCRv4_server_det.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ccpd_text_det\n</code></pre> <p>Similar to model training, model evaluation supports setting through modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command-line parameter, such as <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#53-model-tuning","title":"5.3 Model Tuning","text":"<p>After learning about model training and evaluation, we can improve the model's accuracy by adjusting hyperparameters. By reasonably adjusting the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. The setting of the learning rate is related to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to carefully consider the values of these two parameters and adjust them flexibly according to the actual situation to achieve the best training effect.</p> <p>It is recommended to follow the controlled variable method when debugging parameters:</p> <ol> <li>First, fix the number of training epochs to 10, the batch size to 8, the number of GPUs to 4, and the total batch size to 32.</li> <li>Start four experiments based on the PP-OCRv4_server_det model with learning rates of: 0.00005, 0.0001, 0.0005, 0.001.</li> <li>You can find that Experiment 4 with a learning rate of 0.001 has the highest accuracy, and by observing the validation set score, the accuracy continues to increase in the last few epochs. Therefore, increasing the number of training epochs to 20 will further improve the model accuracy.</li> </ol> <p>Learning Rate Exploration Results:  Experiment ID Learning Rate Detection Hmean (%) 1 0.00005 99.06 2 0.0001 99.55 3 0.0005 99.60 4 0.001 99.70 <p></p> <p>Next, based on a learning rate of 0.001, we can increase the number of training epochs. Comparing Experiments [4, 5] below, it can be seen that increasing the number of training epochs further improves the model accuracy.  Experiment ID Number of Training Epochs Detection Hmean (%) 4 10 99.70 5 20 99.80 <p></p> <p>Note: This tutorial is designed for 4 GPUs. If you only have 1 GPU, you can complete the experiment by adjusting the number of training GPUs, but the final metrics may not align with the above indicators, which is normal.</p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the models in the production line with the fine-tuned models for testing, for example:</p> <pre><code>paddlex --pipeline OCR \\\n        --model PP-OCRv4_server_det PP-OCRv4_server_rec \\\n        --model_dir output/best_accuracy/inference None \\\n        --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR/case1.jpg\n</code></pre> <p>This will generate prediction results under <code>./output</code>, where the prediction result for <code>case1.jpg</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/ocr_det_license_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the general OCR pipeline meets your requirements for inference speed and accuracy in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/OCR.yaml</code> configuration file to your own model path: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/OCR.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR/case1.jpg\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to the General OCR Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html","title":"PaddleX 3.0 General OCR Pipeline \u2014 Handwritten Chinese Recognition Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that collectively solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, they also support fine-tuning with private data. PaddleX provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, please refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of handwritten Chinese recognition.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For handwritten Chinese recognition, this task falls under the Text Recognition category, corresponding to PaddleX's Universal OCR Pipeline. If you are unsure about the correspondence between tasks and pipelines, you can refer to the Pipeline List for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipeline: one is through the PaddleX wheel package locally, and the other is on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:     <pre><code>paddlex --pipeline OCR \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR_rec/case.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Go to Baidu AIStudio Community, click \"Create Pipeline\", and create a Universal OCR pipeline for a quick trial.</p> </li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After experiencing the pipeline, determine if it meets your expectations (including accuracy, speed, etc.). If the pipeline's models need further fine-tuning due to unsatisfactory speed or accuracy, select alternative models for continued testing to determine satisfaction. If the final results are unsatisfactory, fine-tuning the model is necessary.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#3-select-a-model","title":"3. Select a Model","text":"<p>PaddleX provides four end-to-end OCR models. For details, refer to the Model List. Benchmarks for some models are as follows:</p> Model List Detection Hmean(%) Recognition Avg Accuracy(%) GPU Inference Time(ms) CPU Inference Time(ms) Model Size(M) PP-OCRv4_server 82.69 79.20 22.20346 2662.158 198 PP-OCRv4_mobile 77.79 78.20 2.719474 79.1097 15 <p>Note: The evaluation set is a self-built Chinese dataset by PaddleOCR, covering street scenes, web images, documents, and handwritten texts. The text recognition set contains 11,000 images, and the detection set contains 500 images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In summary, models listed from top to bottom have faster inference speeds, while those from bottom to top have higher accuracy. This tutorial uses the <code>PP-OCRv4_server</code> model as an example to complete a full model development process. Based on your actual usage scenario, choose a suitable model for training. After training, evaluate the appropriate model weights within the pipeline and use them in practical scenarios.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Handwritten Chinese Recognition Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to the PaddleX format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to PaddleX Text Detection/Text Recognition Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/handwrite_chinese_text_rec.tar -P ./dataset\ntar -xf ./dataset/handwrite_chinese_text_rec.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_server_rec.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/handwrite_chinese_text_rec\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Upon successful execution, the log will print \"Check dataset passed !\" information, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory, including visualized sample images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is: <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 23965,\n    \"train_sample_paths\": [\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/64957.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/138926.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/86760.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/83191.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/79882.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/58639.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/1187-P16_1.jpg\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/8199.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/1225-P19_9.jpg\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/train_data\\/183335.png\"\n    ],\n    \"val_samples\": 17259,\n    \"val_sample_paths\": [\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/11.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/12.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/13.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/14.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/15.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/16.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/17.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/18.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/19.png\",\n      \"..\\/..\\/handwrite_chinese_text_rec\\/test_data\\/20.png\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset\\/histogram.png\"\n  },\n  \"dataset_path\": \"\\/mnt\\/liujiaxuan01\\/new\\/new2\\/handwrite_chinese_text_rec\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"MSTextRecDataset\"\n}\n</code></pre></p> <p>In the above verification results, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 23965;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 17259;</li> <li><code>attributes.train_sample_paths</code>: The list of relative paths to the visualization images of the samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: The list of relative paths to the visualization images of the samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the distribution of sample numbers across all categories in the dataset and plots a histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#43-dataset-splitting-optional","title":"4.3 Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to convert the dataset format, default is <code>False</code>;</li> <li><code>train_percent</code>: If re-splitting the dataset, you need to set the percentage of the training set, which is an integer between 0-100, and needs to ensure that the sum with <code>val_percent</code> is 100;</li> <li><code>val_percent</code>: If re-splitting the dataset, you need to set the percentage of the validation set, which is an integer between 0-100, and needs to ensure that the sum with <code>train_percent</code> is 100;</li> </ul> </li> </ul> </li> </ul> <p>During data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths. The above parameters also support being set by appending command-line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated your dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_server_rec.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/handwrite_chinese_text_rec\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, etc., by modifying the configuration file or appending command-line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command-line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Training Output Explanation:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task completed normally, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, recording changes in model metrics and loss during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdopt, .pdstates, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/text_recognition/PP-OCRv4_server_rec.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/handwrite_chinese_text_rec\n</code></pre> <p>Similar to model training, model evaluation supports setting parameters by modifying the configuration file or appending command-line arguments.</p> <p>Note: When evaluating a model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command-line argument, e.g., <code>-o Evaluate.weight_path=./output/best_accuracy/best_accuracy.pdparams</code>.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training to avoid overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial for the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters carefully and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the method of controlled variables when debugging parameters:</p> <ol> <li>First, fix the number of training epochs at 20, the batch size at 8, select 4 GPUs, and the total batch size is 32.</li> <li>Initiate four experiments based on the PP-OCRv4_server_rec model with learning rates of: 0.001, 0.005, 0.0002, 0.0001.</li> <li>It can be observed that Experiment 3, with a learning rate of 0.0002, yields the highest accuracy, and the validation set score indicates that accuracy continues to increase in the last few epochs. Therefore, increasing the number of training epochs to 30, 50, and 80 will further improve model accuracy.</li> </ol> <p>Learning Rate Exploration Results:  Experiment ID Learning Rate Recognition Acc (%) 1 0.001 43.28 2 0.005 32.63 3 0.0002 49.64 4 0.0001 46.32 <p></p> <p>Next, based on a learning rate of 0.0002, we can increase the number of training epochs. Comparing Experiments [4, 5, 6, 7] below, it can be seen that increasing the number of training epochs further improves model accuracy.  Experiment ID Number of Training Epochs Recognition Acc (%) 4 20 49.64 5 30 52.03 6 50 54.15 7 80 54.35 <p></p> <p>Note: This tutorial is designed for 4 GPUs. If you only have 1 GPU, you can adjust the number of training GPUs to complete the experiments, but the final metrics may not align with the above indicators, which is normal.</p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, for example:</p> <pre><code>paddlex --pipeline OCR \\\n        --model PP-OCRv4_server_det PP-OCRv4_server_rec \\\n        --model_dir None output/best_accuracy/inference \\\n        --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR_rec/case.png\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, and the prediction result for <code>case.jpg</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/ocr_rec_chinese_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the general OCR pipeline meets your requirements for inference speed and accuracy in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/OCR.yaml</code> configuration file to your own model path: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/OCR.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/OCR_rec/case.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to the General OCR Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html","title":"PaddleX 3.0 General Semantic Segmentation Pipeline \u2014 Lane Line Segmentation Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, fine-tuning the models with private data is also supported. PaddleX provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Installation. This tutorial introduces the usage of the pipeline tool with an example of a lane line segmentation task.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For lane line segmentation, this falls under the category of semantic segmentation tasks, corresponding to PaddleX's Universal Semantic Segmentation Pipeline. If you are unsure about the correspondence between tasks and pipelines, you can refer to the Pipeline List for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#2-quick-start","title":"2. Quick Start","text":"<p>PaddleX offers two ways to experience the pipeline: one is through the PaddleX wheel package locally, and the other is on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience:   <pre><code>paddlex --pipeline semantic_segmentation \\\n    --input https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/semantic_segmentation/makassaridn-road_demo.png\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Go to Baidu AIStudio Community, click \"Create Pipeline\", and create a Universal Semantic Segmentation pipeline for a quick trial.</p> </li> </ul> <p>Quick trial output example:  <p></p> <p></p> <p>After experiencing the pipeline, determine if it meets your expectations (including accuracy, speed, etc.). If the model's speed or accuracy does not meet your requirements, you can select alternative models for further testing. If the final results are unsatisfactory, you may need to fine-tune the model. This tutorial aims to produce a model that segments lane lines, and the default weights (trained on the Cityscapes dataset) cannot meet this requirement. Therefore, you need to collect and annotate data for training and fine-tuning.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides 18 end-to-end semantic segmentation models. For details, refer to the Model List. Some model benchmarks are as follows:</p> Model List mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) OCRNet_HRNet-W48 82.15 87.97 2180.76 270 PP-LiteSeg-T 77.04 5.98 140.02 31 <p>Note: The above accuracy metrics are measured on the Cityscapes dataset. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p> <p>In short, models listed from top to bottom have faster inference speeds, while those from bottom to top have higher accuracy. This tutorial uses the PP-LiteSeg-T model as an example to complete the full model development process. You can choose a suitable model for training based on your actual usage scenario, evaluate the appropriate model weights within the pipeline, and finally use them in practical scenarios.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>This tutorial uses the \"Lane Line Segmentation Dataset\" as an example dataset. You can obtain the example dataset using the following commands. If you use your own annotated dataset, you need to adjust it according to PaddleX's format requirements to meet PaddleX's data format specifications. For an introduction to data formats, you can refer to PaddleX Semantic Segmentation Task Module Data Annotation Tutorial.</p> <p>Dataset acquisition commands: <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/semantic-segmentation-makassaridn-road-dataset.tar -P ./dataset\ntar -xf ./dataset/semantic-segmentation-makassaridn-road-dataset.tar -C ./dataset/\n</code></pre></p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#42-dataset-verification","title":"4.2 Dataset Verification","text":"<p>To verify the dataset, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/semantic-segmentation-makassaridn-road-dataset\n</code></pre> <p>After executing the above command, PaddleX will verify the dataset and collect basic information about it. Upon successful execution, the log will print \"Check dataset passed !\" information, and relevant outputs will be saved in the current directory's <code>./output/check_dataset</code> directory, including visualized sample images and sample distribution histograms. The verification result file is saved in <code>./output/check_dataset_result.json</code>, and the specific content of the verification result file is</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"num_classes\": 4,\n    \"train_samples\": 300,\n    \"train_sample_paths\": [\n      \"check_dataset/demo_img/20220311151733_0060_040.jpg\",\n      \"check_dataset/demo_img/20220311153115_0023_039.jpg\"\n    ],\n    \"val_samples\": 74,\n    \"val_sample_paths\": [\n      \"check_dataset/demo_img/20220311152033_0060_007.jpg\",\n      \"check_dataset/demo_img/20220311144930_0060_026.jpg\"\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/semantic-segmentation-makassaridn-road-dataset\",\n  \"show_type\": \"image\",\n  \"dataset_type\": \"COCODetDataset\"\n}\n</code></pre> <p>In the verification results above, <code>check_pass</code> being <code>True</code> indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.num_classes</code>: The number of classes in this dataset is 4, which is the number of classes that need to be passed for subsequent training;</li> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 300;</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 74;</li> <li><code>attributes.train_sample_paths</code>: A list of relative paths to the visualization images of samples in the training set of this dataset;</li> <li><code>attributes.val_sample_paths</code>: A list of relative paths to the visualization images of samples in the validation set of this dataset;</li> </ul> <p>Additionally, the dataset verification also analyzes the sample distribution across all classes and plots a histogram (<code>histogram.png</code>):  <p></p> <p></p> <p>Note: Only data that passes verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#43-dataset-format-conversion-dataset-splitting-optional","title":"4.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can set it by modifying the configuration file or appending hyperparameters.</p> <p>Parameters related to dataset verification can be set by modifying the fields under <code>CheckDataset</code> in the configuration file. Examples of some parameters in the configuration file are as follows:</p> <ul> <li><code>CheckDataset</code>:<ul> <li><code>convert</code>:<ul> <li><code>enable</code>: Whether to convert the dataset format. Set to <code>True</code> to enable dataset format conversion, default is <code>False</code>;</li> <li><code>src_dataset_type</code>: If dataset format conversion is enabled, the source dataset format must be set. Available source formats are <code>LabelMe</code> and <code>VOC</code>;</li> </ul> </li> <li><code>split</code>:<ul> <li><code>enable</code>: Whether to re-split the dataset. Set to <code>True</code> to enable dataset splitting, default is <code>False</code>;</li> <li><code>train_percent</code>: If dataset splitting is enabled, the percentage of the training set must be set. The type is any integer between 0-100, and the sum with <code>val_percent</code> must be 100;</li> <li><code>val_percent</code>: If dataset splitting is enabled, the percentage of the validation set must be set. The type is any integer between 0-100, and the sum with <code>train_percent</code> must be 100;</li> </ul> </li> </ul> </li> </ul> <p>Data conversion and splitting can be enabled simultaneously. For data splitting, the original annotation files will be renamed to <code>xxx.bak</code> in their original paths. These parameters also support being set by appending command-line arguments, for example, to re-split the dataset and set the training and validation set ratios: <code>-o CheckDataset.split.enable=True -o CheckDataset.split.train_percent=80 -o CheckDataset.split.val_percent=20</code>.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated your dataset. To complete the training of a PaddleX model, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=train \\\n    -o Global.dataset_dir=./dataset/semantic-segmentation-makassaridn-road-dataset \\\n    -o Train.num_classes=4\n</code></pre> <p>PaddleX supports modifying training hyperparameters, single/multi-GPU training, and more, simply by modifying the configuration file or appending command line arguments.</p> <p>Each model in PaddleX provides a configuration file for model development, which is used to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>Global</code>:<ul> <li><code>mode</code>: Mode, supports dataset validation (<code>check_dataset</code>), model training (<code>train</code>), and model evaluation (<code>evaluate</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>. For multi-GPU training, specify card numbers, e.g., <code>gpu:0,1,2,3</code>;</li> </ul> </li> <li><code>Train</code>: Training hyperparameter settings;<ul> <li><code>epochs_iters</code>: Number of training iterations;</li> <li><code>learning_rate</code>: Training learning rate;</li> </ul> </li> </ul> <p>For more hyperparameter introductions, refer to PaddleX General Model Configuration File Parameter Explanation.</p> <p>Note: - The above parameters can be set by appending command line arguments, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first two GPUs for training: <code>-o Global.device=gpu:0,1</code>; setting the number of training iterations to 5000: <code>-o Train.epochs_iters=5000</code>. - During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file. - PaddleX shields you from the concepts of dynamic graph weights and static graph weights. During model training, both dynamic and static graph weights are produced, and static graph weights are selected by default for model inference.</p> <p>Training Outputs Explanation:</p> <p>After completing model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <ul> <li>train_result.json: Training result record file, recording whether the training task completed normally, as well as the output weight metrics, relevant file paths, etc.;</li> <li>train.log: Training log file, recording changes in model metrics, loss, etc. during training;</li> <li>config.yaml: Training configuration file, recording the hyperparameter configuration for this training session;</li> <li>.pdparams, .pdema, .pdopt.pdstate, .pdiparams, .pdmodel: Model weight-related files, including network parameters, optimizer, EMA, static graph network parameters, static graph network structure, etc.;</li> </ul>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weight file on the validation set to verify the model's accuracy. To evaluate a model using PaddleX, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/semantic-segmentation-makassaridn-road-dataset\n</code></pre> <p>Similar to model training, model evaluation supports setting parameters by modifying the configuration file or appending command line arguments.</p> <p>Note: When evaluating a model, you need to specify the model weight file path. Each configuration file has a default weight save path. If you need to change it, simply set it by appending a command line argument, e.g., <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can enhance model accuracy by adjusting hyperparameters. By carefully tuning the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to consider the values of these two parameters prudently and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>It is recommended to follow the method of controlled variables when debugging parameters:</p> <ol> <li>First, fix the number of training iterations at 5000 and the batch size at 2.</li> <li>Initiate three experiments based on the PP-LiteSeg-T model, with learning rates of: 0.006, 0.008, and 0.01.</li> <li>It can be observed that the configuration with the highest accuracy in Experiment 2 is a learning rate of 0.008. Based on this training hyperparameter, change the number of training epochs and observe the accuracy results of different iterations, finding that the optimal accuracy is basically achieved at 80000 iterations.</li> </ol> <p>Learning Rate Exploration Results:  Experiment Iterations Learning Rate batch_size Training Environment mIoU Experiment 1 5000 0.006 2 4 GPUs 0.623 Experiment 2 5000 0.008 2 4 GPUs 0.629 Experiment 3 5000 0.01 2 4 GPUs 0.619 <p></p> <p>Changing Epoch Results:  Experiment Iterations Learning Rate batch_size Training Environment mIoU Experiment 2 5000 0.008 2 4 GPUs 0.629 Experiment 2 with fewer epochs 10000 0.008 2 4 GPUs 0.773 Experiment 2 with more epochs 40000 0.008 2 4 GPUs 0.855 Experiment 2 with more epochs 80000 0.008 2 4 GPUs 0.863 <p></p> <p>Note: This tutorial is designed for 4 GPUs. If you have only 1 GPU, you can adjust the number of training GPUs to complete the experiment, but the final metrics may not align with the above indicators, which is normal.</p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, for example:</p> <pre><code>python main.py -c paddlex/configs/semantic_segmentation/PP-LiteSeg-T.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"output/best_model/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/semantic_segmentation/makassaridn-road_demo.png\"\n</code></pre> <p>The prediction results will be generated under <code>./output</code>, where the prediction result for <code>makassaridn-road_demo.png</code> is shown below:  <p></p> <p></p>"},{"location":"en/practical_tutorials/semantic_segmentation_road_tutorial.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the general semantic segmentation pipeline meets your requirements for inference speed and accuracy in the production line, you can proceed directly with development integration/deployment. 1. Directly apply the trained model in your Python project by referring to the following sample code, and modify the <code>Pipeline.model</code> in the <code>paddlex/pipelines/semantic_segmentation.yaml</code> configuration file to your own model path: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"paddlex/pipelines/semantic_segmentation.yaml\")\noutput = pipeline.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/application/semantic_segmentation/makassaridn-road_demo.png\")\nfor res in output:\n    res.print() # Print the structured output of the prediction\n    res.save_to_img(\"./output/\") # Save the visualized image of the result\n    res.save_to_json(\"./output/\") # Save the structured output of the prediction\n</code></pre> For more parameters, please refer to General Semantic Segmentation Pipeline Usage Tutorial.</p> <ol> <li> <p>Additionally, PaddleX offers three other deployment methods, detailed as follows:</p> </li> <li> <p>high-performance inference: In actual production environments, many applications have stringent standards for deployment strategy performance metrics (especially response speed) to ensure efficient system operation and smooth user experience. To this end, PaddleX provides high-performance inference plugins aimed at deeply optimizing model inference and pre/post-processing for significant end-to-end process acceleration. For detailed high-performance inference procedures, please refer to the PaddleX High-Performance Inference Guide.</p> </li> <li>Service-Oriented Deployment: Service-oriented deployment is a common deployment form in actual production environments. By encapsulating inference functions as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving cost-effective service-oriented deployment of production lines. For detailed service-oriented deployment procedures, please refer to the PaddleX Service-Oriented Deployment Guide.</li> <li>Edge Deployment: Edge deployment is a method that places computing and data processing capabilities directly on user devices, allowing devices to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the PaddleX Edge Deployment Guide.</li> </ol> <p>You can select the appropriate deployment method for your model pipeline according to your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html","title":"PaddleX 3.0 Time Series Anomaly Detection Pipeline \u2014 Equipment Anomaly Detection Application Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models that can solve specific scenario-based tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, fine-tuning the models with private data is also supported. PaddleX provides Python APIs for easy integration of pipelines into personal projects. Before use, you need to install PaddleX. For installation instructions, please refer to PaddleX Local Installation Tutorial. This tutorial introduces the usage of the pipeline tool with an example of detecting anomalies in equipment nodes.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. This task aims to identify and mark abnormal behaviors or states in equipment nodes, helping enterprises and organizations promptly detect and resolve issues in application server nodes, thereby improving system reliability and availability. Recognizing this as a time series anomaly detection task, we will use PaddleX's Time Series Anomaly Detection Pipeline. If you are unsure about the correspondence between tasks and pipelines, you can refer to the PaddleX Pipeline List (CPU/GPU) for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#2-quick-experience","title":"2. Quick Experience","text":"<p>PaddleX offers two ways to experience its capabilities: locally on your machine or on the Baidu AIStudio Community.</p> <ul> <li>Local Experience: <pre><code>from paddlex import create_model\nmodel = create_model(\"PatchTST_ad\")\noutput = model.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_ad.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre></li> <li>AIStudio Community Experience: Visit the Official Time Series Anomaly Detection Application to experience the capabilities of time series anomaly detection tasks. Note: Due to the tight correlation between time series data and scenarios, the online experience of official models for time series tasks is tailored to a specific scenario and is not a general solution. Therefore, the experience mode does not support using arbitrary files to evaluate the official model's performance. However, after training a model with your own scenario data, you can select your trained model and use data from the corresponding scenario for online experience.</li> </ul>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides five end-to-end time series anomaly detection models. For details, refer to the Model List. The benchmarks of these models are as follows:</p> Model Name Precision Recall F1-Score Model Size (M) Description DLinear_ad 0.9898 0.9396 0.9641 72.8K A simple, efficient, and easy-to-use time series anomaly detection model Nonstationary_ad 0.9855 0.8895 0.9351 1.5MB A transformer-based model optimized for anomaly detection in non-stationary time series AutoEncoder_ad 0.9936 0.8436 0.9125 32K A classic autoencoder-based model that is efficient and easy to use for time series anomaly detection PatchTST_ad 0.9878 0.9070 0.9457 164K A high-precision time series anomaly detection model that balances local patterns and global dependencies TimesNet_ad 0.9837 0.9480 0.9656 732K A highly adaptive and high-precision time series anomaly detection model through multi-period analysis <p>Note: The above accuracy metrics are measured on the PSM dataset with a time series length of 100.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#4-data-preparation-and-validation","title":"4. Data Preparation and Validation","text":""},{"location":"en/practical_tutorials/ts_anomaly_detection.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>To demonstrate the entire process of time series anomaly detection, we will use the publicly available MSL (Mars Science Laboratory) dataset for model training and validation. The PSM (Planetary Science Mission) dataset, sourced from NASA, comprises 55 dimensions and includes telemetry anomaly data reported by the spacecraft's monitoring system for unexpected event anomalies (ISA). With its practical application background, it better reflects real-world anomaly scenarios and is commonly used to test and validate the performance of time series anomaly detection models. This tutorial will perform anomaly detection based on this dataset.</p> <p>We have converted the dataset into a standard data format, and you can obtain a sample dataset using the following command. For an introduction to the data format, please refer to the Time Series Anomaly Detection Module Development Tutorial.</p> <p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/timeseries_anomaly_detection/msl.tar -P ./dataset\ntar -xf ./dataset/msl.tar -C ./dataset/\n</code></pre> <p>Data Considerations  * Time series anomaly detection is an unsupervised learning task, thus labeled training data is not required. The collected training samples should ideally consist solely of normal data, i.e., devoid of anomalies, with the label column in the training set set to 0 or, alternatively, the label column can be omitted entirely. For the validation set, to assess accuracy, labeling is necessary. Points that are anomalous at a particular timestamp should have their labels set to 1, while normal points should have labels of 0.  * Handling Missing Values: To ensure data quality and integrity, missing values can be imputed based on expert knowledge or statistical methods.  * Non-Repetitiveness: Ensure that data is collected in chronological order by row, with no duplication of timestamps.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#42-data-validation","title":"4.2 Data Validation","text":"<p>Data Validation can be completed with just one command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/PatchTST_ad.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/msl\n</code></pre> After executing the above command, PaddleX will validate the dataset, summarize its basic information, and print <code>Check dataset passed !</code> in the log if the command runs successfully. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the current directory's <code>./output/check_dataset</code> directory, including example time series data and class distribution histograms.</p> <p><pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 58317,\n    \"train_table\": [\n      [\n        \"timestamp\",\n        \"0\",\n        \"1\",\n        \"2\",\n        \"...\"\n      ],\n      [\n        \"...\"\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"\"\n  },\n  \"dataset_path\": \"./dataset/msl\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSADDataset\"\n}\n</code></pre> The above verification results have omitted some data parts. <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 58317.</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 73729.</li> <li><code>attributes.train_table</code>: Sample data rows from the training set of this dataset.</li> <li><code>attributes.val_table</code>: Sample data rows from the validation set of this dataset.</li> </ul> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#43-dataset-format-conversiondataset-splitting-optional","title":"4.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, refer to Section 4.1.3 in the Time Series Anomaly Detection Module Development Tutorial.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/ts_anomaly_detection.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have verified the dataset. To complete PaddleX model training, simply use the following command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/PatchTST_ad.yaml \\\n-o Global.mode=train \\\n-o Global.dataset_dir=./dataset/msl \\\n-o Train.epochs_iters=5 \\\n-o Train.batch_size=16 \\\n-o Train.learning_rate=0.0001 \\\n-o Train.time_col=timestamp \\\n-o Train.feature_cols=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54 \\\n-o Train.freq=1 \\\n-o Train.label_col=label \\\n-o Train.seq_len=96\n</code></pre> PaddleX supports modifying training hyperparameters and single-machine single-GPU training (time series models only support single-GPU training). Simply modify the configuration file or append command-line parameters.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example explanations for parameters in the configuration file are as follows:</p> <ul> <li><code>Global</code>:</li> <li><code>mode</code>: Mode, supporting dataset verification (<code>check_dataset</code>), model training (<code>train</code>), model evaluation (<code>evaluate</code>), and single-case testing (<code>predict</code>).</li> <li><code>device</code>: Training device, options include <code>cpu</code> and <code>gpu</code>. You can check the models supported on different devices in the PaddleX Model List (CPU/GPU) document at the same level directory.</li> <li><code>Train</code>: Training hyperparameter settings;</li> <li><code>epochs_iters</code>: Number of training epochs.</li> <li><code>learning_rate</code>: Training learning rate.</li> <li><code>batch_size</code>: Training batch size for a single GPU.</li> <li><code>time_col</code>: Time column, set the column name of the time series dataset's time column based on your data.</li> <li><code>feature_cols</code>: Feature variables indicating variables related to whether the device is abnormal.</li> <li><code>freq</code>: Frequency of the time series dataset.</li> <li><code>input_len</code>: The length of the time series input to the model. The time series will be sliced according to this length, and the model will predict whether there is an anomaly in this segment of the time series for that length. The recommended input length should be considered in the context of the actual scenario. In this tutorial, the input length is 96, which means we hope to predict whether there are anomalies at 96 time points.</li> <li><code>label</code>: Represents the number indicating whether a time point in the time series is abnormal. Anomalous points are labeled as 1, and normal points are labeled as 0. In this tutorial, the anomaly monitoring dataset uses label for this purpose.</li> </ul> <p>For more introductions to hyperparameters, please refer to PaddleX Time Series Task Model Configuration File Parameter Instructions. The above parameters can be set by appending command-line arguments. For example, to specify the mode as model training: <code>-o Global.mode=train</code>; to specify the first GPU for training: <code>-o Global.device=gpu:0</code>; to set the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>.</p>  More Details (Click to Expand)   <ul> <li>During the model training process, PaddleX automatically saves the model weight files, with the default directory being output. If you need to specify a different save path, you can configure it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts away the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced simultaneously. By default, static graph weights are selected for inference.</li> <li>When training other models, you need to specify the corresponding configuration file. The correspondence between models and configuration files can be found in the PaddleX Model List (CPU/GPU)</li> </ul> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p>Explanation of Training Outputs:</p> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p><code>train_result.json</code>: A training result record file that logs whether the training task was completed normally, as well as the output weight metrics, relevant file paths, etc. <code>train.log</code>: A training log file that records changes in model metrics, loss values, and other information during the training process. <code>config.yaml</code>: The training configuration file that records the hyperparameter configurations for this training session. <code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference*</code>: Files related to model weights, including network parameters, optimizers, static graph network parameters, etc.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation requires just one command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/PatchTST_ad.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/msl\n</code></pre> Similar to model training, model evaluation supports setting through modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command-line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, typically, the following outputs are generated:</p> <p>Upon completion of model evaluation, <code>evaluate_result.json</code> will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully, and the model's evaluation metrics, including F1, recall, and precision.</p>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#53-model-tuning","title":"5.3 Model Tuning","text":"<p>After learning about model training and evaluation, we can improve the model's accuracy by adjusting hyperparameters. By reasonably adjusting the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. The setting of the learning rate is related to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to carefully consider the values of these two parameters and adjust them flexibly based on actual conditions to achieve the best training effect.</p> <p>Based on the method of controlled variables, we can adopt the following approach for hyperparameter tuning in time-series anomaly detection:</p> <p>It is recommended to follow the method of controlled variables when debugging parameters:</p> <ol> <li>First, fix the number of training epochs to 5, batch size to 16, and input length to 96.</li> <li>Initiate three experiments based on the PatchTST_ad model with learning rates of: 0.0001, 0.0005, 0.001.</li> <li>It can be found that Experiment 3 has the highest accuracy, with a learning rate of 0.001. Therefore, fix the learning rate at 0.001 and try increasing the number of training epochs to 20.</li> <li>It can be found that the accuracy of Experiment 4 is the same as that of Experiment 3, indicating that there is no need to further increase the number of training epochs.</li> </ol> <p>Learning Rate Exploration Results:</p> Experiment Epochs Learning Rate Batch Size Input Length Training Environment Validation F1 Score (%) Experiment 1 5 0.0001 16 96 1 GPU 79.5 Experiment 2 5 0.0005 16 96 1 GPU 80.1 Experiment 3 5 0.001 16 96 1 GPU 80.9 <p>Increasing Training Epochs Results:</p> Experiment Epochs Learning Rate Batch Size Input Length Training Environment Validation F1 Score (%) Experiment 3 5 0.0005 16 96 1 GPU 80.9 Experiment 4 20 0.0005 16 96 1 GPU 80.9"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model for testing, using the test file for prediction:</p> <p><pre><code>python main.py -c paddlex/configs/ts_anomaly_detection/PatchTST_ad.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/timeseries_anomaly_detection/test.csv\"\n</code></pre> Similar to model training, the following steps are required:</p> <ul> <li>Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>PatchTST_ad.yaml</code>)</li> <li>Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code></li> <li>Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul>"},{"location":"en/practical_tutorials/ts_anomaly_detection.html#7-integrationdeployment","title":"7. Integration/Deployment","text":"<p>If the general-purpose time series anomaly detection pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <ol> <li> <p>If you need to apply the general-purpose time series anomaly detection pipeline directly in your Python project, you can refer to the following sample code: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"ts_anomaly_detection\")\noutput = pipeline.predict(\"pre_ts.csv\")\nfor res in output:\n    res.print()\n    res.save_to_csv(\"./output/\")\n</code></pre> For more parameters, please refer to the Time Series Anomaly Detection Pipeline Usage Tutorial</p> </li> <li> <p>Additionally, PaddleX's time series anomaly detection pipeline also offers a service-oriented deployment method, detailed as follows:</p> </li> </ol> <p>Service-Oriented Deployment: This is a common deployment form in actual production environments. By encapsulating the inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed instructions on service-oriented deployment, please refer to the PaddleX Service-Oriented Deployment Guide. You can choose the appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/ts_classification.html","title":"PaddleX 3.0 Time Series Classification Pipeline \u2014 Heartbeat Monitoring Time Series Classification Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models tailored to solve specific scenario-based tasks. All PaddleX pipelines support quick trials, and if the results do not meet expectations, they also allow fine-tuning with private data. PaddleX provides Python APIs for easy integration into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Local Installation Guide. This tutorial introduces the usage of the pipeline tool with an example of heartbeat time series data classification.</p>"},{"location":"en/practical_tutorials/ts_classification.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. For this task, the goal is to train a time series classification model based on heartbeat monitoring data to classify heartbeat time series conditions. Recognizing this as a time series classification task, we select PaddleX's Time Series Classification Pipeline. If unsure about the task-pipeline correspondence, consult the PaddleX Pipeline List (CPU/GPU) for pipeline capabilities.</p>"},{"location":"en/practical_tutorials/ts_classification.html#2-quick-experience","title":"2. Quick Experience","text":"<p>PaddleX offers two ways to experience its capabilities: locally or on the Baidu AIStudio Community.</p> <ul> <li>Local Experience: <pre><code>from paddlex import create_model\nmodel = create_model(\"TimesNet_cls\")\noutput = model.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_cls.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre></li> <li>AIStudio Community Experience: Access the Official Time Series Classification Application to experience time series classification capabilities.</li> </ul> <p>Note: Due to the tight coupling between time series data and scenarios, the online experience of official models is tailored to a specific scenario and not a universal solution. It does not support arbitrary files for model effect evaluation. However, after training a model with your scenario data, you can select your trained model and use corresponding scenario data for online experience.</p>"},{"location":"en/practical_tutorials/ts_classification.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides a time series classification model. Refer to the Model List for details. The model benchmark is as follows:</p> Model Name Acc (%) Model Size (M) Description TimesNet_cls 87.5 792K TimesNet is an adaptive and high-precision time series classification model through multi-cycle analysis <p>Note: The evaluation set for the above accuracy metrics is UWaveGestureLibrary.</p>"},{"location":"en/practical_tutorials/ts_classification.html#4-data-preparation-and-verification","title":"4. Data Preparation and Verification","text":""},{"location":"en/practical_tutorials/ts_classification.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>To demonstrate the entire time series classification process, we will use the public Heartbeat Dataset for model training and validation. The Heartbeat Dataset is part of the UEA Time Series Classification Archive, addressing the practical task of heartbeat monitoring for medical diagnosis. The dataset comprises multiple time series groups, with each data point consisting of a label variable, group ID, and 61 feature variables. This dataset is commonly used to test and validate the performance of time series classification prediction models.</p> <p>We have converted the dataset into a standard format, which can be obtained using the following commands. For data format details, refer to the Time Series Classification Module Development Tutorial.</p> <p>Dataset Acquisition Command:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/data/ts_classify_examples.tar -P ./dataset\ntar -xf ./dataset/ts_classify_examples.tar -C ./dataset/\n</code></pre> <p>Data Considerations   * Based on collected real data, clarify the classification objectives of the time series data and define corresponding classification labels. For example, in stock price classification, labels might be \"Rise\" or \"Fall.\" For a time series that is \"Rising\" over a period, it can be considered a sample (group), where each time point in this period shares a common group_id.   * Uniform Time Series Length: Ensure that the length of the time series for each group is consistent. Missing Value Handling: To guarantee the quality and integrity of the data, missing values can be imputed based on expert experience or statistical methods.   * Non-Duplication: Ensure that the data is collected row by row in chronological order, with no duplication of the same time point.</p>"},{"location":"en/practical_tutorials/ts_classification.html#42-data-validation","title":"4.2 Data Validation","text":"<p>Data Validation can be completed with just one command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples\n</code></pre> After executing the above command, PaddleX will validate the dataset, summarize its basic information, and print <code>Check dataset passed !</code> in the log if the command runs successfully. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the current directory's <code>./output/check_dataset</code> directory, including example time series data and class distribution histograms.</p> <p><pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 82620,\n    \"train_table\": [\n      [ ...\n    ],\n    ],\n    \"val_samples\": 83025,\n    \"val_table\": [\n      [ ...\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"check_dataset/histogram.png\"\n  },\n  \"dataset_path\": \"./dataset/ts_classify_examples\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSCLSDataset\"\n}\n</code></pre> The above verification results have omitted some data parts. <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 58317.</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 73729.</li> <li><code>attributes.train_table</code>: Sample data rows from the training set of this dataset.</li> <li><code>attributes.val_table</code>: Sample data rows from the validation set of this dataset.</li> </ul> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/ts_classification.html#43-dataset-format-conversion-dataset-splitting-optional","title":"4.3 Dataset Format Conversion / Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, please refer to Section 4.1.3 in the Time Series Classification Module Development Tutorial.</p>"},{"location":"en/practical_tutorials/ts_classification.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/ts_classification.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n-o Global.mode=train \\\n-o Global.dataset_dir=./dataset/ts_classify_examples \\\n-o Train.epochs_iters=5 \\\n-o Train.batch_size=16 \\\n-o Train.learning_rate=0.0001 \\\n-o Train.time_col=time \\\n-o Train.target_cols=dim_0,dim_1,dim_2 \\\n-o Train.freq=1 \\\n-o Train.group_id=group_id \\\n-o Train.static_cov_cols=label\n</code></pre> PaddleX supports modifying training hyperparameters and single-machine single-GPU training (time-series models only support single-GPU training). Simply modify the configuration file or append command-line parameters.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>Global</code>:</li> <li><code>mode</code>: Mode, supports dataset validation (<code>check_dataset</code>), model training (<code>train</code>), model evaluation (<code>evaluate</code>), and single-case testing (<code>predict</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>. Check the Model Support List for models supported on different devices.</li> <li><code>Train</code>: Training hyperparameter settings;</li> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> <li><code>batch_size</code>: Training batch size per GPU;</li> <li><code>time_col</code>: Time column, set the column name of the time series dataset's time column based on your data;</li> <li><code>target_cols</code>: Set the column names of the target variables of the time series dataset based on your data. Can be multiple, separated by commas. Generally, the more relevant the target variables are to the prediction target, the higher the model accuracy. In this tutorial, the heartbeat monitoring dataset has 61 feature variables for the time column, such as <code>dim_0, dim_1</code>, etc.;</li> <li><code>freq</code>: Frequency, set the time frequency based on your data, e.g., 1min, 5min, 1h;</li> <li><code>group_id</code>: A group ID represents a time series sample, and time series sequences with the same ID form a sample. Set the column name of the specified group ID based on your data, e.g., <code>group_id</code>.</li> <li><code>static_cov_cols</code>: Represents the category ID column of the time series. The same sample has the same label. Set the column name of the category based on your data, e.g., <code>label</code>. For more hyperparameter introductions, please refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul> <p>Note:</p> <ul> <li>The above parameters can be set by appending command-line parameters, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first GPU for training: <code>-o Global.device=gpu:0</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>.</li> <li>During model training, PaddleX automatically saves model weight files, with the default being <code>output</code>. To specify a save path, use the <code>-o Global.output</code> field in the configuration file.</li> </ul>  More Details (Click to Expand)   <ul> <li>During the model training process, PaddleX automatically saves the model weight files, with the default directory being output. If you need to specify a different save path, you can configure it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts away the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced simultaneously. By default, static graph weights are selected for inference.</li> <li>When training other models, you need to specify the corresponding configuration file. The correspondence between models and configuration files can be found in the PaddleX Model List (CPU/GPU)</li> </ul> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p>Explanation of Training Outputs:</p> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p><code>train_result.json</code>: A training result record file that logs whether the training task was completed normally, as well as the output weight metrics, relevant file paths, etc. <code>train.log</code>: A training log file that records changes in model metrics, loss values, and other information during the training process. <code>config.yaml</code>: The training configuration file that records the hyperparameter configurations for this training session. <code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference*</code>: Files related to model weights, including network parameters, optimizers, static graph network parameters, etc.</p>"},{"location":"en/practical_tutorials/ts_classification.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation requires just one command:</p> <pre><code>    python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/ts_classify_examples \\\n    -o Evaluate.weight_path=./output/best_model/model.pdparams\n</code></pre> <p>Similar to model training, model evaluation supports setting through modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command-line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, typically, the following outputs are generated:</p> <p>Upon completion of model evaluation, <code>evaluate_result.json</code> will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully, and the model's evaluation metrics, including Acc Top1.</p>"},{"location":"en/practical_tutorials/ts_classification.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>By adjusting the number of training epochs appropriately, you can control the depth of model training to avoid overfitting or underfitting. Meanwhile, the setting of the learning rate is crucial for the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to carefully consider the values of these two parameters and adjust them flexibly based on actual conditions to achieve the best training results.</p> <p>Using the method of controlled variables, we can start with a fixed, relatively small number of epochs in the initial stage and adjust the learning rate multiple times to find an optimal learning rate. After that, we can increase the number of training epochs to further improve the results. Below, we introduce the parameter tuning method for time series classification in detail:</p> <p>It is recommended to follow the method of controlled variables when debugging parameters:</p> <ol> <li>First, fix the number of training epochs to 5 and the batch size to 16.</li> <li>Launch three experiments based on the TimesNet_cls model with learning rates of 0.00001, 0.0001, and 0.001, respectively.</li> <li>It can be found that Experiment 3 has the highest accuracy. Therefore, fix the learning rate at 0.001 and try increasing the number of training epochs to 30. Note: Due to the built-in early stopping mechanism for time series tasks, training will automatically stop if the validation set accuracy does not improve after 10 patience epochs. If you need to change the patience epochs for early stopping, you can manually modify the value of the <code>patience</code> hyperparameter in the configuration file.</li> </ol> <p>Learning Rate Exploration Results:</p> Experiment Epochs Learning Rate Batch Size Training Environment Validation Accuracy Experiment 1 5 0.00001 16 1 GPU 72.20% Experiment 2 5 0.0001 16 1 GPU 72.20% Experiment 3 5 0.001 16 1 GPU 73.20% <p>Results of Increasing Training Epochs:</p> Experiment Epochs Learning Rate Batch Size Training Environment Validation Accuracy Experiment 3 5 0.001 16 1 GPU 73.20% Experiment 4 30 0.001 16 1 GPU 75.10%"},{"location":"en/practical_tutorials/ts_classification.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Set the model directory to the trained model for testing, using the test file to perform predictions:</p> <p><pre><code>python main.py -c paddlex/configs/ts_classification/TimesNet_cls.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=\"https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/timeseries_classification/test.csv\"\n</code></pre> Similar to model training and evaluation, the following steps are required:</p> <ul> <li>Specify the path to the <code>.yaml</code> configuration file of the model (here it is <code>TimesNet_cls.yaml</code>)</li> <li>Specify the mode as model inference prediction: <code>-o Global.mode=predict</code></li> <li>Specify the path to the model weights: <code>-o Predict.model_dir=\"./output/inference\"</code></li> <li>Specify the path to the input data: <code>-o Predict.input=\"...\"</code></li> </ul> <p>Other related parameters can be set by modifying the fields under <code>Global</code> and <code>Predict</code> in the <code>.yaml</code> configuration file. For details, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</p>"},{"location":"en/practical_tutorials/ts_classification.html#7-development-integrationdeployment","title":"7. Development Integration/Deployment","text":"<p>If the general time series classification pipeline meets your requirements for inference speed and accuracy, you can directly proceed with development integration/deployment.</p> <ol> <li>If you need to directly apply the general time series classification pipeline in your Python project, you can refer to the following sample code:</li> </ol> <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"ts_classification\")\noutput = pipeline.predict(\"pre_ts.csv\")\nfor res in output:\n    res.print() # \u200b\u6253\u5370\u200b\u9884\u6d4b\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u8f93\u51fa\u200b\n    res.save_to_csv(\"./output/\") # \u200b\u4fdd\u5b58\u200bcsv\u200b\u683c\u5f0f\u200b\u7ed3\u679c\u200b\n</code></pre> <p>For more parameters, please refer to the Time Series Classification Pipeline Usage Tutorial</p> <ol> <li>Additionally, PaddleX's time series classification pipeline also offers a service-oriented deployment method, detailed as follows:</li> </ol> <p>Service-Oriented Deployment: This is a common deployment form in actual production environments. By encapsulating the inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed instructions on service-oriented deployment, please refer to the PaddleX Service-Oriented Deployment Guide. You can choose the appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/practical_tutorials/ts_forecast.html","title":"PaddleX 3.0 Time Series Forecasting Pipeline \u2014 Long-term Electricity Consumption Forecasting Tutorial","text":"<p>PaddleX offers a rich set of pipelines, each consisting of one or more models tailored to solve specific scenario tasks. All PaddleX pipelines support quick trials, and if the results are not satisfactory, you can also fine-tune the models with private data. PaddleX provides Python APIs to easily integrate pipelines into personal projects. Before use, you need to install PaddleX. For installation instructions, refer to PaddleX Local Installation Guide. This tutorial introduces the usage of the time series forecasting pipeline tool with an example of long-term electricity consumption forecasting.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#1-select-a-pipeline","title":"1. Select a Pipeline","text":"<p>First, choose the corresponding PaddleX pipeline based on your task scenario. The goal of this task is to predict future electricity consumption based on historical data. Recognizing this as a time series forecasting task, we will use PaddleX's time series forecasting pipeline. If you're unsure about the correspondence between tasks and pipelines, you can refer to the PaddleX Pipeline List (CPU/GPU) for an overview of pipeline capabilities.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#2-quick-experience","title":"2. Quick Experience","text":"<p>PaddleX offers two ways to experience its pipelines: locally on your machine or on the Baidu AIStudio Community.</p> <ul> <li> <p>Local Experience: <pre><code>from paddlex import create_model\nmodel = create_model(\"DLinear\")\noutput = model.predict(\"https://paddle-model-ecology.bj.bcebos.com/paddlex/ts/demo_ts/ts_fc.csv\", batch_size=1)\nfor res in output:\n    res.print(json_format=False)\n    res.save_to_csv(\"./output/\")\n</code></pre></p> </li> <li> <p>AIStudio Community Experience: Visit the Official Time Series Forecasting Application to experience time series forecasting capabilities. Note: Due to the tight correlation between time series data and scenarios, the official online experience models for time series tasks are tailored to specific scenarios and are not universal. Therefore, the online experience does not support using arbitrary files to test the official model solutions. However, after training your own model with scenario-specific data, you can select your trained model solution and use corresponding scenario data for online experience.</p> </li> </ul>"},{"location":"en/practical_tutorials/ts_forecast.html#3-choose-a-model","title":"3. Choose a Model","text":"<p>PaddleX provides five end-to-end time series forecasting models. For details, refer to the Model List. The benchmarks of these models are as follows:</p> Model Name MSE MAE Model Size (M) Description DLinear 0.382 0.394 76k A simple, efficient, and easy-to-use time series forecasting model Nonstationary 0.600 0.515 60.3M Based on transformer architecture, optimized for long-term forecasting of non-stationary time series PatchTST 0.385 0.397 2.2M A high-accuracy long-term forecasting model that balances local patterns and global dependencies TiDE 0.405 0.412 34.9M A high-accuracy model suitable for handling multivariate, long-term time series forecasting problems TimesNet 0.417 0.431 5.2M Through multi-period analysis, TimesNet is an adaptable and high-accuracy time series analysis model <p>Note: The above accuracy metrics are measured on the ETTH1 test dataset with an input sequence length of 96 and a prediction sequence length of 96 for all models except TiDE, which is 720.</p> <p>Based on your actual usage scenario, select an appropriate model for training. After training, evaluate the model weights within the pipeline and use them in practical scenarios.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#4-data-preparation-and-validation","title":"4. Data Preparation and Validation","text":""},{"location":"en/practical_tutorials/ts_forecast.html#41-data-preparation","title":"4.1 Data Preparation","text":"<p>To demonstrate the entire time series forecasting process, we will use the Electricity dataset for model training and validation. This dataset collects electricity consumption at a certain node from 2012 to 2014, with data collected every hour. Each data point consists of the current timestamp and corresponding electricity consumption. This dataset is commonly used to test and validate the performance of time series forecasting models.</p> <p>In this tutorial, we will use this dataset to predict the electricity consumption for the next 96 hours. We have already converted this dataset into a standard data format, and you can obtain a sample dataset by running the following command. For an introduction to the data format, you can refer to the Time Series Prediction Module Development Tutorial.</p> <p>You can use the following commands to download the demo dataset to a specified folder:</p> <pre><code>cd /path/to/paddlex\nwget https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/timeseries_forecast/electricity.tar -P ./dataset\ntar -xf ./dataset/electricity.tar -C ./dataset/\n</code></pre> <p>Data Considerations</p> <ul> <li>When annotating data for time series forecasting tasks, based on the collected real data, all data is arranged in chronological order. During training, the data is automatically divided into multiple time segments, where the historical time series data and the future sequences respectively represent the input data for training the model and its corresponding prediction targets, forming a set of training samples.</li> <li>Handling Missing Values: To ensure data quality and integrity, missing values can be imputed based on expert knowledge or statistical methods.</li> <li>Non-Repetitiveness: Ensure that data is collected in chronological order by row, with no duplication of timestamps.</li> </ul>"},{"location":"en/practical_tutorials/ts_forecast.html#42-data-validation","title":"4.2 Data Validation","text":"<p>Data Validation can be completed with just one command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=check_dataset \\\n    -o Global.dataset_dir=./dataset/electricity\n</code></pre> <p>After executing the above command, PaddleX will validate the dataset, summarize its basic information, and print <code>Check dataset passed !</code> in the log if the command runs successfully. The validation result file is saved in <code>./output/check_dataset_result.json</code>, and related outputs are saved in the current directory's <code>./output/check_dataset</code> directory, including example time series data.</p> <pre><code>{\n  \"done_flag\": true,\n  \"check_pass\": true,\n  \"attributes\": {\n    \"train_samples\": 22880,\n    \"train_table\": [\n      [\n        \"date\",\n        \"OT\"\n      ],\n      [\n        \"2012-01-01 00:00:00\",\n        2162.0\n      ],\n      [\n        \"2012-01-01 01:00:00\",\n        2835.0\n      ],\n      [\n        \"2012-01-01 02:00:00\",\n        2764.0\n      ],\n      [\n        \"2012-01-01 03:00:00\",\n        2735.0\n      ],\n      [\n        \"2012-01-01 04:00:00\",\n        2721.0\n      ],\n      [\n        \"2012-01-01 05:00:00\",\n        2742.0\n      ],\n      [\n        \"2012-01-01 06:00:00\",\n        2716.0\n      ],\n      [\n        \"2012-01-01 07:00:00\",\n        2716.0\n      ],\n      [\n        \"2012-01-01 08:00:00\",\n        2680.0\n      ],\n      [\n        \"2012-01-01 09:00:00\",\n        2581.0\n      ]\n    ],\n    \"val_samples\": 3424,\n    \"val_table\": [\n      [\n        \"date\",\n        \"OT\"\n      ],\n      [\n        \"2014-08-11 08:00:00\",\n        3528.0\n      ],\n      [\n        \"2014-08-11 09:00:00\",\n        3800.0\n      ],\n      [\n        \"2014-08-11 10:00:00\",\n        3889.0\n      ],\n      [\n        \"2014-08-11 11:00:00\",\n        4340.0\n      ],\n      [\n        \"2014-08-11 12:00:00\",\n        4321.0\n      ],\n      [\n        \"2014-08-11 13:00:00\",\n        4293.0\n      ],\n      [\n        \"2014-08-11 14:00:00\",\n        4393.0\n      ],\n      [\n        \"2014-08-11 15:00:00\",\n        4384.0\n      ],\n      [\n        \"2014-08-11 16:00:00\",\n        4495.0\n      ],\n      [\n        \"2014-08-11 17:00:00\",\n        4374.0\n      ]\n    ]\n  },\n  \"analysis\": {\n    \"histogram\": \"\"\n  },\n  \"dataset_path\": \"./dataset/electricity\",\n  \"show_type\": \"csv\",\n  \"dataset_type\": \"TSDataset\"\n}\n</code></pre> <p>The above verification results have omitted some data parts. <code>check_pass</code> being True indicates that the dataset format meets the requirements. Explanations for other indicators are as follows:</p> <ul> <li><code>attributes.train_samples</code>: The number of samples in the training set of this dataset is 22880.</li> <li><code>attributes.val_samples</code>: The number of samples in the validation set of this dataset is 3424.</li> <li><code>attributes.train_table</code>: Sample data rows from the training set of this dataset.</li> <li><code>attributes.val_table</code>: Sample data rows from the validation set of this dataset.</li> </ul> <p>Note: Only data that passes the verification can be used for training and evaluation.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#43-dataset-format-conversiondataset-splitting-optional","title":"4.3 Dataset Format Conversion/Dataset Splitting (Optional)","text":"<p>If you need to convert the dataset format or re-split the dataset, you can modify the configuration file or append hyperparameters for settings. Refer to Section 4.1.3 in the Time Series Prediction Module Development Tutorial.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":""},{"location":"en/practical_tutorials/ts_forecast.html#51-model-training","title":"5.1 Model Training","text":"<p>Before training, ensure that you have validated the dataset. To complete PaddleX model training, simply use the following command:</p> <pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n-o Global.mode=train \\\n-o Global.dataset_dir=./dataset/electricity \\\n-o Train.epochs_iters=5 \\\n-o Train.batch_size=16 \\\n-o Train.learning_rate=0.0001 \\\n-o Train.time_col=date \\\n-o Train.target_cols=OT \\\n-o Train.freq=1h \\\n-o Train.input_len=96 \\\n-o Train.predict_len=96\n</code></pre> <p>PaddleX supports modifying training hyperparameters and single-machine single-GPU training (time series models only support single-GPU training). Simply modify the configuration file or append command-line parameters.</p> <p>Each model in PaddleX provides a configuration file for model development to set relevant parameters. Model training-related parameters can be set by modifying the <code>Train</code> fields in the configuration file. Some example parameter descriptions in the configuration file are as follows:</p> <ul> <li><code>Global</code>:</li> <li><code>mode</code>: Mode, supporting dataset validation (<code>check_dataset</code>), model training (<code>train</code>), model evaluation (<code>evaluate</code>), and single instance testing (<code>predict</code>);</li> <li><code>device</code>: Training device, options include <code>cpu</code>, <code>gpu</code>, <code>xpu</code>, <code>npu</code>, <code>mlu</code>; check the Model Support List for models supported on different devices.</li> <li><code>Train</code>: Training hyperparameter settings;</li> <li><code>epochs_iters</code>: Number of training epochs;</li> <li><code>learning_rate</code>: Training learning rate;</li> <li><code>batch_size</code>: Training batch size per GPU;</li> <li><code>time_col</code>: Time column, set the column name of the time series dataset's time column based on your data;</li> <li><code>target_cols</code>: Target variable columns, set the column name(s) of the time series dataset's target variable(s) based on your data. Multiple columns can be separated by commas;</li> <li><code>freq</code>: Frequency, set the time frequency based on your data, e.g., 1min, 5min, 1h;</li> <li><code>input_len</code>: The length of historical time series input to the model; the input length should be considered comprehensively with the prediction length. Generally, the larger the setting, the more historical information can be referenced, and the higher the model accuracy.</li> <li><code>predict_len</code>: The length of the future sequence that the model is expected to predict; the prediction length should be considered comprehensively with the actual scenario. Generally, the larger the setting, the longer the future sequence you want to predict, and the lower the model accuracy.</li> <li><code>patience</code>: The parameter for the early stopping mechanism, indicating how many times the model's performance on the validation set can be continuously unimproved before stopping training; the larger the patience value, the longer the training time. For more hyperparameter introductions, refer to PaddleX Time Series Task Model Configuration File Parameter Description.</li> </ul> <p>Note:</p> <ul> <li>The above parameters can be set by appending command-line parameters, e.g., specifying the mode as model training: <code>-o Global.mode=train</code>; specifying the first GPU for training: <code>-o Global.device=gpu:0</code>; setting the number of training epochs to 10: <code>-o Train.epochs_iters=10</code>.</li> <li>During model training, PaddleX automatically saves the model weight files, with the default being <code>output</code>. If you need to specify a save path, you can use the <code>-o Global.output</code> field in the configuration file.</li> </ul>  More Details (Click to Expand)   <ul> <li>During the model training process, PaddleX automatically saves the model weight files, with the default directory being output. If you need to specify a different save path, you can configure it through the <code>-o Global.output</code> field in the configuration file.</li> <li>PaddleX abstracts away the concepts of dynamic graph weights and static graph weights from you. During model training, both dynamic and static graph weights are produced simultaneously. By default, static graph weights are selected for inference.</li> <li>When training other models, you need to specify the corresponding configuration file. The correspondence between models and configuration files can be found in the PaddleX Model List (CPU/GPU)</li> </ul> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p>Explanation of Training Outputs:</p> <p>After completing the model training, all outputs are saved in the specified output directory (default is <code>./output/</code>), typically including the following:</p> <p><code>train_result.json</code>: A training result record file that logs whether the training task was completed normally, as well as the output weight metrics, relevant file paths, etc. <code>train.log</code>: A training log file that records changes in model metrics, loss values, and other information during the training process. <code>config.yaml</code>: The training configuration file that records the hyperparameter configurations for this training session. <code>best_accuracy.pdparams.tar</code>, <code>scaler.pkl</code>, <code>.checkpoints</code>, <code>.inference*</code>: Files related to model weights, including network parameters, optimizers, static graph network parameters, etc.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#52-model-evaluation","title":"5.2 Model Evaluation","text":"<p>After completing model training, you can evaluate the specified model weights file on the validation set to verify the model's accuracy. Using PaddleX for model evaluation requires just one command:</p> <pre><code>    python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=evaluate \\\n    -o Global.dataset_dir=./dataset/electricity \\\n</code></pre> <p>Similar to model training, model evaluation supports setting through modifying the configuration file or appending command-line parameters.</p> <p>Note: When evaluating the model, you need to specify the model weights file path. Each configuration file has a default weight save path built-in. If you need to change it, simply set it by appending a command-line parameter, such as <code>-o Evaluate.weight_path=./output/best_model/model.pdparams</code>.</p> <p>After completing the model evaluation, typically, the following outputs are generated:</p> <p>Upon completion of model evaluation, <code>evaluate_result.json</code> will be produced, which records the evaluation results, specifically, whether the evaluation task was completed successfully, and the model's evaluation metrics, including mse and mae.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#53-model-optimization","title":"5.3 Model Optimization","text":"<p>After learning about model training and evaluation, we can improve the model's accuracy by adjusting hyperparameters. By reasonably adjusting the number of training epochs, you can control the depth of model training, avoiding overfitting or underfitting. The setting of the learning rate is related to the speed and stability of model convergence. Therefore, when optimizing model performance, it is essential to carefully consider the values of these two parameters and adjust them flexibly based on actual conditions to achieve the best training effect.</p> <p>Based on the method of controlled variables, we can adopt the following approach for hyperparameter tuning in time-series forecast:</p> <p>It is recommended to follow the method of controlled variables when debugging parameters: 1. Initial Setup: Set the training epochs to 5, batch size to 16, and input length to 96.</p> <ol> <li> <p>Experiments with DLinear Model and Launch three experiments with learning rates: 0.0001, 0.001, 0.01.</p> </li> <li> <p>Learning Rate Exploration: Experiment 2 yields the highest accuracy. Therefore, fix the learning rate at 0.001 and increase the training epochs to 30. Note: Due to the built-in early stopping mechanism for temporal tasks, training will automatically stop if the validation set accuracy does not improve after 10 patience epochs. To adjust the patience epochs, modify the <code>patience</code> hyperparameter in the advanced configuration.</p> </li> <li> <p>Increasing Training Epochs and Input Length.</p> </li> </ol> <p>After increasing the training epochs, Experiment 4 achieves the highest accuracy. Next, increase the input length to 144 (using 144 hours of historical data to predict the next 96 hours), resulting in Experiment 5 with an accuracy of 0.188.</p> <p>Learning Rate Exploration Results:</p> Experiment ID Epochs Learning Rate Batch Size Input Length Prediction Length Training Environment Validation MSE Experiment 1 5 0.0001 16 96 96 1 GPU 0.314 Experiment 2 5 0.001 16 96 96 1 GPU 0.302 Experiment 3 5 0.01 16 96 96 1 GPU 0.320 <p>Increasing Training Epochs Results:</p> Experiment ID Epochs Learning Rate Batch Size Input Length Prediction Length Training Environment Validation MSE Experiment 2 5 0.001 16 96 96 1 GPU 0.302 Experiment 4 30 0.001 16 96 96 1 GPU 0.301 <p>Increasing Input Length Results:</p> Experiment ID Epochs Learning Rate Batch Size Input Length Prediction Length Training Environment Validation MSE Experiment 4 30 0.001 16 96 96 1 GPU 0.301 Experiment 5 30 0.001 16 144 96 1 GPU 0.188"},{"location":"en/practical_tutorials/ts_forecast.html#6-production-line-testing","title":"6. Production Line Testing","text":"<p>Replace the model in the production line with the fine-tuned model and test using this power test data for prediction:</p> <p><pre><code>python main.py -c paddlex/configs/ts_forecast/DLinear.yaml \\\n    -o Global.mode=predict \\\n    -o Predict.model_dir=\"./output/inference\" \\\n    -o Predict.input=https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/doc_images/practical_tutorial/timeseries_forecast/test.csv\n</code></pre> This will generate prediction results under <code>./output</code>, with the predictions for <code>test.csv</code> saved in <code>result.csv</code>.</p> <p>Similar to model training and evaluation, follow these steps: * Specify the path to the model's <code>.yaml</code> configuration file (here it's <code>PatchTST_ad.yaml</code>) * Specify the mode as model evaluation: <code>-o Global.mode=evaluate</code> * Specify the path to the validation dataset: <code>-o Global.dataset_dir</code> Other related parameters can be set by modifying the <code>Global</code> and <code>Evaluate</code> fields in the <code>.yaml</code> configuration file. For details, refer to PaddleX Configuration.</p>"},{"location":"en/practical_tutorials/ts_forecast.html#7integrationdeployment","title":"7.Integration/Deployment","text":"<p>If the general-purpose time series forecast pipeline meets your requirements for inference speed and accuracy, you can proceed directly with development integration/deployment.</p> <ol> <li> <p>If you need to apply the general-purpose time series forecast pipeline directly in your Python project, you can refer to the following sample code: <pre><code>from paddlex import create_pipeline\npipeline = create_pipeline(pipeline=\"ts_forecast\")\noutput = pipeline.predict(\"pre_ts.csv\")\nfor res in output:\n    res.print()\n    res.save_to_csv(\"./output/\")\n</code></pre> For more parameters, please refer to the Time Series forecast Pipeline Usage Tutorial</p> </li> <li> <p>Additionally, PaddleX's time series forecast pipeline also offers a service-oriented deployment method, detailed as follows:</p> </li> </ol> <p>Service-Oriented Deployment: This is a common deployment form in actual production environments. By encapsulating the inference functionality as services, clients can access these services through network requests to obtain inference results. PaddleX supports users in achieving service-oriented deployment of pipelines at low cost. For detailed instructions on service-oriented deployment, please refer to the PPaddleX Service-Oriented Deployment Guide. You can choose the appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.</p>"},{"location":"en/support_list/model_list_dcu.html","title":"PaddleX Model List (Hygon DCU)","text":"<p>PaddleX incorporates multiple pipelines, each containing several modules, and each module encompasses various models. The specific models to use can be selected based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy. If you prioritize model storage size, select models with smaller storage sizes.</p>"},{"location":"en/support_list/model_list_dcu.html#image-classification-module","title":"Image Classification Module","text":"Model Name Top-1 Accuracy (%) Model Storage Size (M) Model Download Link ResNet18 71.0 41.5 M Inference Model/Trained Model ResNet34 74.6 77.3 M Inference Model/Trained Model ResNet50 76.5 90.8 M Inference Model/Trained Model ResNet101 77.6 158.7 M Inference Model/Trained Model ResNet152 78.3 214.2 M Inference Model/Trained Model <p>Note: The above accuracy metrics are Top-1 Accuracy on the ImageNet-1k validation set.</p>"},{"location":"en/support_list/model_list_dcu.html#semantic-segmentation-module","title":"Semantic Segmentation Module","text":"Model Name mIoU (%) Model Storage Size (M) Model Download Link Deeplabv3_Plus-R50 80.36 94.9 M Inference Model/Trained Model Deeplabv3_Plus-R101 81.10 162.5 M Inference Model/Trained Model <p>Note: The above accuracy metrics are mIoU on the Cityscapes dataset.</p>"},{"location":"en/support_list/model_list_mlu.html","title":"PaddleX Model List (Cambricon MLU)","text":"<p>PaddleX incorporates multiple pipelines, each containing several modules, and each module encompasses various models. You can select the appropriate models based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy. If you prioritize model size, select models with smaller storage requirements.</p>"},{"location":"en/support_list/model_list_mlu.html#image-classification-module","title":"Image Classification Module","text":"Model Name Top-1 Accuracy (%) Model Size (M) Model Download Link MobileNetV3_large_x0_5 69.2 9.6 M Inference Model/Trained Model MobileNetV3_large_x0_35 64.3 7.5 M Inference Model/Trained Model MobileNetV3_large_x0_75 73.1 14.0 M Inference Model/Trained Model MobileNetV3_large_x1_0 75.3 19.5 M Inference Model/Trained Model MobileNetV3_large_x1_25 76.4 26.5 M Inference Model/Trained Model MobileNetV3_small_x0_5 59.2 6.8 M Inference Model/Trained Model MobileNetV3_small_x0_35 53.0 6.0 M Inference Model/Trained Model MobileNetV3_small_x0_75 66.0 8.5 M Inference Model/Trained Model MobileNetV3_small_x1_0 68.2 10.5 M Inference Model/Trained Model MobileNetV3_small_x1_25 70.7 13.0 M Inference Model/Trained Model PP-HGNet_small 81.51 86.5 M Inference Model/Trained Model PP-LCNet_x0_5 63.14 6.7 M Inference Model/Trained Model PP-LCNet_x0_25 51.86 5.5 M Inference Model/Trained Model PP-LCNet_x0_35 58.09 5.9 M Inference Model/Trained Model PP-LCNet_x0_75 68.18 8.4 M Inference Model/Trained Model PP-LCNet_x1_0 71.32 10.5 M Inference Model/Trained Model PP-LCNet_x1_5 73.71 16.0 M Inference Model/Trained Model PP-LCNet_x2_0 75.18 23.2 M Inference Model/Trained Model PP-LCNet_x2_5 76.60 32.1 M Inference Model/Trained Model ResNet18 71.0 41.5 M Inference Model/Trained Model ResNet34 74.6 77.3 M Inference Model/Trained Model ResNet50 76.5 90.8 M Inference Model/Trained Model ResNet101 77.6 158.7 M Inference Model/Trained Model ResNet152 78.3 214.2 M Inference Model/Trained Model <p>Note: The above accuracy metrics are Top-1 Accuracy on the ImageNet-1k validation set.</p>"},{"location":"en/support_list/model_list_mlu.html#object-detection-module","title":"Object Detection Module","text":"Model Name mAP (%) Model Size (M) Model Download Link PicoDet-L 42.6 20.9 M Inference Model/Trained Model PicoDet-S 29.1 4.4 M Inference Model/Trained Model PP-YOLOE_plus-L 52.9 185.3 M Inference Model/Trained Model PP-YOLOE_plus-M 49.8 83.2 M Inference Model/Trained Model PP-YOLOE_plus-S 43.7 28.3 M Inference Model/Trained Model PP-YOLOE_plus-X 54.7 349.4 M Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the COCO2017 validation set.</p>"},{"location":"en/support_list/model_list_mlu.html#semantic-segmentation-module","title":"Semantic Segmentation Module","text":"Model Name mIoU (%) Model Size (M) Model Download Link PP-LiteSeg-T 73.10 28.5 M Inference Model/Trained Model <p>Note: The above accuracy metrics are based on the mIoU of the Cityscapes dataset.</p>"},{"location":"en/support_list/model_list_mlu.html#text-detection-module","title":"Text Detection Module","text":"Model Name Detection Hmean (%) Model Size (M) Model Download Link PP-OCRv4_mobile_det 77.79 4.2 M Inference Model/Trained Model PP-OCRv4_server_det 82.69 100.1M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more scenarios, with 500 images for detection.</p>"},{"location":"en/support_list/model_list_mlu.html#text-recognition-module","title":"Text Recognition Module","text":"Model Name Recognition Avg Accuracy (%) Model Size (M) Model Download Link PP-OCRv4_mobile_rec 78.20 10.6 M Inference Model/Trained Model PP-OCRv4_server_rec 79.20 71.2 M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more scenarios, with 11,000 images for text recognition.</p>"},{"location":"en/support_list/model_list_mlu.html#layout-analysis-module","title":"Layout Analysis Module","text":"Model Name mAP (%) Model Size (M) Model Download Link PicoDet_layout_1x 86.8 7.4M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout analysis dataset, containing 10,000 images.</p>"},{"location":"en/support_list/model_list_mlu.html#time-series-forecasting-module","title":"Time Series Forecasting Module","text":"Model Name mse mae Model Size (M) Model Download Link DLinear 0.382 0.394 72K Inference Model/Trained Model NLinear 0.386 0.392 40K Inference Model/Trained Model RLinear 0.384 0.392 40K Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the ETTH1 dataset (evaluation results on the test set test.csv).</p>"},{"location":"en/support_list/model_list_npu.html","title":"PaddleX Model List (Huawei Ascend NPU)","text":"<p>PaddleX incorporates multiple pipelines, each containing several modules, and each module encompasses various models. You can select the appropriate models based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy. If you prioritize model size, select models with smaller storage requirements.</p>"},{"location":"en/support_list/model_list_npu.html#image-classification-module","title":"Image Classification Module","text":"Model Name Top-1 Accuracy (%) Model Size (M) Model Download Link CLIP_vit_base_patch16_224 85.36 306.5 M Inference Model/Trained Model CLIP_vit_large_patch14_224 88.1 1.04 G Inference Model/Trained Model ConvNeXt_base_224 83.84 313.9 M Inference Model/Trained Model ConvNeXt_base_384 84.90 313.9 M Inference Model/Trained Model ConvNeXt_large_224 84.26 700.7 M Inference Model/Trained Model ConvNeXt_large_384 85.27 700.7 M Inference Model/Trained Model ConvNeXt_small 83.13 178.0 M Inference Model/Trained Model ConvNeXt_tiny 82.03 101.4 M Inference Model/Trained Model MobileNetV1_x0_5 63.5 4.8 M Inference Model/Trained Model MobileNetV1_x0_25 51.4 1.8 M Inference Model/Trained Model MobileNetV1_x0_75 68.8 9.3 M Inference Model/Trained Model MobileNetV1_x1_0 71.0 15.2 M Inference Model/Trained Model MobileNetV2_x0_5 65.0 7.1 M Inference Model/Trained Model MobileNetV2_x0_25 53.2 5.5 M Inference Model/Trained Model MobileNetV2_x1_0 72.2 12.6 M Inference Model/Trained Model MobileNetV2_x1_5 74.1 25.0 M Inference Model/Trained Model MobileNetV2_x2_0 75.2 41.2 M Inference Model/Trained Model MobileNetV3_large_x0_5 69.2 9.6 M Inference Model/Trained Model MobileNetV3_large_x0_35 64.3 7.5 M Inference Model/Trained Model MobileNetV3_large_x0_75 73.1 14.0 M Inference Model/Trained Model MobileNetV3_large_x1_0 75.3 19.5 M Inference Model/Trained Model MobileNetV3_large_x1_25 76.4 26.5 M Inference Model/Trained Model MobileNetV3_small_x0_5 59.2 6.8 M Inference Model/Trained Model MobileNetV3_small_x0_35 53.0 6.0 M Inference Model/Trained Model MobileNetV3_small_x0_75 66.0 8.5 M Inference Model/Trained Model MobileNetV3_small_x1_0 68.2 10.5 M Inference Model/Trained Model MobileNetV3_small_x1_25 70.7 13.0 M Inference Model/Trained Model MobileNetV4_conv_large 83.4 125.2 M Inference Model/Trained Model MobileNetV4_conv_medium 79.9 37.6 M Inference Model/Trained Model MobileNetV4_conv_small 74.6 14.7 M Inference Model/Trained Model PP-HGNet_base 85.0 249.4 M Inference Model/Trained Model PP-HGNet_small 81.51 86.5 M Inference Model/Trained Model PP-HGNet_tiny 79.83 52.4 M Inference Model/Trained Model PP-HGNetV2-B0 77.77 21.4 M Inference Model/Trained Model PP-HGNetV2-B1 79.18 22.6 M Inference Model/Trained Model PP-HGNetV2-B2 81.74 39.9 M Inference Model/Trained Model PP-HGNetV2-B3 82.98 57.9 M Inference Model/Trained Model PP-HGNetV2-B4 83.57 70.4 M Inference Model/Trained Model PP-HGNetV2-B5 84.75 140.8 M Inference Model/Trained Model PP-HGNetV2-B6 86.30 268.4 M Inference Model/Trained Model PP-LCNet_x0_5 63.14 6.7 M Inference Model/Trained Model PP-LCNet_x0_25 51.86 5.5 M Inference Model/Trained Model PP-LCNet_x0_35 58.09 5.9 M Inference Model/Trained Model PP-LCNet_x0_75 68.18 8.4 M Inference Model/Trained Model PP-LCNet_x1_0 71.32 10.5 M Inference Model/Trained Model PP-LCNet_x1_5 73.71 16.0 M Inference Model/Trained Model PP-LCNet_x2_0 75.18 23.2 M Inference Model/Trained Model PP-LCNet_x2_5 76.60 32.1 M Inference Model/Trained Model PP-LCNetV2_base 77.05 23.7 M Inference Model/Trained Model PP-LCNetV2_large 78.51 37.3 M Inference Model/Trained Model PP-LCNetV2_small 73.97 14.6 M Inference Model/Trained Model ResNet18_vd 72.3 41.5 M Inference Model/Trained Model ResNet18 71.0 41.5 M Inference Model/Trained Model ResNet34_vd 76.0 77.3 M Inference Model/Trained Model ResNet34 74.6 77.3 M Inference Model/Trained Model ResNet50_vd 79.1 90.8 M Inference Model/Trained Model ResNet50 76.5 90.8 M Inference Model/Trained Model ResNet101_vd 80.2 158.4 M Inference Model/Trained Model ResNet101 77.6 158.7 M Inference Model/Trained Model ResNet152_vd 80.6 214.3 M Inference Model/Trained Model ResNet152 78.3 214.2 M Inference Model/Trained Model ResNet200_vd 80.9 266.0 M Inference Model/Trained Model SwinTransformer_base_patch4_window7_224 83.37 310.5 M Inference Model/Trained Model SwinTransformer_base_patch4_window12_384 84.17 311.4 M Inference Model/Trained Model SwinTransformer_large_patch4_window7_224 86.19 694.8 M Inference Model/Trained Model SwinTransformer_large_patch4_window12_384 87.06 696.1 M Inference Model/Trained Model SwinTransformer_small_patch4_window7_224 83.21 175.6 M Inference Model/Trained Model SwinTransformer_tiny_patch4_window7_224 81.10 100.1 M Inference Model/Trained Model <p>Note: The above accuracy metrics refer to Top-1 Accuracy on the ImageNet-1k validation set.</p>"},{"location":"en/support_list/model_list_npu.html#_1","title":"\u56fe\u50cf\u200b\u591a\u200b\u6807\u7b7e\u200b\u5206\u7c7b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b mAP\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link CLIP_vit_base_patch16_448_ML 89.15 325.6 M Inference Model/Trained Model PP-HGNetV2-B0_ML 80.98 39.6 M PP-HGNetV2-B4_ML 87.96 88.5 M Inference Model/Trained Model PP-HGNetV2-B6_ML 91.25 286.5 M Inference Model/Trained Model Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b COCO2017 \u200b\u7684\u200b\u591a\u200b\u6807\u7b7e\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200bmAP\u3002</p>"},{"location":"en/support_list/model_list_npu.html#object-detection-module","title":"Object Detection Module","text":"Model Name mAP (%) Model Size (M) Model Download Link Cascade-FasterRCNN-ResNet50-FPN 41.1 245.4 M Inference Model/Trained Model Cascade-FasterRCNN-ResNet50-vd-SSLDv2-FPN 45.0 246.2 M Inference Model/Trained Model CenterNet-DLA-34 37.6 75.4 M Inference Model/Trained Model CenterNet-ResNet50 38.9 319.7 M Inference Model/Trained Model DETR-R50 42.3 159.3 M Inference Model/Trained Model FasterRCNN-ResNet34-FPN 37.8 137.5 M Inference Model/Trained Model FasterRCNN-ResNet50 36.7 120.2 M Inference Model/Trained Model FasterRCNN-ResNet50-FPN 38.4 148.1 M Inference Model/Trained Model FasterRCNN-ResNet50-vd-FPN 39.5 148.1 M Inference Model/Trained Model FasterRCNN-ResNet50-vd-SSLDv2-FPN 41.4 148.1 M Inference Model/Trained Model FasterRCNN-ResNet101 39.0 188.1 M Inference Model/Trained Model FasterRCNN-ResNet101-FPN 41.4 216.3 M Inference Model/Trained Model FasterRCNN-ResNeXt101-vd-FPN 43.4 360.6 M Inference Model/Trained Model FasterRCNN-Swin-Tiny-FPN 42.6 159.8 M Inference Model/Trained Model FCOS-ResNet50 39.6 124.2 M Inference Model/Trained Model PicoDet-L 42.6 20.9 M Inference Model/Trained Model PicoDet-M 37.5 16.8 M Inference Model/Trained Model PicoDet-S 29.1 4.4 M Inference Model/Trained Model PicoDet-XS 26.2 5.7M Inference Model/Trained Model PP-YOLOE_plus-L 52.9 185.3 M Inference Model/Trained Model PP-YOLOE_plus-M 49.8 83.2 M Inference Model/Trained Model PP-YOLOE_plus-S 43.7 28.3 M Inference Model/Trained Model PP-YOLOE_plus-X 54.7 349.4 M Inference Model/Trained Model RT-DETR-H 56.3 435.8 M Inference Model/Trained Model RT-DETR-L 53.0 113.7 M Inference Model/Trained Model RT-DETR-R18 46.5 70.7 M Inference Model/Trained Model RT-DETR-R50 53.1 149.1 M Inference Model/Trained Model RT-DETR-X 54.8 232.9 M Inference Model/Trained Model YOLOv3-DarkNet53 39.1 219.7 M Inference Model/Trained Model YOLOv3-MobileNetV3 31.4 83.8 M Inference Model/Trained Model YOLOv3-ResNet50_vd_DCN 40.6 163.0 M Inference Model/Trained Model <p>Note: The above accuracy metrics are for COCO2017 validation set mAP(0.5:0.95).</p>"},{"location":"en/support_list/model_list_npu.html#_2","title":"\u5c0f\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b mAP\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link PP-YOLOE_plus_SOD-S 25.1 77.3 M Inference Model/Trained Model PP-YOLOE_plus_SOD-L 31.9 325.0 M Inference Model/Trained Model PP-YOLOE_plus_SOD-largesize-L 42.7 340.5 M Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b VisDrone-DET \u200b\u9a8c\u8bc1\u200b\u96c6\u200b mAP(0.5:0.95)\u3002</p>"},{"location":"en/support_list/model_list_npu.html#_3","title":"\u884c\u4eba\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b mAP\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link PP-YOLOE-L_human 48.0 196.1 M Inference Model/Trained Model PP-YOLOE-S_human 42.5 28.8 M Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b CrowdHuman \u200b\u9a8c\u8bc1\u200b\u96c6\u200b mAP(0.5:0.95)\u3002</p>"},{"location":"en/support_list/model_list_npu.html#semantic-segmentation-module","title":"Semantic Segmentation Module","text":"Model Name mIoU (%) Model Size (M) Model Download Link Deeplabv3_Plus-R50 80.36 94.9 M Inference Model/Trained Model Deeplabv3_Plus-R101 81.10 162.5 M Inference Model/Trained Model Deeplabv3-R50 79.90 138.3 M Inference Model/Trained Model Deeplabv3-R101 80.85 205.9 M Inference Model/Trained Model OCRNet_HRNet-W48 82.15 249.8 M Inference Model/Trained Model PP-LiteSeg-T 73.10 28.5 M Inference Model/Trained Model <p>Note: The above accuracy metrics are for Cityscapes dataset mIoU.</p>"},{"location":"en/support_list/model_list_npu.html#instance-segmentation-module","title":"Instance Segmentation Module","text":"Model Name Mask AP Model Size (M) Model Download Link Mask-RT-DETR-H 50.6 449.9 M Inference Model/Trained Model Mask-RT-DETR-L 45.7 113.6 M Inference Model/Trained Model Mask-RT-DETR-M 42.7 66.6 M Inference Model/Trained Model Mask-RT-DETR-S 41.0 51.8 M Inference Model/Trained Model Mask-RT-DETR-X 47.5 237.5 M Inference Model/Trained Model Cascade-MaskRCNN-ResNet50-FPN 36.3 254.8 M Inference Model/Trained Model Cascade-MaskRCNN-ResNet50-vd-SSLDv2-FPN 39.1 254.7 M Inference Model/Trained Model MaskRCNN-ResNet50-FPN 35.6 157.5 M Inference Model/Trained Model MaskRCNN-ResNet50-vd-FPN 36.4 157.5 M Inference Model/Trained Model MaskRCNN-ResNet50 32.8 127.8 M Inference Model/Trained Model MaskRCNN-ResNet101-FPN 36.6 225.4 M Inference Model/Trained Model MaskRCNN-ResNet101-vd-FPN 38.1 225.1 M Inference Model/Trained Model MaskRCNN-ResNeXt101-vd-FPN 39.5 370.0 M Inference Model/Trained Model PP-YOLOE_seg-S 32.5 31.5 M Inference Model/Trained Model <p>Note: The above accuracy metrics are for COCO2017 validation set Mask AP(0.5:0.95).</p>"},{"location":"en/support_list/model_list_npu.html#_4","title":"\u56fe\u50cf\u200b\u7279\u5f81\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b recall@1\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link PP-ShiTuV2_rec_CLIP_vit_base 88.69 306.6 M Inference Model/Trained Model PP-ShiTuV2_rec_CLIP_vit_large 91.03 1.05 G Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b AliProducts recall@1\u3002</p>"},{"location":"en/support_list/model_list_npu.html#_5","title":"\u4e3b\u4f53\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b mAP\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link PP-ShiTuV2_det 41.5 27.6 M Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b PaddleClas\u200b\u4e3b\u4f53\u200b\u68c0\u6d4b\u200b\u6570\u636e\u200b\u96c6\u200b mAP(0.5:0.95)\u3002</p>"},{"location":"en/support_list/model_list_npu.html#_6","title":"\u8f66\u8f86\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b mAP\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link PP-YOLOE-L_vehicle 63.9 196.1 M Inference Model/Trained Model PP-YOLOE-S_vehicle 61.3 28.8 M Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b PPVehicle \u200b\u9a8c\u8bc1\u200b\u96c6\u200b mAP(0.5:0.95)\u3002</p>"},{"location":"en/support_list/model_list_npu.html#_7","title":"\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u6a21\u5757","text":"\u6a21\u578b\u200b\u540d\u79f0\u200b Avg\uff08%\uff09 \u200b\u6a21\u578b\u200b\u5b58\u50a8\u200b\u5927\u5c0f\u200b Model Download Link STFPM 96.2 21.5 M Inference Model/Trained Model <p>\u200b\u6ce8\u200b\uff1a\u200b\u4ee5\u4e0a\u200b\u7cbe\u5ea6\u200b\u6307\u6807\u200b\u4e3a\u200b MVTec AD \u200b\u9a8c\u8bc1\u200b\u96c6\u200b \u200b\u5e73\u5747\u200b\u5f02\u5e38\u200b\u5206\u6570\u200b\u3002</p>"},{"location":"en/support_list/model_list_npu.html#text-detection-module","title":"Text Detection Module","text":"Model Name Detection Hmean (%) Model Size (M) Model Download Link PP-OCRv4_mobile_det 77.79 4.2 M Inference Model/Trained Model PP-OCRv4_server_det 82.69 100.1 M Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten scenarios, with 500 images for detection.</p>"},{"location":"en/support_list/model_list_npu.html#text-recognition-module","title":"Text Recognition Module","text":"Model Name Recognition Avg Accuracy (%) Model Size (M) Model Download Link PP-OCRv4_mobile_rec 78.20 10.6 M Inference Model/Trained Model PP-OCRv4_server_rec 79.20 71.2 M Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, and handwritten scenarios, with 11,000 images for text recognition.</p> Model Name Recognition Avg Accuracy (%) Model Size (M) Model Download Link ch_SVTRv2_rec 68.81 73.9 M Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition A-Rank.</p> Model Name Recognition Avg Accuracy (%) Model Size (M) Model Download Link ch_RepSVTR_rec 65.07 22.1 M Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition B-Rank.</p>"},{"location":"en/support_list/model_list_npu.html#table-structure-recognition-module","title":"Table Structure Recognition Module","text":"Model Name Accuracy (%) Model Size (M) Model Download Link SLANet 76.31 6.9 M Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the PubtabNet English table recognition dataset.</p>"},{"location":"en/support_list/model_list_npu.html#layout-analysis-module","title":"Layout Analysis Module","text":"Model Name mAP (%) Model Size (M) Model Download Link PicoDet_layout_1x 86.8 7.4M Inference Model/Trained Model PicoDet-L_layout_3cls 89.3 22.6 M Inference Model/Trained Model RT-DETR-H_layout_3cls 95.9 470.1 M Inference Model/Trained Model RT-DETR-H_layout_17cls 92.6 470.2 M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout analysis dataset, containing 10,000 images.</p>"},{"location":"en/support_list/model_list_npu.html#time-series-forecasting-module","title":"Time Series Forecasting Module","text":"Model Name MSE MAE Model Size (M) Model Download Link DLinear 0.382 0.394 72K Inference Model/Trained Model NLinear 0.386 0.392 40K Inference Model/Trained Model Nonstationary 0.600 0.515 55.5 M Inference Model/Trained Model PatchTST 0.385 0.397 2.0M Inference Model/Trained Model RLinear 0.384 0.392 40K Inference Model/Trained Model TiDE 0.405 0.412 31.7M Inference Model/Trained Model TimesNet 0.417 0.431 4.9M Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the ETTH1 dataset (evaluation results on the test set test.csv).</p>"},{"location":"en/support_list/model_list_npu.html#time-series-anomaly-detection-module","title":"Time Series Anomaly Detection Module","text":"Model Name Precision Recall F1-Score Model Size (M) Model Download Link AutoEncoder_ad 99.36 84.36 91.25 52K Inference Model/Trained Model DLinear_ad 98.98 93.96 96.41 112K Inference Model/Trained Model Nonstationary_ad 98.55 88.95 93.51 1.8M Inference Model/Trained Model PatchTST_ad 98.78 90.70 94.57 320K Inference Model/Trained Model TimesNet_ad 98.37 94.80 96.56 1.3M Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the PSM dataset.</p>"},{"location":"en/support_list/model_list_npu.html#time-series-classification-module","title":"Time Series Classification Module","text":"Model Name Acc (%) Model Size (M) Model Download Link TimesNet_cls 87.5 792K Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the UWaveGestureLibrary: Training, Evaluation datasets.</p>"},{"location":"en/support_list/model_list_xpu.html","title":"PaddleX Model List (Kunlun XPU)","text":"<p>PaddleX incorporates multiple pipelines, each containing several modules, and each module encompasses various models. You can select the appropriate models based on the benchmark data below. If you prioritize model accuracy, choose models with higher accuracy. If you prioritize model size, select models with smaller storage requirements.</p>"},{"location":"en/support_list/model_list_xpu.html#image-classification-module","title":"Image Classification Module","text":"Model Name Top-1 Accuracy (%) Model Size (M) Model Download Link MobileNetV3_large_x0_5 69.2 9.6 M Inference Model/Trained Model MobileNetV3_large_x0_35 64.3 7.5 M Inference Model/Trained Model MobileNetV3_large_x0_75 73.1 14.0 M Inference Model/Trained Model MobileNetV3_large_x1_0 75.3 19.5 M Inference Model/Trained Model MobileNetV3_large_x1_25 76.4 26.5 M Inference Model/Trained Model MobileNetV3_small_x0_5 59.2 6.8 M Inference Model/Trained Model MobileNetV3_small_x0_35 53.0 6.0 M Inference Model/Trained Model MobileNetV3_small_x0_75 66.0 8.5 M Inference Model/Trained Model MobileNetV3_small_x1_0 68.2 10.5 M Inference Model/Trained Model MobileNetV3_small_x1_25 70.7 13.0 M Inference Model/Trained Model PP-HGNet_small 81.51 86.5 M Inference Model/Trained Model PP-LCNet_x0_5 63.14 6.7 M Inference Model/Trained Model PP-LCNet_x0_25 51.86 5.5 M Inference Model/Trained Model PP-LCNet_x0_35 58.09 5.9 M Inference Model/Trained Model PP-LCNet_x0_75 68.18 8.4 M Inference Model/Trained Model PP-LCNet_x1_0 71.32 10.5 M Inference Model/Trained Model PP-LCNet_x1_5 73.71 16.0 M Inference Model/Trained Model PP-LCNet_x2_0 75.18 23.2 M Inference Model/Trained Model PP-LCNet_x2_5 76.60 32.1 M Inference Model/Trained Model ResNet18 71.0 41.5 M Inference Model/Trained Model ResNet34 74.6 77.3 M Inference Model/Trained Model ResNet50 76.5 90.8 M Inference Model/Trained Model ResNet101 77.6 158.7 M Inference Model/Trained Model ResNet152 78.3 214.2 M Inference Model/Trained Model <p>Note: The above accuracy metrics are Top-1 Accuracy on the ImageNet-1k validation set.</p>"},{"location":"en/support_list/model_list_xpu.html#object-detection-module","title":"Object Detection Module","text":"Model Name mAP (%) Model Size (M) Model Download Link PicoDet-L 42.6 20.9 M Inference Model/Trained Model PicoDet-S 29.1 4.4 M Inference Model/Trained Model PP-YOLOE_plus-L 52.9 185.3 M Inference Model/Trained Model PP-YOLOE_plus-M 49.8 83.2 M Inference Model/Trained Model PP-YOLOE_plus-S 43.7 28.3 M Inference Model/Trained Model PP-YOLOE_plus-X 54.7 349.4 M Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the COCO2017 validation set.</p>"},{"location":"en/support_list/model_list_xpu.html#semantic-segmentation-module","title":"Semantic Segmentation Module","text":"Model Name mIoU (%) Model Size (M) Model Download Link PP-LiteSeg-T 73.10 28.5 M Inference Model/Trained Model <p>Note: The above accuracy metrics are based on the mIoU of the Cityscapes dataset.</p>"},{"location":"en/support_list/model_list_xpu.html#text-detection-module","title":"Text Detection Module","text":"Model Name Detection Hmean (%) Model Size (M) Model Download Link PP-OCRv4_mobile_det 77.79 4.2 M Inference Model/Trained Model PP-OCRv4_server_det 82.69 100.1M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more scenarios, with 500 images for detection.</p>"},{"location":"en/support_list/model_list_xpu.html#text-recognition-module","title":"Text Recognition Module","text":"Model Name Recognition Avg Accuracy (%) Model Size (M) Model Download Link PP-OCRv4_mobile_rec 78.20 10.6 M Inference Model/Trained Model PP-OCRv4_server_rec 79.20 71.2 M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built Chinese dataset, covering street scenes, web images, documents, handwriting, and more scenarios, with 11,000 images for text recognition.</p>"},{"location":"en/support_list/model_list_xpu.html#layout-analysis-module","title":"Layout Analysis Module","text":"Model Name mAP (%) Model Size (M) Model Download Link PicoDet_layout_1x 86.8 7.4M Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is PaddleOCR's self-built layout analysis dataset, containing 10,000 images.</p>"},{"location":"en/support_list/model_list_xpu.html#time-series-forecasting-module","title":"Time Series Forecasting Module","text":"Model Name mse mae Model Size (M) Model Download Link DLinear 0.382 0.394 72K Inference Model/Trained Model NLinear 0.386 0.392 40K Inference Model/Trained Model RLinear 0.384 0.392 40K Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the ETTH1 dataset (evaluation results on the test set test.csv).</p>"},{"location":"en/support_list/models_list.html","title":"PaddleX Model List\uff08CPU/GPU\uff09","text":"<p>PaddleX incorporates multiple pipelines, each containing several modules, and each module includes various models. You can choose which models to use based on the benchmark data below. If you prioritize model accuracy, select models with higher accuracy. If you prioritize inference speed, choose models with faster inference. If you prioritize model storage size, select models with smaller storage sizes.</p>"},{"location":"en/support_list/models_list.html#image-classification-module","title":"Image Classification Module","text":"Model Name Top-1 Acc (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link CLIP_vit_base_patch16_224 85.36 13.1957 285.493 306.5 M CLIP_vit_base_patch16_224.yaml Inference Model/Trained Model CLIP_vit_large_patch14_224 88.1 51.1284 1131.28 1.04 G CLIP_vit_large_patch14_224.yaml Inference Model/Trained Model ConvNeXt_base_224 83.84 12.8473 1513.87 313.9 M ConvNeXt_base_224.yaml Inference Model/Trained Model ConvNeXt_base_384 84.90 31.7607 3967.05 313.9 M ConvNeXt_base_384.yaml Inference Model/Trained Model ConvNeXt_large_224 84.26 26.8103 2463.56 700.7 M ConvNeXt_large_224.yaml Inference Model/Trained Model ConvNeXt_large_384 85.27 66.4058 6598.92 700.7 M ConvNeXt_large_384.yaml Inference Model/Trained Model ConvNeXt_small 83.13 9.74075 1127.6 178.0 M ConvNeXt_small.yaml Inference Model/Trained Model ConvNeXt_tiny 82.03 5.48923 672.559 101.4 M ConvNeXt_tiny.yaml Inference Model/Trained Model FasterNet-L 83.5 23.4415 - 357.1 M FasterNet-L.yaml Inference Model/Trained Model FasterNet-M 83.0 21.8936 - 204.6 M FasterNet-M.yaml Inference Model/Trained Model FasterNet-S 81.3 13.0409 - 119.3 M FasterNet-S.yaml Inference Model/Trained Model FasterNet-T0 71.9 12.2432 - 15.1 M FasterNet-T0.yaml Inference Model/Trained Model FasterNet-T1 75.9 11.3562 - 29.2 M FasterNet-T1.yaml Inference Model/Trained Model FasterNet-T2 79.1 10.703 - 57.4 M FasterNet-T2.yaml Inference Model/Trained Model MobileNetV1_x0_5 63.5 1.86754 7.48297 4.8 M MobileNetV1_x0_5.yaml Inference Model/Trained Model MobileNetV1_x0_25 51.4 1.83478 4.83674 1.8 M MobileNetV1_x0_25.yaml Inference Model/Trained Model MobileNetV1_x0_75 68.8 2.57903 10.6343 9.3 M MobileNetV1_x0_75.yaml Inference Model/Trained Model MobileNetV1_x1_0 71.0 2.78781 13.98 15.2 M MobileNetV1_x1_0.yaml Inference Model/Trained Model MobileNetV2_x0_5 65.0 4.94234 11.1629 7.1 M MobileNetV2_x0_5.yaml Inference Model/Trained Model MobileNetV2_x0_25 53.2 4.50856 9.40991 5.5 M MobileNetV2_x0_25.yaml Inference Model/Trained Model MobileNetV2_x1_0 72.2 6.12159 16.0442 12.6 M MobileNetV2_x1_0.yaml Inference Model/Trained Model MobileNetV2_x1_5 74.1 6.28385 22.5129 25.0 M MobileNetV2_x1_5.yaml Inference Model/Trained Model MobileNetV2_x2_0 75.2 6.12888 30.8612 41.2 M MobileNetV2_x2_0.yaml Inference Model/Trained Model MobileNetV3_large_x0_5 69.2 6.31302 14.5588 9.6 M MobileNetV3_large_x0_5.yaml Inference Model/Trained Model MobileNetV3_large_x0_35 64.3 5.76207 13.9041 7.5 M MobileNetV3_large_x0_35.yaml Inference Model/Trained Model MobileNetV3_large_x0_75 73.1 8.41737 16.9506 14.0 M MobileNetV3_large_x0_75.yaml Inference Model/Trained Model MobileNetV3_large_x1_0 75.3 8.64112 19.1614 19.5 M MobileNetV3_large_x1_0.yaml Inference Model/Trained Model MobileNetV3_large_x1_25 76.4 8.73358 22.1296 26.5 M MobileNetV3_large_x1_25.yaml Inference Model/Trained Model MobileNetV3_small_x0_5 59.2 5.16721 11.2688 6.8 M MobileNetV3_small_x0_5.yaml Inference Model/Trained Model MobileNetV3_small_x0_35 53.0 5.22053 11.0055 6.0 M MobileNetV3_small_x0_35.yaml Inference Model/Trained Model MobileNetV3_small_x0_75 66.0 5.39831 12.8313 8.5 M MobileNetV3_small_x0_75.yaml Inference Model/Trained Model MobileNetV3_small_x1_0 68.2 6.00993 12.9598 10.5 M MobileNetV3_small_x1_0.yaml Inference Model/Trained Model MobileNetV3_small_x1_25 70.7 6.9589 14.3995 13.0 M MobileNetV3_small_x1_25.yaml Inference Model/Trained Model MobileNetV4_conv_large 83.4 12.5485 51.6453 125.2 M MobileNetV4_conv_large.yaml Inference Model/Trained Model MobileNetV4_conv_medium 79.9 9.65509 26.6157 37.6 M MobileNetV4_conv_medium.yaml Inference Model/Trained Model MobileNetV4_conv_small 74.6 5.24172 11.0893 14.7 M MobileNetV4_conv_small.yaml Inference Model/Trained Model MobileNetV4_hybrid_large 83.8 20.0726 213.769 145.1 M MobileNetV4_hybrid_large.yaml Inference Model/Trained Model MobileNetV4_hybrid_medium 80.5 19.7543 62.2624 42.9 M MobileNetV4_hybrid_medium.yaml Inference Model/Trained Model PP-HGNet_base 85.0 14.2969 327.114 249.4 M PP-HGNet_base.yaml Inference Model/Trained Model PP-HGNet_small 81.51 5.50661 119.041 86.5 M PP-HGNet_small.yaml Inference Model/Trained Model PP-HGNet_tiny 79.83 5.22006 69.396 52.4 M PP-HGNet_tiny.yaml Inference Model/Trained Model PP-HGNetV2-B0 77.77 6.53694 23.352 21.4 M PP-HGNetV2-B0.yaml Inference Model/Trained Model PP-HGNetV2-B1 79.18 6.56034 27.3099 22.6 M PP-HGNetV2-B1.yaml Inference Model/Trained Model PP-HGNetV2-B2 81.74 9.60494 43.1219 39.9 M PP-HGNetV2-B2.yaml Inference Model/Trained Model PP-HGNetV2-B3 82.98 11.0042 55.1367 57.9 M PP-HGNetV2-B3.yaml Inference Model/Trained Model PP-HGNetV2-B4 83.57 9.66407 54.2462 70.4 M PP-HGNetV2-B4.yaml Inference Model/Trained Model PP-HGNetV2-B5 84.75 15.7091 115.926 140.8 M PP-HGNetV2-B5.yaml Inference Model/Trained Model PP-HGNetV2-B6 86.30 21.226 255.279 268.4 M PP-HGNetV2-B6.yaml Inference Model/Trained Model PP-LCNet_x0_5 63.14 3.67722 6.66857 6.7 M PP-LCNet_x0_5.yaml Inference Model/Trained Model PP-LCNet_x0_25 51.86 2.65341 5.81357 5.5 M PP-LCNet_x0_25.yaml Inference Model/Trained Model PP-LCNet_x0_35 58.09 2.7212 6.28944 5.9 M PP-LCNet_x0_35.yaml Inference Model/Trained Model PP-LCNet_x0_75 68.18 3.91032 8.06953 8.4 M PP-LCNet_x0_75.yaml Inference Model/Trained Model PP-LCNet_x1_0 71.32 3.84845 9.23735 10.5 M PP-LCNet_x1_0.yaml Inference Model/Trained Model PP-LCNet_x1_5 73.71 3.97666 12.3457 16.0 M PP-LCNet_x1_5.yaml Inference Model/Trained Model PP-LCNet_x2_0 75.18 4.07556 16.2752 23.2 M PP-LCNet_x2_0.yaml Inference Model/Trained Model PP-LCNet_x2_5 76.60 4.06028 21.5063 32.1 M PP-LCNet_x2_5.yaml Inference Model/Trained Model PP-LCNetV2_base 77.05 5.23428 19.6005 23.7 M PP-LCNetV2_base.yaml Inference Model/Trained Model PP-LCNetV2_large 78.51 6.78335 30.4378 37.3 M PP-LCNetV2_large.yaml Inference Model/Trained Model PP-LCNetV2_small 73.97 3.89762 13.0273 14.6 M PP-LCNetV2_small.yaml Inference Model/Trained Model ResNet18_vd 72.3 3.53048 31.3014 41.5 M ResNet18_vd.yaml Inference Model/Trained Model ResNet18 71.0 2.4868 27.4601 41.5 M ResNet18.yaml Inference Model/Trained Model ResNet34_vd 76.0 5.60675 56.0653 77.3 M ResNet34_vd.yaml Inference Model/Trained Model ResNet34 74.6 4.16902 51.925 77.3 M ResNet34.yaml Inference Model/Trained Model ResNet50_vd 79.1 10.1885 68.446 90.8 M ResNet50_vd.yaml Inference Model/Trained Model ResNet50 76.5 9.62383 64.8135 90.8 M ResNet50.yaml Inference Model/Trained Model ResNet101_vd 80.2 20.0563 124.85 158.4 M ResNet101_vd.yaml Inference Model/Trained Model ResNet101 77.6 19.2297 121.006 158.7 M ResNet101.yaml Inference Model/Trained Model ResNet152_vd 80.6 29.6439 181.678 214.3 M ResNet152_vd.yaml Inference Model/Trained Model ResNet152 78.3 30.0461 177.707 214.2 M ResNet152.yaml Inference Model/Trained Model ResNet200_vd 80.9 39.1628 235.185 266.0 M ResNet200_vd.yaml Inference Model/Trained Model StarNet-S1 73.6 9.895 23.0465 11.2 M StarNet-S1.yaml Inference Model/Trained Model StarNet-S2 74.8 7.91279 21.9571 14.3 M StarNet-S2.yaml Inference Model/Trained Model StarNet-S3 77.0 10.7531 30.7656 22.2 M StarNet-S3.yaml Inference Model/Trained Model StarNet-S4 79.0 15.2868 43.2497 28.9 M StarNet-S4.yaml Inference Model/Trained Model SwinTransformer_base_patch4_window7_224 83.37 16.9848 383.83 310.5 M SwinTransformer_base_patch4_window7_224.yaml Inference Model/Trained Model SwinTransformer_base_patch4_window12_384 84.17 37.2855 1178.63 311.4 M SwinTransformer_base_patch4_window12_384.yaml Inference Model/Trained Model SwinTransformer_large_patch4_window7_224 86.19 27.5498 689.729 694.8 M SwinTransformer_large_patch4_window7_224.yaml Inference Model/Trained Model SwinTransformer_large_patch4_window12_384 87.06 74.1768 2105.22 696.1 M SwinTransformer_large_patch4_window12_384.yaml Inference Model/Trained Model SwinTransformer_small_patch4_window7_224 83.21 16.3982 285.56 175.6 M SwinTransformer_small_patch4_window7_224.yaml Inference Model/Trained Model SwinTransformer_tiny_patch4_window7_224 81.10 8.54846 156.306 100.1 M SwinTransformer_tiny_patch4_window7_224.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are Top-1 Acc on the ImageNet-1k validation set.</p>"},{"location":"en/support_list/models_list.html#image-multi-label-classification-module","title":"Image Multi-Label Classification Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link CLIP_vit_base_patch16_448_ML 89.15 - - 325.6 M CLIP_vit_base_patch16_448_ML.yaml Inference Model/Trained Model PP-HGNetV2-B0_ML 80.98 - - 39.6 M PP-HGNetV2-B0_ML.yaml Inference Model/Trained Model PP-HGNetV2-B4_ML 87.96 - - 88.5 M PP-HGNetV2-B4_ML.yaml Inference Model/Trained Model PP-HGNetV2-B6_ML 91.25 - - 286.5 M PP-HGNetV2-B6_ML.yaml Inference Model/Trained Model PP-LCNet_x1_0_ML 77.96 - - 29.4 M PP-LCNet_x1_0_ML.yaml Inference Model/Trained Model ResNet50_ML 83.50 - - 108.9 M ResNet50_ML.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP for the multi-label classification task on COCO2017.</p>"},{"location":"en/support_list/models_list.html#pedestrian-attribute-module","title":"Pedestrian Attribute Module","text":"Model Name mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-LCNet_x1_0_pedestrian_attribute 92.2 3.84845 9.23735 6.7 M PP-LCNet_x1_0_pedestrian_attribute.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mA on PaddleX's internal self-built dataset.</p>"},{"location":"en/support_list/models_list.html#vehicle-attribute-module","title":"Vehicle Attribute Module","text":"Model Name mA (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-LCNet_x1_0_vehicle_attribute 91.7 3.84845 9.23735 6.7 M PP-LCNet_x1_0_vehicle_attribute.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mA on the VeRi dataset.</p>"},{"location":"en/support_list/models_list.html#image-feature-module","title":"Image Feature Module","text":"Model Name recall@1 (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-ShiTuV2_rec 84.2 5.23428 19.6005 16.3 M PP-ShiTuV2_rec.yaml Inference Model/Trained Model PP-ShiTuV2_rec_CLIP_vit_base 88.69 13.1957 285.493 306.6 M PP-ShiTuV2_rec_CLIP_vit_base.yaml Inference Model/Trained Model PP-ShiTuV2_rec_CLIP_vit_large 91.03 51.1284 1131.28 1.05 G PP-ShiTuV2_rec_CLIP_vit_large.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are recall@1 on AliProducts.</p>"},{"location":"en/support_list/models_list.html#document-orientation-classification-module","title":"Document Orientation Classification Module","text":"Model Name Top-1 Acc (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-LCNet_x1_0_doc_ori 99.26 3.84845 9.23735 7.1 M PP-LCNet_x1_0_doc_ori.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are Top-1 Acc on PaddleX's internal self-built dataset.</p>"},{"location":"en/support_list/models_list.html#face-feature-module","title":"Face Feature Module","text":"Model Name Output Feature Dimension Acc (%)AgeDB-30/CFP-FP/LFW GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) YAML File Model Download Link MobileFaceNet 128 96.28/96.71/99.58 4.1 MobileFaceNet.yaml Inference Model/Trained Model ResNet50_face 512 98.12/98.56/99.77 87.2 ResNet50_face.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are Accuracy scores measured on the AgeDB-30, CFP-FP, and LFW datasets, respectively.</p>"},{"location":"en/support_list/models_list.html#main-body-detection-module","title":"Main Body Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-ShiTuV2_det 41.5 33.7426 537.003 27.6 M PP-ShiTuV2_det.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the PaddleClas main body detection dataset.</p>"},{"location":"en/support_list/models_list.html#object-detection-module","title":"Object Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link Cascade-FasterRCNN-ResNet50-FPN 41.1 - - 245.4 M Cascade-FasterRCNN-ResNet50-FPN.yaml Inference Model/Trained Model Cascade-FasterRCNN-ResNet50-vd-SSLDv2-FPN 45.0 - - 246.2 M Cascade-FasterRCNN-ResNet50-vd-SSLDv2-FPN.yaml Inference Model/Trained Model CenterNet-DLA-34 37.6 - - 75.4 M CenterNet-DLA-34.yaml Inference Model/Trained Model CenterNet-ResNet50 38.9 - - 319.7 M CenterNet-ResNet50.yaml Inference Model/Trained Model DETR-R50 42.3 59.2132 5334.52 159.3 M DETR-R50.yaml Inference Model/Trained Model FasterRCNN-ResNet34-FPN 37.8 - - 137.5 M FasterRCNN-ResNet34-FPN.yaml Inference Model/Trained Model FasterRCNN-ResNet50-FPN 38.4 - - 148.1 M FasterRCNN-ResNet50-FPN.yaml Inference Model/Trained Model FasterRCNN-ResNet50-vd-FPN 39.5 - - 148.1 M FasterRCNN-ResNet50-vd-FPN.yaml Inference Model/Trained Model FasterRCNN-ResNet50-vd-SSLDv2-FPN 41.4 - - 148.1 M FasterRCNN-ResNet50-vd-SSLDv2-FPN.yaml Inference Model/Trained Model FasterRCNN-ResNet50 36.7 - - 120.2 M FasterRCNN-ResNet50.yaml Inference Model/Trained Model FasterRCNN-ResNet101-FPN 41.4 - - 216.3 M FasterRCNN-ResNet101-FPN.yaml Inference Model/Trained Model FasterRCNN-ResNet101 39.0 - - 188.1 M FasterRCNN-ResNet101.yaml Inference Model/Trained Model FasterRCNN-ResNeXt101-vd-FPN 43.4 - - 360.6 M FasterRCNN-ResNeXt101-vd-FPN.yaml Inference Model/Trained Model FasterRCNN-Swin-Tiny-FPN 42.6 - - 159.8 M FasterRCNN-Swin-Tiny-FPN.yaml Inference Model/Trained Model FCOS-ResNet50 39.6 103.367 3424.91 124.2 M FCOS-ResNet50.yaml Inference Model/Trained Model PicoDet-L 42.6 16.6715 169.904 20.9 M PicoDet-L.yaml Inference Model/Trained Model PicoDet-M 37.5 16.2311 71.7257 16.8 M PicoDet-M.yaml Inference Model/Trained Model PicoDet-S 29.1 14.097 37.6563 4.4 M PicoDet-S.yaml Inference Model/Trained Model PicoDet-XS 26.2 13.8102 48.3139 5.7M PicoDet-XS.yaml Inference Model/Trained Model PP-YOLOE_plus-L 52.9 33.5644 814.825 185.3 M PP-YOLOE_plus-L.yaml Inference Model/Trained Model PP-YOLOE_plus-M 49.8 19.843 449.261 83.2 M PP-YOLOE_plus-M.yaml Inference Model/Trained Model PP-YOLOE_plus-S 43.7 16.8884 223.059 28.3 M PP-YOLOE_plus-S.yaml Inference Model/Trained Model PP-YOLOE_plus-X 54.7 57.8995 1439.93 349.4 M PP-YOLOE_plus-X.yaml Inference Model/Trained Model RT-DETR-H 56.3 114.814 3933.39 435.8 M RT-DETR-H.yaml Inference Model/Trained Model RT-DETR-L 53.0 34.5252 1454.27 113.7 M RT-DETR-L.yaml Inference Model/Trained Model RT-DETR-R18 46.5 19.89 784.824 70.7 M RT-DETR-R18.yaml Inference Model/Trained Model RT-DETR-R50 53.1 41.9327 1625.95 149.1 M RT-DETR-R50.yaml Inference Model/Trained Model RT-DETR-X 54.8 61.8042 2246.64 232.9 M RT-DETR-X.yaml Inference Model/Trained Model YOLOv3-DarkNet53 39.1 40.1055 883.041 219.7 M YOLOv3-DarkNet53.yaml Inference Model/Trained Model YOLOv3-MobileNetV3 31.4 18.6692 267.214 83.8 M YOLOv3-MobileNetV3.yaml Inference Model/Trained Model YOLOv3-ResNet50_vd_DCN 40.6 31.6276 856.047 163.0 M YOLOv3-ResNet50_vd_DCN.yaml Inference Model/Trained Model YOLOX-L 50.1 185.691 1250.58 192.5 M YOLOX-L.yaml Inference Model/Trained Model YOLOX-M 46.9 123.324 688.071 90.0 M YOLOX-M.yaml Inference Model/Trained Model YOLOX-N 26.1 79.1665 155.59 3.4 M YOLOX-N.yaml Inference Model/Trained Model YOLOX-S 40.4 184.828 474.446 32.0 M YOLOX-S.yaml Inference Model/Trained Model YOLOX-T 32.9 102.748 212.52 18.1 M YOLOX-T.yaml Inference Model/Trained Model YOLOX-X 51.8 227.361 2067.84 351.5 M YOLOX-X.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the COCO2017 validation set.</p>"},{"location":"en/support_list/models_list.html#small-object-detection-module","title":"Small Object Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-YOLOE_plus_SOD-S 25.1 65.4608 324.37 77.3 M PP-YOLOE_plus_SOD-S.yaml Inference Model/Trained Model PP-YOLOE_plus_SOD-L 31.9 57.1448 1006.98 325.0 M PP-YOLOE_plus_SOD-L.yaml Inference Model/Trained Model PP-YOLOE_plus_SOD-largesize-L 42.7 458.521 11172.7 340.5 M PP-YOLOE_plus_SOD-largesize-L.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the VisDrone-DET validation set.</p>"},{"location":"en/support_list/models_list.html#pedestrian-detection-module","title":"Pedestrian Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-YOLOE-L_human 48.0 32.7754 777.691 196.1 M PP-YOLOE-L_human.yaml Inference Model/Trained Model PP-YOLOE-S_human 42.5 15.0118 179.317 28.8 M PP-YOLOE-S_human.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the CrowdHuman validation set.</p>"},{"location":"en/support_list/models_list.html#vehicle-detection-module","title":"Vehicle Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-YOLOE-L_vehicle 63.9 32.5619 775.633 196.1 M PP-YOLOE-L_vehicle.yaml Inference Model/Trained Model PP-YOLOE-S_vehicle 61.3 15.3787 178.441 28.8 M PP-YOLOE-S_vehicle.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are mAP(0.5:0.95) on the PPVehicle validation set.</p>"},{"location":"en/support_list/models_list.html#face-detection-module","title":"Face Detection Module","text":"Model AP (%)Easy/Medium/Hard GPU Inference Time (ms) CPU Inference Time (ms) Model Size (M) YAML File Model Download Link BlazeFace 77.7/73.4/49.5 0.447 BlazeFace.yaml Inference Model/Trained Model BlazeFace-FPN-SSH 83.2/80.5/60.5 0.606 BlazeFace-FPN-SSH.yaml Inference Model/Trained Model PicoDet_LCNet_x2_5_face 93.7/90.7/68.1 28.9 PicoDet_LCNet_x2_5_face.yaml Inference Model/Trained Model PP-YOLOE_plus-S_face 93.9/91.8/79.8 26.5 PP-YOLOE_plus-S_face Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the WIDER-FACE validation set with an input size of 640*640.</p>"},{"location":"en/support_list/models_list.html#abnormality-detection-module","title":"Abnormality Detection Module","text":"Model Name Avg (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link STFPM 96.2 - - 21.5 M STFPM.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the MVTec AD dataset using the average anomaly score.</p>"},{"location":"en/support_list/models_list.html#semantic-segmentation-module","title":"Semantic Segmentation Module","text":"Model Name mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link Deeplabv3_Plus-R50 80.36 61.0531 1513.58 94.9 M Deeplabv3_Plus-R50.yaml Inference Model/Trained Model Deeplabv3_Plus-R101 81.10 100.026 2460.71 162.5 M Deeplabv3_Plus-R101.yaml Inference Model/Trained Model Deeplabv3-R50 79.90 82.2631 1735.83 138.3 M Deeplabv3-R50.yaml Inference Model/Trained Model Deeplabv3-R101 80.85 121.492 2685.51 205.9 M Deeplabv3-R101.yaml Inference Model/Trained Model OCRNet_HRNet-W18 80.67 48.2335 906.385 43.1 M OCRNet_HRNet-W18.yaml Inference Model/Trained Model OCRNet_HRNet-W48 82.15 78.9976 2226.95 249.8 M OCRNet_HRNet-W48.yaml Inference Model/Trained Model PP-LiteSeg-T 73.10 7.6827 138.683 28.5 M PP-LiteSeg-T.yaml Inference Model/Trained Model PP-LiteSeg-B 75.25 10.9935 194.727 47.0 M PP-LiteSeg-B.yaml Inference Model/Trained Model SegFormer-B0 (slice) 76.73 11.1946 268.929 13.2 M SegFormer-B0.yaml Inference Model/Trained Model SegFormer-B1 (slice) 78.35 17.9998 403.393 48.5 M SegFormer-B1.yaml Inference Model/Trained Model SegFormer-B2 (slice) 81.60 48.0371 1248.52 96.9 M SegFormer-B2.yaml Inference Model/Trained Model SegFormer-B3 (slice) 82.47 64.341 1666.35 167.3 M SegFormer-B3.yaml Inference Model/Trained Model SegFormer-B4 (slice) 82.38 82.4336 1995.42 226.7 M SegFormer-B4.yaml Inference Model/Trained Model SegFormer-B5 (slice) 82.58 97.3717 2420.19 229.7 M SegFormer-B5.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the Cityscapes dataset using mIoU.</p> Model Name mIoU (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link SeaFormer_base(slice) 40.92 24.4073 397.574 30.8 M SeaFormer_base.yaml Inference Model/Trained Model SeaFormer_large (slice) 43.66 27.8123 550.464 49.8 M SeaFormer_large.yaml Inference Model/Trained Model SeaFormer_small (slice) 38.73 19.2295 358.343 14.3 M SeaFormer_small.yaml Inference Model/Trained Model SeaFormer_tiny (slice) 34.58 13.9496 330.132 6.1 M SeaFormer_tiny.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the ADE20k dataset. \"slice\" indicates that the input image has been cropped.</p>"},{"location":"en/support_list/models_list.html#instance-segmentation-module","title":"Instance Segmentation Module","text":"Model Name Mask AP GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link Mask-RT-DETR-H 50.6 132.693 4896.17 449.9 M Mask-RT-DETR-H.yaml Inference Model/Trained Model Mask-RT-DETR-L 45.7 46.5059 2575.92 113.6 M Mask-RT-DETR-L.yaml Inference Model/Trained Model Mask-RT-DETR-M 42.7 36.8329 - 66.6 M Mask-RT-DETR-M.yaml Inference Model/Trained Model Mask-RT-DETR-S 41.0 33.5007 - 51.8 M Mask-RT-DETR-S.yaml Inference Model/Trained Model Mask-RT-DETR-X 47.5 75.755 3358.04 237.5 M Mask-RT-DETR-X.yaml Inference Model/Trained Model Cascade-MaskRCNN-ResNet50-FPN 36.3 - - 254.8 M Cascade-MaskRCNN-ResNet50-FPN.yaml Inference Model/Trained Model Cascade-MaskRCNN-ResNet50-vd-SSLDv2-FPN 39.1 - - 254.7 M Cascade-MaskRCNN-ResNet50-vd-SSLDv2-FPN.yaml Inference Model/Trained Model MaskRCNN-ResNet50-FPN 35.6 - - 157.5 M MaskRCNN-ResNet50-FPN.yaml Inference Model/Trained Model MaskRCNN-ResNet50-vd-FPN 36.4 - - 157.5 M MaskRCNN-ResNet50-vd-FPN.yaml Inference Model/Trained Model MaskRCNN-ResNet50 32.8 - - 127.8 M MaskRCNN-ResNet50.yaml Inference Model/Trained Model MaskRCNN-ResNet101-FPN 36.6 - - 225.4 M MaskRCNN-ResNet101-FPN.yaml Inference Model/Trained Model MaskRCNN-ResNet101-vd-FPN 38.1 - - 225.1 M MaskRCNN-ResNet101-vd-FPN.yaml Inference Model/Trained Model MaskRCNN-ResNeXt101-vd-FPN 39.5 - - 370.0 M MaskRCNN-ResNeXt101-vd-FPN.yaml Inference Model/Trained Model PP-YOLOE_seg-S 32.5 - - 31.5 M PP-YOLOE_seg-S.yaml Inference Model/Trained Model SOLOv2 35.5 - - 179.1 M SOLOv2.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on the COCO2017 validation set using Mask AP(0.5:0.95).</p>"},{"location":"en/support_list/models_list.html#text-detection-module","title":"Text Detection Module","text":"Model Name Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-OCRv4_mobile_det 77.79 10.6923 120.177 4.2 M PP-OCRv4_mobile_det.yaml Inference Model/Trained Model PP-OCRv4_server_det 82.69 83.3501 2434.01 100.1M PP-OCRv4_server_det.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on a self-built Chinese dataset by PaddleOCR, covering street scenes, web images, documents, and handwritten texts, with 500 images for detection.</p>"},{"location":"en/support_list/models_list.html#seal-text-detection-module","title":"Seal Text Detection Module","text":"Model Name Detection Hmean (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-OCRv4_mobile_seal_det 96.47 10.5878 131.813 4.7 M PP-OCRv4_mobile_seal_det.yaml Inference Model/Trained Model PP-OCRv4_server_seal_det 98.21 84.341 2425.06 108.3 M PP-OCRv4_server_seal_det.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on a self-built seal dataset by PaddleX, containing 500 seal images.</p>"},{"location":"en/support_list/models_list.html#text-recognition-module","title":"Text Recognition Module","text":"Model Name Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PP-OCRv4_mobile_rec 78.20 7.95018 46.7868 10.6 M PP-OCRv4_mobile_rec.yaml Inference Model/Trained Model PP-OCRv4_server_rec 79.20 7.19439 140.179 71.2 M PP-OCRv4_server_rec.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on a self-built Chinese dataset by PaddleOCR, covering street scenes, web images, documents, and handwritten texts, with 11,000 images for text recognition.</p> Model Name Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link ch_SVTRv2_rec 68.81 8.36801 165.706 73.9 M ch_SVTRv2_rec.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition A-Rank.</p> Model Name Recognition Avg Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link ch_RepSVTR_rec 65.07 10.5047 51.5647 22.1 M ch_RepSVTR_rec.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on PaddleOCR Algorithm Model Challenge - Task 1: OCR End-to-End Recognition B-Rank.</p>"},{"location":"en/support_list/models_list.html#formula-recognition-module","title":"Formula Recognition Module","text":"Model Name BLEU Score Normed Edit Distance ExpRate (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link LaTeX_OCR_rec 0.8821 0.0823 40.01 - - 89.7 M LaTeX_OCR_rec.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the LaTeX-OCR formula recognition test set.</p>"},{"location":"en/support_list/models_list.html#table-structure-recognition-module","title":"Table Structure Recognition Module","text":"Model Name Accuracy (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link SLANet 59.52 522.536 1845.37 6.9 M SLANet.yaml Inference Model/Trained Model SLANet_plus 63.69 522.536 1845.37 6.9 M SLANet_plus.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are evaluated on a self-built English table recognition dataset by PaddleX.</p>"},{"location":"en/support_list/models_list.html#image-rectification-module","title":"Image Rectification Module","text":"Model Name MS-SSIM (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link UVDoc 54.40 - - 30.3 M UVDoc.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on a self-built image rectification dataset by PaddleX.</p>"},{"location":"en/support_list/models_list.html#layout-detection-module","title":"Layout Detection Module","text":"Model Name mAP (%) GPU Inference Time (ms) CPU Inference Time (ms) Model Size YAML File Model Download Link PicoDet_layout_1x 86.8 13.036 91.2634 7.4 M PicoDet_layout_1x.yaml Inference Model/Trained Model PicoDet_layout_1x_table 95.7 12.623 90.8934 7.4 M PicoDet_layout_1x_table.yaml Inference Model/Trained Model PicoDet-S_layout_3cls 87.1 13.521 45.7633 4.8 M PicoDet-S_layout_3cls.yaml Inference Model/Trained Model PicoDet-S_layout_17cls 70.3 13.5632 46.2059 4.8 M PicoDet-S_layout_17cls.yaml Inference Model/Trained Model PicoDet-L_layout_3cls 89.3 15.7425 159.771 22.6 M PicoDet-L_layout_3cls.yaml Inference Model/Trained Model PicoDet-L_layout_17cls 79.9 17.1901 160.262 22.6 M PicoDet-L_layout_17cls.yaml Inference Model/Trained Model RT-DETR-H_layout_3cls 95.9 114.644 3832.62 470.1 M RT-DETR-H_layout_3cls.yaml Inference Model/Trained Model RT-DETR-H_layout_17cls 92.6 115.126 3827.25 470.2 M RT-DETR-H_layout_17cls.yaml Inference Model/Trained Model <p>Note: The evaluation set for the above accuracy metrics is the PaddleX self-built Layout Detection Dataset, containing 10,000 images.</p>"},{"location":"en/support_list/models_list.html#time-series-forecasting-module","title":"Time Series Forecasting Module","text":"Model Name mse mae Model Size YAML File Model Download Link DLinear 0.382 0.394 72 K DLinear.yaml Inference Model/Trained Model NLinear 0.386 0.392 40 K NLinear.yaml Inference Model/Trained Model Nonstationary 0.600 0.515 55.5 M Nonstationary.yaml Inference Model/Trained Model PatchTST 0.385 0.397 2.0 M PatchTST.yaml Inference Model/Trained Model RLinear 0.384 0.392 40 K RLinear.yaml Inference Model/Trained Model TiDE 0.405 0.412 31.7 M TiDE.yaml Inference Model/Trained Model TimesNet 0.417 0.431 4.9 M TimesNet.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the ETTH1 dataset (evaluation results on the test set test.csv).</p>"},{"location":"en/support_list/models_list.html#time-series-anomaly-detection-module","title":"Time Series Anomaly Detection Module","text":"Model Name Precision Recall f1_score Model Size YAML File Model Download Link AutoEncoder_ad 99.36 84.36 91.25 52 K AutoEncoder_ad.yaml Inference Model/Trained Model DLinear_ad 98.98 93.96 96.41 112 K DLinear_ad.yaml Inference Model/Trained Model Nonstationary_ad 98.55 88.95 93.51 1.8 M Nonstationary_ad.yaml Inference Model/Trained Model PatchTST_ad 98.78 90.70 94.57 320 K PatchTST_ad.yaml Inference Model/Trained Model TimesNet_ad 98.37 94.80 96.56 1.3 M TimesNet_ad.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the PSM dataset.</p>"},{"location":"en/support_list/models_list.html#time-series-classification-module","title":"Time Series Classification Module","text":"Model Name acc (%) Model Size YAML File Model Download Link TimesNet_cls 87.5 792 K TimesNet_cls.yaml Inference Model/Trained Model <p>Note: The above accuracy metrics are measured on the UWaveGestureLibrary dataset.</p> <p>&gt;Note: All GPU inference times for the above models are based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speeds are based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</p>"},{"location":"en/support_list/pipelines_list.html","title":"PaddleX Pipelines (CPU/GPU)","text":""},{"location":"en/support_list/pipelines_list.html#1-basic-pipelines","title":"1. Basic Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience URL Pipeline Introduction Applicable Scenarios Image Classification Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The General Image Classification Pipeline is designed to solve image classification tasks for given images. <ul> <li>Automatic classification and recognition of product images</li> <li>Real-time monitoring of defective products on production lines</li> <li>Personnel recognition in security surveillance</li> </ul> Object Detection Object Detection Online Experience Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. Unlike simple image classification, object detection not only recognizes what objects are in the image, such as people, cars, and animals, but also accurately determines the specific location of each object, usually represented by a rectangular box. This technology is widely used in autonomous driving, surveillance systems, and smart photo albums, relying on deep learning models (e.g., YOLO, Faster R-CNN) that efficiently extract features and perform real-time detection, significantly enhancing the computer's ability to understand image content. <ul> <li>Tracking moving objects in video surveillance</li> <li>Vehicle detection in autonomous driving</li> <li>Defect detection in industrial manufacturing</li> <li>Shelf product detection in retail</li> </ul> Semantic Segmentation Semantic Segmentation Online Experience Semantic segmentation is a computer vision technique that assigns each pixel in an image to a specific category, enabling detailed understanding of image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing entire regions of the same category to be marked. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, sky, and roads at the pixel level, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (e.g., FCN, U-Net) that use Convolutional Neural Networks (CNNs) to extract features and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis. <ul> <li>Analysis of satellite images in Geographic Information Systems</li> <li>Segmentation of obstacles and passable areas in robot vision</li> <li>Separation of foreground and background in film production</li> </ul> Instance Segmentation Instance Segmentation Online Experience Instance segmentation is a computer vision task that identifies object categories in images and distinguishes the pixels of different instances within the same category, enabling precise segmentation of each object. Instance segmentation can separately mark each car, person, or animal in an image, ensuring they are processed independently at the pixel level. For example, in a street scene image with multiple cars and pedestrians, instance segmentation can clearly separate the contours of each car and person, forming multiple independent region labels. This technology is widely used in autonomous driving, video surveillance, and robot vision, often relying on deep learning models (e.g., Mask R-CNN) that use CNNs for efficient pixel classification and instance differentiation, providing powerful support for understanding complex scenes. <ul> <li>Crowd counting in malls</li> <li>Counting crops or fruits in agricultural intelligence</li> <li>Selecting and segmenting specific objects in image editing</li> </ul> Document Scene Information Extraction v3 Table Structure Recognition Online Experience Document Image Scene Information Extraction v3 (PP-ChatOCRv3-doc) is a PaddlePaddle-specific intelligent document and image analysis solution that integrates LLM and OCR technologies to solve common complex document information extraction challenges such as layout analysis, rare characters, multi-page PDFs, tables, and seal recognition. By integrating the Wenxin large model, it combines vast data and knowledge, providing high accuracy and wide applicability. The open-source version supports local experience and deployment, and fine-tuning training for each module. <ul> <li>Construction of knowledge graphs</li> <li>Detection of information related to specific events in online news and social media</li> <li>Extraction and analysis of key information in academic literature (especially in scenarios requiring recognition of seals, distorted images, and more complex tables)</li> </ul> Layout Area Detection Text Detection Text Recognition Seal Text Detection Text Image Correction Document Image Orientation Classification OCR Text Detection Online Experience OCR (Optical Character Recognition) is a technology that converts text in images into editable text. It is widely used in document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols. The General OCR Pipeline is designed to solve text recognition tasks, extracting text information from images and outputting it in text form. PP-OCRv4 is an end-to-end OCR system that achieves millisecond-level text content prediction on CPUs, achieving state-of-the-art (SOTA) performance in general scenarios. Based on this project, developers from academia, industry, and research have quickly implemented various OCR applications covering general, manufacturing, finance, transportation. <ul> <li>Document digitization</li> <li>Information extraction</li> <li>Data processing</li> </ul> Text Recognition Table Recognition Layout Detection Online Experience Table recognition is a technology that automatically identifies and extracts table content and its structure from documents or images. It is widely used in data entry, information retrieval, and document analysis. By leveraging computer vision and machine learning algorithms, table recognition can convert complex table information into editable formats, facilitating further data processing and analysis by users <ul> <li>Processing of bank statements</li> <li>recognition and extraction of various indicators in medical reports</li> <li>extraction of tabular information from contracts</li> </ul> Table Structure Recognition  Text Detection Text Recognition Time Series Forecasting Time Series Forecasting Module Online Experience Time series forecasting is a technique that utilizes historical data to predict future trends by analyzing patterns in time series data. It is widely applied in financial markets, weather forecasting, and sales prediction. Time series forecasting typically employs statistical methods or deep learning models (such as LSTM, ARIMA, etc.), which can handle time dependencies in data to provide accurate predictions, assisting decision-makers in better planning and response. This technology plays a crucial role in many industries, including energy management, supply chain optimization, and market analysis <ul> <li>Stock prediction</li> <li>climate forecasting</li> <li>disease spread prediction</li> <li>energy demand forecasting</li> <li>traffic flow prediction</li> <li>product lifecycle prediction</li> <li>electric load forecasting</li> </ul> Time Series Anomaly Detection Time Series Anomaly Detection Module Online Experience Time series anomaly detection is a technique that identifies abnormal patterns or behaviors in time series data. It is widely used in network security, device monitoring, and financial fraud detection. By analyzing normal trends and patterns in historical data, it discovers events that significantly differ from expected behaviors, such as sudden increases in network traffic or unusual transaction activities. Time series anomaly detection often employs statistical methods or machine learning algorithms (like Isolation Forest, LSTM, etc.), which can automatically identify anomalies in data, providing real-time alerts to enterprises and organizations to help promptly address potential risks and issues. This technology plays a vital role in ensuring system stability and security <ul> <li>Financial fraud detection</li> <li>network intrusion detection</li> <li>equipment failure detection</li> <li>industrial production anomaly detection</li> <li>stock market anomaly detection</li> <li>power system anomaly detection</li> </ul> Time Series Classification Time Series Classification Module Online Experience Time series classification is a technique that categorizes time series data into predefined classes. It is widely applied in behavior recognition, speech recognition, and financial trend analysis. By analyzing features that vary over time, it identifies different patterns or events, such as classifying a speech signal as \"greeting\" or \"request\" or dividing stock price movements into \"rising\" or \"falling.\" Time series classification typically utilizes machine learning and deep learning models, effectively capturing time dependencies and variation patterns to provide accurate classification labels for data. This technology plays a key role in intelligent monitoring, voice assistants, and market forecasting applications <ul> <li>Electrocardiogram Classification</li> <li>Stock Market Behavior Classification</li> <li>Electroencephalogram Classification</li> <li>Emotion Classification</li> <li>Traffic Condition Classification</li> <li>Network Traffic Classification</li> <li>Equipment Operating Condition Classification</li> </ul> Multi-label Image Classification Multi-label Image Classification None Image multi-label classification is a technology that assigns an image to multiple related categories simultaneously. It is widely used in image tagging, content recommendation, and social media analysis. It can identify multiple objects or features present in an image, such as both \"dog\" and \"outdoor\" labels in a single picture. By using deep learning models, image multi-label classification can automatically extract image features and perform accurate classification to provide more comprehensive information for users. This technology is significant in applications like intelligent search engines and automatic content generation. <ul> <li>Medical image diagnosis</li> <li>Complex scene recognition</li> <li>Multi-target monitoring</li> <li>Product attribute recognition</li> <li>Ecological environment monitoring</li> <li>Security monitoring</li> <li>Disaster warning</li> </ul> Small Object Detection Small Object Detection None Small object detection is a technology specifically for identifying small objects in images. It is widely used in surveillance, autonomous driving, and satellite image analysis. It can accurately find and classify small-sized objects like pedestrians, traffic signs, or small animals in complex scenes. By using deep learning algorithms and optimized convolutional neural networks, small object detection can effectively enhance the recognition ability of small objects, ensuring that important information is not missed in practical applications. This technology plays an important role in improving safety and automation levels. <ul> <li>Pedestrian detection in autonomous vehicles</li> <li>Identification of small buildings in satellite images</li> <li>Detection of small traffic signs in intelligent transportation systems</li> <li>Identification of small intruding objects in security surveillance</li> <li>Detection of small defects in industrial inspection</li> <li>Monitoring of small animals in drone images</li> </ul> Image Anomaly Detection Image Anomaly Detection None Image anomaly detection is a technology that identifies images that deviate from or do not conform to normal patterns by analyzing their content. It is widely used in industrial quality inspection, medical image analysis, and security surveillance. By using machine learning and deep learning algorithms, image anomaly detection can automatically identify potential defects, anomalies, or abnormal behavior in images, helping us detect problems and take appropriate measures promptly. Image anomaly detection systems are designed to automatically detect and label abnormal situations in images to improve work efficiency and accuracy. <ul> <li>Industrial quality control</li> <li>Medical image analysis</li> <li>Anomaly detection in surveillance videos</li> <li>Identification of violations in traffic monitoring</li> <li>Obstacle detection in autonomous driving</li> <li>Agricultural pest and disease monitoring</li> <li>Pollutant identification in environmental monitoring</li> </ul> Layout Parsing Table Structure Recognition None Layout analysis is a technology for extracting structured information from document images, primarily used to convert complex document layouts into machine-readable data formats. This technology has wide applications in document management, information extraction, and data digitization. By combining optical character recognition (OCR), image processing, and machine learning algorithms, layout analysis can identify and extract text blocks, titles, paragraphs, images, tables, and other layout elements from documents. This process typically includes three main steps: layout analysis, element analysis, and data formatting, ultimately generating structured document data that enhances data processing efficiency and accuracy. <ul> <li>Financial and legal document analysis</li> <li>Digitization of historical documents and archives</li> <li>Automated form filling</li> <li>Page structure analysis</li> </ul> Layout Area Detection Text Detection Text Recognition Formula Recognition Seal Text Detection Text Image Correction Document Image Orientation Classification Formula Recognition Layout Area Detection None Formula recognition is a technology that automatically identifies and extracts LaTeX formula content and its structure from documents or images. It is widely used in document editing and data analysis in fields such as mathematics, physics, and computer science. By using computer vision and machine learning algorithms, formula recognition can convert complex mathematical formula information into an editable LaTeX format, facilitating further data processing and analysis by users. <ul> <li>Document digitization and retrieval</li> <li>Formula search engine</li> <li>Formula editor</li> <li>Automated typesetting</li> </ul> Formula Recognition Seal Text Recognition Layout Area Detection None Seal text recognition is a technology that automatically extracts and recognizes seal content from documents or images. Recognizing seal text is part of document processing and has applications in many scenarios, such as contract comparison, inventory audit, and invoice reimbursement audit. <ul> <li>Contract and agreement validation</li> <li>Check processing</li> <li>Loan approval</li> <li>Legal document management</li> </ul> Seal Text Detection Text Recognition"},{"location":"en/support_list/pipelines_list.html#2-featured-pipelines","title":"2. Featured Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience Link Pipeline Introduction Applicable Scenarios Semi-supervised Learning for Large Models - Image Classification Semi-supervised Learning for Large Models - Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The general image classification pipeline is designed to solve image classification tasks for given images. <ul> <li>Commodity image classification</li> <li>Artwork style classification</li> <li>Crop disease and pest identification</li> <li>Animal species recognition</li> <li>Classification of land, water bodies, and buildings in satellite remote sensing images</li> </ul> Semi-supervised Learning for Large Models - Object Detection Semi-supervised Learning for Large Models - Object Detection Online Experience The semi-supervised learning for large models - object detection pipeline is a unique offering from PaddlePaddle. It utilizes a joint training approach with large and small models, leveraging a small amount of labeled data and a large amount of unlabeled data to enhance model accuracy, significantly reducing the costs of manual model iteration and data annotation. The figure below demonstrates the performance of this pipeline on the COCO dataset with 10% labeled data. After training with this pipeline, on COCO 10% labeled data + 90% unlabeled data, the large model (RT-DETR-H) achieves an 8.4% higher accuracy (47.7% -&gt; 56.1%), setting a new state-of-the-art (SOTA) for this dataset. The small model (PicoDet-S) also achieves over 10% higher accuracy (18.3% -&gt; 28.8%) compared to direct training. <ul> <li>Pedestrian, vehicle, and traffic sign detection in autonomous driving</li> <li>Enemy facility and equipment detection in military reconnaissance</li> <li>Seabed organism detection in deep-sea exploration</li> </ul> Semi-supervised Learning for Large Models - OCR Text Detection Online Experience The semi-supervised learning for large models - OCR pipeline is a unique OCR training pipeline from PaddlePaddle. It consists of a text detection model and a text recognition model working in series. The input image is first processed by the text detection model to obtain and rectify all text line bounding boxes, which are then fed into the text recognition model to generate OCR text results. In the text recognition part, a joint training approach with large and small models is adopted, utilizing a small amount of labeled data and a large amount of unlabeled data to enhance model accuracy, significantly reducing the costs of manual model iteration and data annotation. The figure below shows the effects of this pipeline in two OCR application scenarios, demonstrating significant improvements for both large and small models in different contexts. <ul> <li>Digitizing paper documents</li> <li>Reading and verifying personal information on IDs, passports, and driver's licenses</li> <li>Recognizing product information in retail</li> </ul> Large Model Semi-supervised Learning - Text Recognition General Scene Information Extraction v2 Text Detection Online Experience The General Scene Information Extraction Pipeline (PP-ChatOCRv2-common) is a unique intelligent analysis solution for complex documents from PaddlePaddle. It combines Large Language Models (LLMs) and OCR technology, leveraging the Wenxin Large Model to integrate massive data and knowledge, achieving high accuracy and wide applicability. The system flow of PP-ChatOCRv2-common is as follows: Input the prediction image, send it to the general OCR system, predict text through text detection and text recognition models, perform vector retrieval between the predicted text and user queries to obtain relevant text information, and finally pass these text information to the prompt generator to recombine them into prompts for the Wenxin Large Model to generate prediction results. <ul> <li>Key information extraction from various scenarios such as ID cards, bank cards, household registration books, train tickets, and paper invoices</li> </ul> Text Recognition"},{"location":"en/support_list/pipelines_list_dcu.html","title":"PaddleX Pipelines (DCU)","text":""},{"location":"en/support_list/pipelines_list_dcu.html#1-basic-pipelines","title":"1. Basic Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience URL Pipeline Introduction Applicable Scenarios General Image Classification Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The General Image Classification Pipeline is designed to solve image classification tasks for given images. <ul> <li>Automatic classification and recognition of product images</li> <li>Real-time monitoring of defective products on production lines</li> <li>Personnel recognition in security surveillance</li> </ul> General Semantic Segmentation Semantic Segmentation Online Experience Semantic segmentation is a computer vision technique that assigns each pixel in an image to a specific category, enabling detailed understanding of image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing entire regions of the same category to be marked. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, sky, and roads at the pixel level, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (e.g., FCN, U-Net) that use Convolutional Neural Networks (CNNs) to extract features and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis. <ul> <li>Analysis of satellite images in Geographic Information Systems</li> <li>Segmentation of obstacles and passable areas in robot vision</li> <li>Separation of foreground and background in film production</li> </ul>"},{"location":"en/support_list/pipelines_list_dcu.html#2-featured-pipelines","title":"2. Featured Pipelines","text":"<p>Not supported yet, please stay tuned!</p>"},{"location":"en/support_list/pipelines_list_mlu.html","title":"PaddleX Pipelines (MLU)","text":""},{"location":"en/support_list/pipelines_list_mlu.html#1-basic-pipelines","title":"1. Basic Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience URL Pipeline Introduction Applicable Scenarios General Image Classification Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The General Image Classification Pipeline is designed to solve image classification tasks for given images. <ul> <li>Automatic classification and recognition of product images</li> <li>Real-time monitoring of defective products on production lines</li> <li>Personnel recognition in security surveillance</li> </ul> General Object Detection Object Detection Online Experience Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. Unlike simple image classification, object detection not only recognizes what objects are in the image, such as people, cars, and animals, but also accurately determines the specific location of each object, usually represented by a rectangular box. This technology is widely used in autonomous driving, surveillance systems, and smart photo albums, relying on deep learning models (e.g., YOLO, Faster R-CNN) that efficiently extract features and perform real-time detection, significantly enhancing the computer's ability to understand image content. <ul> <li>Tracking moving objects in video surveillance</li> <li>Vehicle detection in autonomous driving</li> <li>Defect detection in industrial manufacturing</li> <li>Shelf product detection in retail</li> </ul> General Semantic Segmentation Semantic Segmentation Online Experience Semantic segmentation is a computer vision technique that assigns each pixel in an image to a specific category, enabling detailed understanding of image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing entire regions of the same category to be marked. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, sky, and roads at the pixel level, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (e.g., FCN, U-Net) that use Convolutional Neural Networks (CNNs) to extract features and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis. <ul> <li>Analysis of satellite images in Geographic Information Systems</li> <li>Segmentation of obstacles and passable areas in robot vision</li> <li>Separation of foreground and background in film production</li> </ul> General OCR Text Detection Online Experience OCR (Optical Character Recognition) is a technology that converts text in images into editable text. It is widely used in document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols. The General OCR Pipeline is designed to solve text recognition tasks, extracting text information from images and outputting it in text form. PP-OCRv4 is an end-to-end OCR system that achieves millisecond-level text content prediction on CPUs, achieving state-of-the-art (SOTA) performance in general scenarios. Based on this project, developers from academia, industry, and research have quickly implemented various OCR applications covering general, manufacturing, finance, transportation. <ul> <li>Document digitization</li> <li>Information extraction</li> <li>Data processing</li> </ul> Text Recognition Time Series Forecasting Time Series Forecasting Module Online Experience Time series forecasting is a technique that utilizes historical data to predict future trends by analyzing patterns in time series data. It is widely applied in financial markets, weather forecasting, and sales prediction. Time series forecasting typically employs statistical methods or deep learning models (such as LSTM, ARIMA, etc.), which can handle time dependencies in data to provide accurate predictions, assisting decision-makers in better planning and response. This technology plays a crucial role in many industries, including energy management, supply chain optimization, and market analysis. <ul> <li>Stock prediction</li> <li>Climate forecasting</li> <li>Disease spread prediction</li> <li>Energy demand forecasting</li> <li>Traffic flow prediction</li> <li>Product lifecycle prediction</li> <li>Electric load forecasting</li> </ul>"},{"location":"en/support_list/pipelines_list_mlu.html#2-featured-pipelines","title":"2. Featured Pipelines","text":"<p>Not supported yet, please stay tuned!</p>"},{"location":"en/support_list/pipelines_list_npu.html","title":"PaddleX Pipelines (NPU)","text":""},{"location":"en/support_list/pipelines_list_npu.html#1-basic-pipelines","title":"1. Basic Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience URL Pipeline Introduction Applicable Scenarios General Image Classification Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The General Image Classification Pipeline is designed to solve image classification tasks for given images. <ul> <li>Automatic classification and recognition of product images</li> <li>Real-time monitoring of defective products on production lines</li> <li>Personnel recognition in security surveillance</li> </ul> General Object Detection Object Detection Online Experience Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. Unlike simple image classification, object detection not only recognizes what objects are in the image, such as people, cars, and animals, but also accurately determines the specific location of each object, usually represented by a rectangular box. This technology is widely used in autonomous driving, surveillance systems, and smart photo albums, relying on deep learning models (e.g., YOLO, Faster R-CNN) that efficiently extract features and perform real-time detection, significantly enhancing the computer's ability to understand image content. <ul> <li>Tracking moving objects in video surveillance</li> <li>Vehicle detection in autonomous driving</li> <li>Defect detection in industrial manufacturing</li> <li>Shelf product detection in retail</li> </ul> General Semantic Segmentation Semantic Segmentation Online Experience Semantic segmentation is a computer vision technique that assigns each pixel in an image to a specific category, enabling detailed understanding of image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing entire regions of the same category to be marked. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, sky, and roads at the pixel level, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (e.g., FCN, U-Net) that use Convolutional Neural Networks (CNNs) to extract features and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis. <ul> <li>Analysis of satellite images in Geographic Information Systems</li> <li>Segmentation of obstacles and passable areas in robot vision</li> <li>Separation of foreground and background in film production</li> </ul> General Instance Segmentation Instance Segmentation Online Experience Instance segmentation is a computer vision task that identifies object categories in images and distinguishes the pixels of different instances within the same category, enabling precise segmentation of each object. Instance segmentation can separately mark each car, person, or animal in an image, ensuring they are processed independently at the pixel level. For example, in a street scene image with multiple cars and pedestrians, instance segmentation can clearly separate the contours of each car and person, forming multiple independent region labels. This technology is widely used in autonomous driving, video surveillance, and robot vision, often relying on deep learning models (e.g., Mask R-CNN) that use CNNs for efficient pixel classification and instance differentiation, providing powerful support for understanding complex scenes. <ul> <li>Crowd counting in malls</li> <li>Counting crops or fruits in agricultural intelligence</li> <li>Selecting and segmenting specific objects in image editing</li> </ul> General OCR Text Detection Online Experience OCR (Optical Character Recognition) is a technology that converts text in images into editable text. It is widely used in document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols. The General OCR Pipeline is designed to solve text recognition tasks, extracting text information from images and outputting it in text form. PP-OCRv4 is an end-to-end OCR system that achieves millisecond-level text content prediction on CPUs, achieving state-of-the-art (SOTA) performance in general scenarios. Based on this project, developers from academia, industry, and research have quickly implemented various OCR applications covering general, manufacturing, finance, transportation. <ul> <li>Document digitization</li> <li>Information extraction</li> <li>Data processing</li> </ul> Text Recognition General Table Recognition Layout Detection Online Experience Table recognition is a technology that automatically identifies and extracts table content and its structure from documents or images. It is widely used in data entry, information retrieval, and document analysis. By leveraging computer vision and machine learning algorithms, table recognition can convert complex table information into editable formats, facilitating further data processing and analysis by users <ul> <li>Processing of bank statements</li> <li>recognition and extraction of various indicators in medical reports</li> <li>extraction of tabular information from contracts</li> </ul> Table Structure Recognition  Text Detection Text Recognition Time Series Forecasting Time Series Forecasting Module Online Experience Time series forecasting is a technique that utilizes historical data to predict future trends by analyzing patterns in time series data. It is widely applied in financial markets, weather forecasting, and sales prediction. Time series forecasting typically employs statistical methods or deep learning models (such as LSTM, ARIMA, etc.), which can handle time dependencies in data to provide accurate predictions, assisting decision-makers in better planning and response. This technology plays a crucial role in many industries, including energy management, supply chain optimization, and market analysis <ul> <li>Stock prediction</li> <li>climate forecasting</li> <li>disease spread prediction</li> <li>energy demand forecasting</li> <li>traffic flow prediction</li> <li>product lifecycle prediction</li> <li>electric load forecasting</li> </ul> Time Series Anomaly Detection Time Series Anomaly Detection Module Online Experience Time series anomaly detection is a technique that identifies abnormal patterns or behaviors in time series data. It is widely used in network security, device monitoring, and financial fraud detection. By analyzing normal trends and patterns in historical data, it discovers events that significantly differ from expected behaviors, such as sudden increases in network traffic or unusual transaction activities. Time series anomaly detection often employs statistical methods or machine learning algorithms (like Isolation Forest, LSTM, etc.), which can automatically identify anomalies in data, providing real-time alerts to enterprises and organizations to help promptly address potential risks and issues. This technology plays a vital role in ensuring system stability and security <ul> <li>Financial fraud detection</li> <li>network intrusion detection</li> <li>equipment failure detection</li> <li>industrial production anomaly detection</li> <li>stock market anomaly detection</li> <li>power system anomaly detection</li> </ul> Time Series Classification Time Series Classification Module Online Experience Time series classification is a technique that categorizes time series data into predefined classes. It is widely applied in behavior recognition, speech recognition, and financial trend analysis. By analyzing features that vary over time, it identifies different patterns or events, such as classifying a speech signal as \"greeting\" or \"request\" or dividing stock price movements into \"rising\" or \"falling.\" Time series classification typically utilizes machine learning and deep learning models, effectively capturing time dependencies and variation patterns to provide accurate classification labels for data. This technology plays a key role in intelligent monitoring, voice assistants, and market forecasting applications <ul> <li>Electrocardiogram Classification</li> <li>Stock Market Behavior Classification</li> <li>Electroencephalogram Classification</li> <li>Emotion Classification</li> <li>Traffic Condition Classification</li> <li>Network Traffic Classification</li> <li>Equipment Operating Condition Classification</li> </ul>"},{"location":"en/support_list/pipelines_list_npu.html#2-featured-pipelines","title":"2. Featured Pipelines","text":"<p>Not supported yet, please stay tuned!</p>"},{"location":"en/support_list/pipelines_list_xpu.html","title":"PaddleX Pipelines (XPU)","text":""},{"location":"en/support_list/pipelines_list_xpu.html#1-basic-pipelines","title":"1. Basic Pipelines","text":"Pipeline Name Pipeline Modules Baidu AIStudio Community Experience URL Pipeline Introduction Applicable Scenarios General Image Classification Image Classification Online Experience Image classification is a technique that assigns images to predefined categories. It is widely used in object recognition, scene understanding, and automatic annotation. Image classification can identify various objects such as animals, plants, traffic signs, etc., and categorize them based on their features. By leveraging deep learning models, image classification can automatically extract image features and perform accurate classification. The General Image Classification Pipeline is designed to solve image classification tasks for given images. <ul> <li>Automatic classification and recognition of product images</li> <li>Real-time monitoring of defective products on production lines</li> <li>Personnel recognition in security surveillance</li> </ul> General Object Detection Object Detection Online Experience Object detection aims to identify the categories and locations of multiple objects in images or videos by generating bounding boxes to mark these objects. Unlike simple image classification, object detection not only recognizes what objects are in the image, such as people, cars, and animals, but also accurately determines the specific location of each object, usually represented by a rectangular box. This technology is widely used in autonomous driving, surveillance systems, and smart photo albums, relying on deep learning models (e.g., YOLO, Faster R-CNN) that efficiently extract features and perform real-time detection, significantly enhancing the computer's ability to understand image content. <ul> <li>Tracking moving objects in video surveillance</li> <li>Vehicle detection in autonomous driving</li> <li>Defect detection in industrial manufacturing</li> <li>Shelf product detection in retail</li> </ul> General Semantic Segmentation Semantic Segmentation Online Experience Semantic segmentation is a computer vision technique that assigns each pixel in an image to a specific category, enabling detailed understanding of image content. Semantic segmentation not only identifies the types of objects in an image but also classifies each pixel, allowing entire regions of the same category to be marked. For example, in a street scene image, semantic segmentation can distinguish pedestrians, cars, sky, and roads at the pixel level, forming a detailed label map. This technology is widely used in autonomous driving, medical image analysis, and human-computer interaction, often relying on deep learning models (e.g., FCN, U-Net) that use Convolutional Neural Networks (CNNs) to extract features and achieve high-precision pixel-level classification, providing a foundation for further intelligent analysis. <ul> <li>Analysis of satellite images in Geographic Information Systems</li> <li>Segmentation of obstacles and passable areas in robot vision</li> <li>Separation of foreground and background in film production</li> </ul> General OCR Text Detection Online Experience OCR (Optical Character Recognition) is a technology that converts text in images into editable text. It is widely used in document digitization, information extraction, and data processing. OCR can recognize printed text, handwritten text, and even certain types of fonts and symbols. The General OCR Pipeline is designed to solve text recognition tasks, extracting text information from images and outputting it in text form. PP-OCRv4 is an end-to-end OCR system that achieves millisecond-level text content prediction on CPUs, achieving state-of-the-art (SOTA) performance in general scenarios. Based on this project, developers from academia, industry, and research have quickly implemented various OCR applications covering general, manufacturing, finance, transportation. <ul> <li>Document digitization</li> <li>Information extraction</li> <li>Data processing</li> </ul> Text Recognition Time Series Forecasting Time Series Forecasting Module Online Experience Time series forecasting is a technique that utilizes historical data to predict future trends by analyzing patterns in time series data. It is widely applied in financial markets, weather forecasting, and sales prediction. Time series forecasting typically employs statistical methods or deep learning models (such as LSTM, ARIMA, etc.), which can handle time dependencies in data to provide accurate predictions, assisting decision-makers in better planning and response. This technology plays a crucial role in many industries, including energy management, supply chain optimization, and market analysis <ul> <li>Stock prediction</li> <li>climate forecasting</li> <li>disease spread prediction</li> <li>energy demand forecasting</li> <li>traffic flow prediction</li> <li>product lifecycle prediction</li> <li>electric load forecasting</li> </ul>"},{"location":"en/support_list/pipelines_list_xpu.html#2-featured-pipelines","title":"2. Featured Pipelines","text":"<p>Not supported yet, please stay tuned!</p>"}]}