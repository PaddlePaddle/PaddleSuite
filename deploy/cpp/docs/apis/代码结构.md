# 代码结构

## 名称空间-Namespace

部署里的名称空间namespace目前只有 PaddleDeploy



## 类-Class

### Model

模型进行推理计算的基类，各功能的入口在该类中。 

如果要部署自定义模型(该模型不是支持的套件导出)，需要写一个MyModel继承Model，并按需实现下面介绍的成员虚函数。

#### 成员变量

##### YAML::Node yaml_config_

存储标准化的yaml格式模型配置文件，用于前后处理。

所有套件的配件文件会转换为标准的配置，然后存于此传给前后处理。

##### std::shared_ptr\<BasePreProcess> preprocess_

预处理对象指针，不同的套件初始化时会指向属于自己套件的预处理对象。

##### std::shared_ptr\<InferEngine> infer_engine_

推理引擎对象指针，使用不同的推理引擎初始化会指向不同的推理引擎对象。

当前只支持Paddle Inference

##### std::shared_ptr\<BasePostProcess> postprocess_

后处理对象指针,不同的套件初始化时会指向属于自己套件的后处理对象。

##### std::vector\<Result> results_

保存推理结果的vector数组

#### 成员函数

##### virtual bool Init(const std::string& cfg_file)

初始化配置文件、模型前后处理。创建Model或者其子类对象后必须先调用它初始化，才能调推理接口。

参数:  cfg_file 配置文件的路径

返回值: 是否初始化成功



##### virtual bool YamlConfigInit(const std::string& cfg_file)

配置文件读取并初始化为标准配置文件。如果自定义模型有自己的独有的配置文件，需实现该虚函数。

参数:  cfg_file 配置文件的路径

返回值: 是否读取配置文件并初始化成功



##### virtual bool PreProcessInit()

各套件必须实现该虚函数。创建套件自己的预处理对象赋值给preprocess_，并调用预处理对象的初始化。



##### bool PaddleEngineInit(

​          const std::string& model_filename,

​          const std::string& params_filename,

​          bool use_gpu = false, int gpu_id = 0,

​          bool use_mkl = true);

初始化Paddle 推理引擎,  创建Model或者其子类对象后必须先调用它初始化，才能调推理接口。

参数:  model_filename    paddle导出模型的模型文件

​           params_filename paddle导出模型的参数文件

​           use_gpu                  是否使用GPU进行推理。true为开启，false为关闭

​           gpu_id                    指定哪一张GPU卡进行推理

​          use_mkl                  是否开启mkl进行加速

返回值: 是否初始化成功



##### **virtual** **bool** PostProcessInit()

各套件必须实现该虚函数。创建套件自己的后处理对象赋值给postprocess_，并调用后处理对象的初始化。



#####  virtual bool Predict(const std::vector\<cv::Mat>& imgs,  int thread_num = 1)

进行推理计算。

参数: imgs                cv::Mat格式的图片数组

​          thread_num  OpenMP对batch并行加速的线程数

返回值: 推理是否成功



​         



### MultiGPUModel



## 函数-Function

```c++
std::shared_ptr<Model> ModelFactory::CreateObject(const std::string &name);
```



## 数据结构-Struct